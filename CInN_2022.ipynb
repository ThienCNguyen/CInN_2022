{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"align_translate21.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1V7fNWLFkE9wYO2WMiCCTQv8vxZWWifmm","authorship_tag":"ABX9TyOXLV7OdL/TnJnymhshKCrC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEeLwLnGnaiJ","executionInfo":{"status":"ok","timestamp":1622192635136,"user_tz":-420,"elapsed":2363,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"02935016-35d1-4f0b-84b1-f785d0a83a48"},"source":["!git clone https://github.com/pytorch/fairseq"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'fairseq'...\n","remote: Enumerating objects: 28172, done.\u001b[K\n","remote: Counting objects: 100% (586/586), done.\u001b[K\n","remote: Compressing objects: 100% (359/359), done.\u001b[K\n","remote: Total 28172 (delta 288), reused 443 (delta 214), pack-reused 27586\u001b[K\n","Receiving objects: 100% (28172/28172), 11.91 MiB | 27.97 MiB/s, done.\n","Resolving deltas: 100% (21111/21111), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqPgw0v0nwY6","executionInfo":{"status":"ok","timestamp":1622192663330,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"ad99acc9-da77-4bc7-f23f-f5b3a18cdec9"},"source":["cd /content/fairseq"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/fairseq\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cR_Ki78-sALZ","executionInfo":{"status":"ok","timestamp":1622115463239,"user_tz":-420,"elapsed":442,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"a0e4a1ae-4755-4771-bd7d-18f45888c302"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/fairseq'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyyVuPvJnxi3","executionInfo":{"status":"ok","timestamp":1622192738116,"user_tz":-420,"elapsed":69267,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"4a3a74c8-832a-4b6f-ef48-80f1af9c101e"},"source":["pip install --editable ./"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Obtaining file:///content/fairseq\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting omegaconf<2.1\n","  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n","Requirement already satisfied: numpy; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (1.19.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (2019.12.20)\n","Collecting sacrebleu>=1.4.12\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (0.29.23)\n","Collecting hydra-core<1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 41.3MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (1.8.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (4.41.1)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+19793a7) (1.14.5)\n","Collecting PyYAML>=5.1.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 48.9MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+19793a7) (3.7.4.3)\n","Collecting portalocker==2.0.0\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+19793a7) (5.1.3)\n","Collecting antlr4-python3-runtime==4.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n","\u001b[K     |████████████████████████████████| 112kB 51.2MB/s \n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+19793a7) (2.20)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+19793a7) (3.4.1)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=807eea6963f4565988b984afd9dffe336fdae78435f9bddb1a4def5c0a562310\n","  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: PyYAML, omegaconf, portalocker, sacrebleu, antlr4-python3-runtime, hydra-core, fairseq\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Running setup.py develop for fairseq\n","Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.0.0 sacrebleu-1.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":837},"id":"nDisfOlQmic4","executionInfo":{"status":"ok","timestamp":1623430963841,"user_tz":-420,"elapsed":6103,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"89ea2887-17ae-497e-f884-c2991513c6ab"},"source":["pip install fairseq"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting fairseq\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n","\u001b[K     |████████████████████████████████| 1.7MB 28.8MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.8.1+cu101)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n","Collecting sacrebleu>=1.4.12\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 10.2MB/s \n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.23)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n","Collecting dataclasses\n","  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n","Collecting hydra-core\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n","\u001b[K     |████████████████████████████████| 153kB 58.9MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n","Collecting portalocker==2.0.0\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.1.3)\n","Collecting antlr4-python3-runtime==4.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n","\u001b[K     |████████████████████████████████| 112kB 58.6MB/s \n","\u001b[?25hCollecting omegaconf==2.1.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.1)\n","Collecting PyYAML>=5.1.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 52.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=38af4fb32064837241d496f44fcab7b3a594c588a8a046c91a9fa6763d0ef0de\n","  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: portalocker, sacrebleu, dataclasses, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, fairseq\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.0 omegaconf-2.1.0 portalocker-2.0.0 sacrebleu-1.5.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqxTRtPd9EzA","executionInfo":{"status":"ok","timestamp":1623430979331,"user_tz":-420,"elapsed":5004,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"42dd60d3-5e7e-4fa2-b5b4-f1fc551e56d1"},"source":["!time unzip /content/drive/MyDrive/tools/smt.zip -d /"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/tools/smt.zip\n","   creating: /content/tools/bin/\n","  inflating: /content/tools/bin/GIZA++  \n","  inflating: /content/tools/bin/mkcls  \n","  inflating: /content/tools/bin/snt2cooc.out  \n","   creating: /content/tools/moses/bin/\n","  inflating: /content/tools/moses/bin/score-stsg  \n","  inflating: /content/tools/moses/bin/extractor  \n","  inflating: /content/tools/moses/bin/kbmira  \n","  inflating: /content/tools/moses/bin/kenlm_benchmark  \n","  inflating: /content/tools/moses/bin/TMining  \n","  inflating: /content/tools/moses/bin/pruneGeneration  \n","  inflating: /content/tools/moses/bin/phrase_table_vocab  \n","  inflating: /content/tools/moses/bin/phrase-lookup  \n","  inflating: /content/tools/moses/bin/moses  \n","  inflating: /content/tools/moses/bin/train-expected-bleu  \n","  inflating: /content/tools/moses/bin/filter-rule-table  \n","  inflating: /content/tools/moses/bin/postprocess-egret-forests  \n","  inflating: /content/tools/moses/bin/consolidate-direct  \n","  inflating: /content/tools/moses/bin/prepare-expected-bleu-training  \n","  inflating: /content/tools/moses/bin/project-cache.jam  \n","  inflating: /content/tools/moses/bin/score  \n","  inflating: /content/tools/moses/bin/relax-parse  \n","  inflating: /content/tools/moses/bin/merge-sorted  \n","  inflating: /content/tools/moses/bin/extract-rules  \n","  inflating: /content/tools/moses/bin/statistics  \n","  inflating: /content/tools/moses/bin/extract  \n","  inflating: /content/tools/moses/bin/1-1-Extraction  \n","  inflating: /content/tools/moses/bin/CreateProbingPT  \n","  inflating: /content/tools/moses/bin/consolidate-reverse  \n","  inflating: /content/tools/moses/bin/extract-mixed-syntax  \n","  inflating: /content/tools/moses/bin/dump_counts  \n","  inflating: /content/tools/moses/bin/CreateProbingPT2  \n","  inflating: /content/tools/moses/bin/lmbrgrid  \n","  inflating: /content/tools/moses/bin/processLexicalTable  \n","  inflating: /content/tools/moses/bin/build_binary  \n","  inflating: /content/tools/moses/bin/filter  \n","  inflating: /content/tools/moses/bin/sentence-bleu  \n","  inflating: /content/tools/moses/bin/vwtrainer  \n","  inflating: /content/tools/moses/bin/symal  \n","   creating: /content/tools/moses/bin/gcc-7/\n","   creating: /content/tools/moses/bin/gcc-7/debug/\n","  inflating: /content/tools/moses/bin/gcc-7/debug/empty_test_static.o  \n","  inflating: /content/tools/moses/bin/gcc-7/debug/empty_test_static  \n","  inflating: /content/tools/moses/bin/gcc-7/debug/empty_test_shared.o  \n","  inflating: /content/tools/moses/bin/gcc-7/debug/empty_test_shared  \n","  inflating: /content/tools/moses/bin/pcfg-score  \n","  inflating: /content/tools/moses/bin/prunePhraseTable  \n","  inflating: /content/tools/moses/bin/pro  \n","  inflating: /content/tools/moses/bin/pcfg-extract  \n","  inflating: /content/tools/moses/bin/lmplz  \n","  inflating: /content/tools/moses/bin/config.log  \n","  inflating: /content/tools/moses/bin/sentence-bleu-nbest  \n","  inflating: /content/tools/moses/bin/generateSequences  \n","  inflating: /content/tools/moses/bin/extract-lex  \n","  inflating: /content/tools/moses/bin/hgdecode  \n","  inflating: /content/tools/moses/bin/moses_chart  \n","  inflating: /content/tools/moses/bin/extract-ghkm  \n","  inflating: /content/tools/moses/bin/CreateOnDiskPt  \n","  inflating: /content/tools/moses/bin/consolidate  \n","  inflating: /content/tools/moses/bin/evaluator  \n","  inflating: /content/tools/moses/bin/queryOnDiskPt  \n","  inflating: /content/tools/moses/bin/fragment  \n","  inflating: /content/tools/moses/bin/lexical-reordering-score  \n","  inflating: /content/tools/moses/bin/query  \n","  inflating: /content/tools/moses/bin/queryLexicalTable  \n","  inflating: /content/tools/moses/bin/mert  \n","  inflating: /content/tools/moses/bin/biconcor  \n","   creating: /content/tools/moses/scripts/\n","   creating: /content/tools/moses/scripts/ems/\n","  inflating: /content/tools/moses/scripts/ems/fix-info.perl  \n","   creating: /content/tools/moses/scripts/ems/example/\n","  inflating: /content/tools/moses/scripts/ems/example/config.factored  \n","  inflating: /content/tools/moses/scripts/ems/example/config.toy.bilinguallm  \n","  inflating: /content/tools/moses/scripts/ems/example/config.toy  \n","   creating: /content/tools/moses/scripts/ems/example/data/\n","  inflating: /content/tools/moses/scripts/ems/example/data/run.hierarchical.sh  \n","  inflating: /content/tools/moses/scripts/ems/example/data/test-ref.en.sgm  \n","  inflating: /content/tools/moses/scripts/ems/example/data/nc-5k.en  \n","  inflating: /content/tools/moses/scripts/ems/example/data/weight_bilinguallm.ini  \n","  inflating: /content/tools/moses/scripts/ems/example/data/nc-5k.fr  \n","  inflating: /content/tools/moses/scripts/ems/example/data/weight.ini  \n","  inflating: /content/tools/moses/scripts/ems/example/data/test-src.fr.sgm  \n","  inflating: /content/tools/moses/scripts/ems/example/data/config.hierarchical  \n","  inflating: /content/tools/moses/scripts/ems/example/config.basic.moses2  \n","  inflating: /content/tools/moses/scripts/ems/example/config.syntax  \n","  inflating: /content/tools/moses/scripts/ems/example/config.hierarchical  \n","  inflating: /content/tools/moses/scripts/ems/example/config.basic  \n","   creating: /content/tools/moses/scripts/ems/web/\n","  inflating: /content/tools/moses/scripts/ems/web/diff.php  \n"," extracting: /content/tools/moses/scripts/ems/web/general.css  \n","  inflating: /content/tools/moses/scripts/ems/web/lib.php  \n","  inflating: /content/tools/moses/scripts/ems/web/bilingual-concordance.css  \n","  inflating: /content/tools/moses/scripts/ems/web/analysis.php  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/effects.js  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/\n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/run_unit_tests.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/run_functional_tests.html  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/unittest_test.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_inplaceeditor_text.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_inplaceeditor_tagged.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_autocomplete_result_nobr.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_inplaceeditor_ipce_alt_text.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/slider_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/element_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/bdd_test.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_inplaceeditor_result.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/icon.png  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/loading_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/sortable_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_autocomplete_result.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/dragdrop_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/ajax_autocompleter_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_inplaceeditor_result2.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/position_clone_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/ajax_inplaceeditor_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/builder_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/effects_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/_ajax_updater_result.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/index.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/unit/string_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/test.css  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop7_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_queue_limit_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop8_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects3_test.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/_ajax_inplaceeditor_text.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/ajax_inplacecollectioneditor_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects5b_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_grow_strink_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sound_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/slider_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effect_scale_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop2_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable3_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effect_shake.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sword.mp3  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable4_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_highlight_bg_image.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effect_direct_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop9_test.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/_ajax_inplaceeditor_result.html  \n"," extracting: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/icon.png  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects2_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_float_appear_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable_tree_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable6_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effect_puff_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable2_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable_nested_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/texteffects_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/ajax_autocompleter2_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_blind_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_random_demo.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop4_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop3_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/_autocomplete_result_single.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects5_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/_autocomplete_result.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/sortable5_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop6_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_queue_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/ajax_autocompleter_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/position_clone_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/ajax_inplaceeditor_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dropmarker.png  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop_delay_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/dragdrop5_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects6_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects_toggle_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/index.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/test/functional/effects4_test.html  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/CHANGELOG  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/README.rdoc  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/MIT-LICENSE  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/lib/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/lib/prototype.js  \n","   creating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/\n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/effects.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/controls.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/sound.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/builder.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/scriptaculous.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/dragdrop.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/slider.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous-js-1.8.3/src/unittest.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/controls.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/sound.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/builder.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/scriptaculous.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/prototype.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/dragdrop.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/slider.js  \n","  inflating: /content/tools/moses/scripts/ems/web/javascripts/unittest.js  \n","  inflating: /content/tools/moses/scripts/ems/web/base64.js  \n","  inflating: /content/tools/moses/scripts/ems/web/hierarchical-segmentation.js  \n","  inflating: /content/tools/moses/scripts/ems/web/hierarchical-segmentation.css  \n","  inflating: /content/tools/moses/scripts/ems/web/comment.php  \n","  inflating: /content/tools/moses/scripts/ems/web/progress.perl  \n","  inflating: /content/tools/moses/scripts/ems/web/index.php  \n","  inflating: /content/tools/moses/scripts/ems/web/sgviz.js  \n","  inflating: /content/tools/moses/scripts/ems/web/sgviz.php  \n","  inflating: /content/tools/moses/scripts/ems/web/spinner.gif  \n","  inflating: /content/tools/moses/scripts/ems/web/setup  \n","  inflating: /content/tools/moses/scripts/ems/web/close.gif  \n","  inflating: /content/tools/moses/scripts/ems/web/overview.php  \n","  inflating: /content/tools/moses/scripts/ems/web/analysis_diff.php  \n","  inflating: /content/tools/moses/scripts/ems/web/favicon.ico  \n","  inflating: /content/tools/moses/scripts/ems/experiment.meta  \n","  inflating: /content/tools/moses/scripts/ems/experiment.machines  \n","  inflating: /content/tools/moses/scripts/ems/experiment.perl  \n","   creating: /content/tools/moses/scripts/ems/support/\n","  inflating: /content/tools/moses/scripts/ems/support/input-from-sgm.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/berkeley-train.sh  \n","  inflating: /content/tools/moses/scripts/ems/support/submit-grid.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/symmetrize-fast-align.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/lmplz-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/berkeley-process.sh  \n","  inflating: /content/tools/moses/scripts/ems/support/defaultconfig.py  \n","  inflating: /content/tools/moses/scripts/ems/support/prepare-fast-align.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/mml-score.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/interpolate-lm.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/fast-align-in-parts.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/build-sparse-features.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/tree-converter-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/substitute-filtered-tables-and-weights.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/substitute-weights.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/run-wade.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/run-command-on-multiple-refsets.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/generic-multicore-parallelizer.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/analysis.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/report-experiment-scores.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/split-sentences.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/cache-model.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/thot-lm-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/build-domain-file-from-subcorpora.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/mml-train.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/consolidate-training-data.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/mml-filter.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/generic-parallelizer.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/substitute-filtered-tables.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/mml-filter.py  \n","  inflating: /content/tools/moses/scripts/ems/support/remove-segmentation-markup.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/ter.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/reference-from-sgm.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/create-xml.perl  \n","  inflating: /content/tools/moses/scripts/ems/support/wrap-xml.perl  \n","   creating: /content/tools/moses/scripts/nbest-rescore/\n","  inflating: /content/tools/moses/scripts/nbest-rescore/topbest.py  \n","  inflating: /content/tools/moses/scripts/nbest-rescore/train.py  \n","  inflating: /content/tools/moses/scripts/nbest-rescore/README.md  \n","  inflating: /content/tools/moses/scripts/nbest-rescore/rescore.py  \n","  inflating: /content/tools/moses/scripts/README  \n","   creating: /content/tools/moses/scripts/docker/\n","  inflating: /content/tools/moses/scripts/docker/Dockerfile.ubuntu.fastlightpbmt  \n","  inflating: /content/tools/moses/scripts/docker/Dockerfile.ubuntu.basic  \n","   creating: /content/tools/moses/scripts/tokenizer/\n","  inflating: /content/tools/moses/scripts/tokenizer/lowercase.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/remove-non-printing-char.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/deescape-special-chars-PTB.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/pre-tokenizer.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/detokenizer.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/delete-long-words.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/normalize-punctuation.perl  \n","   creating: /content/tools/moses/scripts/tokenizer/mosestokenizer/\n","  inflating: /content/tools/moses/scripts/tokenizer/mosestokenizer/sentsplitter.py  \n","  inflating: /content/tools/moses/scripts/tokenizer/mosestokenizer/punctnormalizer.py  \n","  inflating: /content/tools/moses/scripts/tokenizer/mosestokenizer/tokenizer.py  \n","  inflating: /content/tools/moses/scripts/tokenizer/mosestokenizer/__init__.py  \n","  inflating: /content/tools/moses/scripts/tokenizer/mosestokenizer/detokenizer.py  \n","  inflating: /content/tools/moses/scripts/tokenizer/escape-special-chars.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/pre-tok-clean.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/tokenizer_PTB.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/replace-unicode-punctuation.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/deescape-special-chars.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/basic-protected-patterns  \n","  inflating: /content/tools/moses/scripts/tokenizer/tokenizer.perl  \n","  inflating: /content/tools/moses/scripts/tokenizer/pre_tokenize_cleaning.py  \n","   creating: /content/tools/moses/scripts/fuzzy-match/\n","  inflating: /content/tools/moses/scripts/fuzzy-match/.cproject  \n","  inflating: /content/tools/moses/scripts/fuzzy-match/create_xml.perl  \n","  inflating: /content/tools/moses/scripts/fuzzy-match/.project  \n","   creating: /content/tools/moses/scripts/analysis/\n","  inflating: /content/tools/moses/scripts/analysis/show-phrases-used.pl  \n","  inflating: /content/tools/moses/scripts/analysis/weight-scan-summarize.sh  \n","  inflating: /content/tools/moses/scripts/analysis/oov.pl  \n","  inflating: /content/tools/moses/scripts/analysis/README  \n","  inflating: /content/tools/moses/scripts/analysis/bootstrap-hypothesis-difference-significance.pl  \n","   creating: /content/tools/moses/scripts/analysis/perllib/\n","  inflating: /content/tools/moses/scripts/analysis/perllib/Error.pm  \n","  inflating: /content/tools/moses/scripts/analysis/extract-target-trees.py  \n","  inflating: /content/tools/moses/scripts/analysis/suspicious_tokenization.pl  \n","   creating: /content/tools/moses/scripts/analysis/smtgui/\n","  inflating: /content/tools/moses/scripts/analysis/smtgui/README  \n","  inflating: /content/tools/moses/scripts/analysis/smtgui/newsmtgui.cgi  \n","  inflating: /content/tools/moses/scripts/analysis/smtgui/Corpus.pm  \n","  inflating: /content/tools/moses/scripts/analysis/smtgui/filter-phrase-table.pl  \n","  inflating: /content/tools/moses/scripts/analysis/smtgui/file-factors  \n","  inflating: /content/tools/moses/scripts/analysis/smtgui/file-descriptions  \n","  inflating: /content/tools/moses/scripts/analysis/nontranslated_words.pl  \n","  inflating: /content/tools/moses/scripts/analysis/weight-scan.pl  \n","  inflating: /content/tools/moses/scripts/analysis/sg2dot.perl  \n","  inflating: /content/tools/moses/scripts/analysis/sentence-by-sentence.pl  \n","   creating: /content/tools/moses/scripts/regression-testing/\n","  inflating: /content/tools/moses/scripts/regression-testing/run-single-test.pl  \n","   creating: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/\n","   creating: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/models/\n"," extracting: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/models/reordering-table.msd-bidirectional-fe.0.5.0-0.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/models/phrase-table.0-0.gz  \n","   creating: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/lm/\n","  inflating: /content/tools/moses/scripts/regression-testing/moses-scripts-reg-test-data-1.0/lm/train.en.w5.msb.nops.blm  \n","  inflating: /content/tools/moses/scripts/regression-testing/create_localized_moses_ini.pl  \n","  inflating: /content/tools/moses/scripts/regression-testing/MosesScriptsRegressionTesting.pm  \n","  inflating: /content/tools/moses/scripts/regression-testing/moses-virtual.pl  \n","  inflating: /content/tools/moses/scripts/regression-testing/moses-virtual  \n","  inflating: /content/tools/moses/scripts/regression-testing/run-test-suite.pl  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/\n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/input  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/reference.2  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/reference.0  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/command  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/\n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run2.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run5.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run11.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run11.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run6.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run14.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run13.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run5.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run7.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run4.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run1.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run1.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run9.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run9.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run8.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run7.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run14.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/archive.list  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run10.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run3.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run6.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run12.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run2.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run10.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run8.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run4.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run13.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run3.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/data/run12.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/moses.ini  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/truth/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/truth/results.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/filter-stdout  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/filter-stderr  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-aggregate/reference.1  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/input  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/reference.2  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/reference.0  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/command  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/\n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run2.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run5.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run6.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run5.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run7.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run4.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run1.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run1.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run7.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/archive.list  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run3.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run6.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run2.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run4.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/data/run3.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/moses.ini  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/truth/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/truth/results.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/filter-stdout  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/filter-stderr  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new/reference.1  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/input  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/reference.2  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/reference.0  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/command  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run2.features.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run6.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run4.features.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run5.scores.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run3.scores.dat  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run7.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/finished_step.txt  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run1.scores.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run5.names.txt  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/actual.index  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run7.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run1.features.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/archive.list  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run3.features.dat  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run6.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run5.mert.log  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run4.scores.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run5.features.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run2.scores.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/data/run5.weights.txt  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/moses.ini  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/truth/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/truth/results.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/filter-stdout  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/filter-stderr  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-continue/reference.1  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/input  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/reference.2  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/reference.0  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/command  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/\n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run2.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run5.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run5.NBEST.out.gz  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run4.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run1.TRANS.out  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run1.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/archive.list  \n"," extracting: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run3.NBEST.out.gz  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run2.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run4.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/data/run3.TRANS.out  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/moses.ini  \n","   creating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/truth/\n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/truth/results.dat  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/filter-stdout  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/filter-stderr  \n","  inflating: /content/tools/moses/scripts/regression-testing/tests/mert-moses-new-nocase/reference.1  \n","  inflating: /content/tools/moses/scripts/regression-testing/modify-pars.pl  \n","  inflating: /content/tools/moses/scripts/regression-testing/compare-results.pl  \n","   creating: /content/tools/moses/scripts/other/\n","  inflating: /content/tools/moses/scripts/other/retain-lines.perl  \n","  inflating: /content/tools/moses/scripts/other/delete-scores.perl  \n","  inflating: /content/tools/moses/scripts/other/gacha_filter.py  \n","  inflating: /content/tools/moses/scripts/other/translate_by_microsoft_bing.perl  \n","  inflating: /content/tools/moses/scripts/other/beautify.py  \n","  inflating: /content/tools/moses/scripts/other/convert-pt.perl  \n","  inflating: /content/tools/moses/scripts/other/get_many_translations_from_google.perl  \n","  inflating: /content/tools/moses/scripts/other/buckwalter.perl  \n","  inflating: /content/tools/moses/scripts/other/blame-stat.sh  \n","   creating: /content/tools/moses/scripts/generic/\n","  inflating: /content/tools/moses/scripts/generic/lopar2pos.pl  \n","  inflating: /content/tools/moses/scripts/generic/reverse-alignment.perl  \n","  inflating: /content/tools/moses/scripts/generic/generic-parallel.perl  \n","  inflating: /content/tools/moses/scripts/generic/qsub-wrapper.pl  \n","  inflating: /content/tools/moses/scripts/generic/ph_numbers.perl  \n","  inflating: /content/tools/moses/scripts/generic/mteval-v14.pl  \n","  inflating: /content/tools/moses/scripts/generic/fsal2fsa.pl  \n","  inflating: /content/tools/moses/scripts/generic/strip-xml.perl  \n","  inflating: /content/tools/moses/scripts/generic/fsa2plf.pl  \n","  inflating: /content/tools/moses/scripts/generic/binarize4moses2.perl  \n","  inflating: /content/tools/moses/scripts/generic/mteval-v11b.pl  \n","  inflating: /content/tools/moses/scripts/generic/mteval-v13a.pl  \n","  inflating: /content/tools/moses/scripts/generic/mteval-v12.pl  \n","  inflating: /content/tools/moses/scripts/generic/moses-parallel.pl  \n","  inflating: /content/tools/moses/scripts/generic/compound-splitter.perl  \n","  inflating: /content/tools/moses/scripts/generic/extract-parallel.perl  \n","  inflating: /content/tools/moses/scripts/generic/score-parallel.perl  \n","  inflating: /content/tools/moses/scripts/generic/fsa2fsal.pl  \n","  inflating: /content/tools/moses/scripts/generic/fsa-sample.fsa  \n","  inflating: /content/tools/moses/scripts/generic/multi_moses.py  \n","  inflating: /content/tools/moses/scripts/generic/extract-factors.pl  \n","  inflating: /content/tools/moses/scripts/generic/multi-bleu-detok.perl  \n","  inflating: /content/tools/moses/scripts/generic/score_parallel.py  \n","  inflating: /content/tools/moses/scripts/generic/bsbleu.py  \n","  inflating: /content/tools/moses/scripts/generic/giza-parallel.perl  \n","  inflating: /content/tools/moses/scripts/generic/multi-bleu.perl  \n","  inflating: /content/tools/moses/scripts/generic/moses_sim_pe.py  \n","  inflating: /content/tools/moses/scripts/generic/trainlm-irst2.perl  \n","   creating: /content/tools/moses/scripts/share/\n","   creating: /content/tools/moses/scripts/share/nonbreaking_prefixes/\n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.en  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.zh  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ta  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.pa  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.bn  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.cs  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.de  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.is  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.el  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.sk  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.sv  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.nl  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.it  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.or  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.lv  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/README.txt  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.yue  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.hi  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.gu  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.mr  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.fr  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.et  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ru  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ro  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.as  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.pl  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.te  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.pt  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.lt  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.kn  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ml  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.hu  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.mni  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.sl  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.fi  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ga  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.es  \n","  inflating: /content/tools/moses/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.ca  \n","   creating: /content/tools/moses/scripts/training/\n","  inflating: /content/tools/moses/scripts/training/reduce-topt-count.pl  \n","  inflating: /content/tools/moses/scripts/training/clean-corpus-n.perl  \n","  inflating: /content/tools/moses/scripts/training/clean-corpus-n-ratio.perl  \n","  inflating: /content/tools/moses/scripts/training/mert-moses.pl  \n","  inflating: /content/tools/moses/scripts/training/reduce-factors.perl  \n","  inflating: /content/tools/moses/scripts/training/giza2bal.pl  \n","  inflating: /content/tools/moses/scripts/training/combine_factors.pl  \n","  inflating: /content/tools/moses/scripts/training/filter-rule-table.py  \n","  inflating: /content/tools/moses/scripts/training/train-model.perl  \n","  inflating: /content/tools/moses/scripts/training/strip-xml.perl  \n","  inflating: /content/tools/moses/scripts/training/flexibility_score.py  \n","  inflating: /content/tools/moses/scripts/training/clone_moses_model.pl  \n","  inflating: /content/tools/moses/scripts/training/reduce_combine.pl  \n","  inflating: /content/tools/moses/scripts/training/analyse_moses_model.pl  \n","  inflating: /content/tools/moses/scripts/training/filter-model-given-input.pl  \n","  inflating: /content/tools/moses/scripts/training/remove-orphan-phrase-pairs-from-reordering-table.perl  \n","  inflating: /content/tools/moses/scripts/training/build-mmsapt.perl  \n","  inflating: /content/tools/moses/scripts/training/exodus.perl  \n","  inflating: /content/tools/moses/scripts/training/get-lexical.perl  \n","  inflating: /content/tools/moses/scripts/training/train-global-lexicon-model.perl  \n","  inflating: /content/tools/moses/scripts/training/combine_factors_syntax.pl  \n","  inflating: /content/tools/moses/scripts/training/corpus-sizes.perl  \n","   creating: /content/tools/moses/scripts/training/wrappers/\n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-de-pos.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/berkeleyparsed2mosesxml_PTB.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-en-egret.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-en-pos.mxpost.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/conll2mosesxml.py  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-en-senna.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/filter-excluded-lines.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/mosesxml2brackets.py  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-de-morph.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-en-stanford.py  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-pos.tree-tagger.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/mada-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/senna2brackets.py  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-de-berkeley.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-brown-cluster-mkcls.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/tagger-german-chunk.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/morfessor-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/berkeleyparsed2mosesxml.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-en-porter.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-suffix.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-en-bllip.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/madamira-wrapper.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/mosesxml2berkeleyparsed.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/tree-converter-mosesxml.sh  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-stem.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/madamira-tok.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/syntax-hyphen-splitting.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-en-collins.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/find-unparseable.perl  \n","  inflating: /content/tools/moses/scripts/training/wrappers/parse-de-bitpar.perl  \n","   creating: /content/tools/moses/scripts/training/wrappers/adam-suffix-array/\n","  inflating: /content/tools/moses/scripts/training/wrappers/adam-suffix-array/suffix-array-create.sh  \n","  inflating: /content/tools/moses/scripts/training/wrappers/adam-suffix-array/suffix-array-extract.sh  \n","  inflating: /content/tools/moses/scripts/training/wrappers/make-factor-de-lemma.perl  \n","  inflating: /content/tools/moses/scripts/training/create_count_tables.py  \n"," extracting: /content/tools/moses/scripts/training/Jamfile  \n","  inflating: /content/tools/moses/scripts/training/binarize-model.perl  \n","  inflating: /content/tools/moses/scripts/training/convert-moses-ini-to-v2.perl  \n","  inflating: /content/tools/moses/scripts/training/threshold-filter.perl  \n","  inflating: /content/tools/moses/scripts/training/postprocess-lopar.perl  \n","  inflating: /content/tools/moses/scripts/training/absolutize_moses_model.pl  \n","  inflating: /content/tools/moses/scripts/training/LexicalTranslationModel.pm  \n","  inflating: /content/tools/moses/scripts/training/train-neurallm.py  \n","  inflating: /content/tools/moses/scripts/training/build-generation-table.perl  \n","  inflating: /content/tools/moses/scripts/training/create_nplm_ini.py  \n","   creating: /content/tools/moses/scripts/training/rdlm/\n","  inflating: /content/tools/moses/scripts/training/rdlm/README  \n","  inflating: /content/tools/moses/scripts/training/rdlm/average_null_embedding.py  \n","  inflating: /content/tools/moses/scripts/training/rdlm/extract_vocab.py  \n","  inflating: /content/tools/moses/scripts/training/rdlm/extract_syntactic_ngrams.py  \n","  inflating: /content/tools/moses/scripts/training/rdlm/train_rdlm.py  \n","  inflating: /content/tools/moses/scripts/training/wrap_moses.py  \n","   creating: /content/tools/moses/scripts/training/bilingual-lm/\n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/train_nplm.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/extract_test.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/README  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/averageNullEmbedding.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/extract.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/extract_training.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/reduce_ngrams.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/test_nplm.py  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/tag.sh  \n","  inflating: /content/tools/moses/scripts/training/bilingual-lm/create_blm_ini.py  \n","   creating: /content/tools/moses/scripts/OSM/\n","  inflating: /content/tools/moses/scripts/OSM/flipAlignment.perl  \n","  inflating: /content/tools/moses/scripts/OSM/extract-singletons.perl  \n","  inflating: /content/tools/moses/scripts/OSM/OSM-Train.perl  \n","   creating: /content/tools/moses/scripts/tests/\n","  inflating: /content/tools/moses/scripts/tests/README  \n","   creating: /content/tools/moses/scripts/tests/cs-en-sample/\n","  inflating: /content/tools/moses/scripts/tests/cs-en-sample/lm.en.gz  \n","  inflating: /content/tools/moses/scripts/tests/cs-en-sample/test.src  \n","  inflating: /content/tools/moses/scripts/tests/cs-en-sample/test.ref  \n","  inflating: /content/tools/moses/scripts/tests/cs-en-sample/train.en  \n","  inflating: /content/tools/moses/scripts/tests/cs-en-sample/train.cs  \n","  inflating: /content/tools/moses/scripts/tests/train-factored-test-step9.test  \n","  inflating: /content/tools/moses/scripts/tests/full-train-mert-decode.test  \n","   creating: /content/tools/moses/scripts/server/\n","  inflating: /content/tools/moses/scripts/server/moses.py  \n","  inflating: /content/tools/moses/scripts/server/sim-pe.py  \n","  inflating: /content/tools/moses/scripts/Jamfile  \n","   creating: /content/tools/moses/scripts/Transliteration/\n","  inflating: /content/tools/moses/scripts/Transliteration/corpusCreator.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/in-decoding-transliteration.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/clean.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/train-transliteration-module.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/post-decoding-transliteration.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/threshold.pl  \n","  inflating: /content/tools/moses/scripts/Transliteration/prepare-transliteration-phrase-table.pl  \n","   creating: /content/tools/moses/scripts/recaser/\n","  inflating: /content/tools/moses/scripts/recaser/recase.perl  \n","  inflating: /content/tools/moses/scripts/recaser/train-truecaser.perl  \n","  inflating: /content/tools/moses/scripts/recaser/train-recaser.perl  \n","  inflating: /content/tools/moses/scripts/recaser/detruecase.perl  \n","  inflating: /content/tools/moses/scripts/recaser/truecase.perl  \n","\n","real\t0m4.765s\n","user\t0m1.985s\n","sys\t0m0.273s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5XgmMj2k9Kcf"},"source":["import os\n","HOME = '/content/drive/MyDrive/nmt_models/Alignment2'\n","os.chdir(HOME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIxQFck79LqV"},"source":["MODEL = 'vi2en-align0vl-hmedian-l4-alignment-lambda50p'\n","SOURCE = 'vi'\n","TARGET = 'en'\n","asuffix='gdalign0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7EGoopkslE9"},"source":["MODEL = 'vi2en-align0vl-h7-l4-alignment-lambda50p'\n","SOURCE = 'vi'\n","TARGET = 'en'\n","asuffix='gdalign0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzRYvfY2c74D"},"source":["MODEL = 'vi2en-align0vl-h8average-l4-alignment-lambda50p'\n","SOURCE = 'vi'\n","TARGET = 'en'\n","asuffix='gdalign0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wi2GWVN_nJ10"},"source":["MODEL = 'wovi2en-align0wl-h8average-l4-alignment-lambda50p'\n","SOURCE = 'wovi'\n","TARGET = 'en'\n","asuffix='gdalign0wl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MN7ijvhvQmwd"},"source":["MODEL = 'en2wovi-align0lw-h1-l4-alignment-lambda5p'\n","SOURCE = 'en'\n","TARGET = 'wovi'\n","asuffix='gdalign0lw'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIZrM6K3LujU"},"source":["##Repeat the experiment for the baseline model"]},{"cell_type":"code","metadata":{"id":"HBe-S_KiLtWw"},"source":["MODEL = 'en2wovi-align0lw-h1-l4-alignment-lambda5p_2'\n","SOURCE = 'en'\n","TARGET = 'wovi'\n","asuffix='gdalign0lw'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA8NfE_Heats"},"source":["MODEL = 'en2wovi-align0lw-h1-l4-alignment-lambda50p'\n","SOURCE = 'en'\n","TARGET = 'wovi'\n","asuffix='gdalign0lw'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cT8qKhq5wHKl"},"source":["MODEL = 'en2wovi-align0lw-h8average-l4-alignment-lambda5p'\n","SOURCE = 'en'\n","TARGET = 'wovi'\n","asuffix='gdalign0lw'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-YGUfnA6Kv0"},"source":["MODEL = 'en2wovi-align0lw-h8average-l4-alignment-lambda50p'\n","SOURCE = 'en'\n","TARGET = 'wovi'\n","asuffix='gdalign0lw'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Hq6VdRmTE_y9","executionInfo":{"status":"ok","timestamp":1623416919456,"user_tz":-420,"elapsed":522,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"cd42a7fe-7905-49e0-a9ee-c7c8658ae335"},"source":["MODEL"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'en2wovi-align0lw-h1-l4-alignment-lambda5p_2'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Utnsbb4DOjdH","executionInfo":{"status":"ok","timestamp":1623416924128,"user_tz":-420,"elapsed":589,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"abf28aab-2cde-428a-e6a9-f7d0b2c24b4f"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/nmt_models/Alignment2'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPPblWDKccx1","executionInfo":{"status":"ok","timestamp":1622216121864,"user_tz":-420,"elapsed":9,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"8e59674a-8974-4f2f-8c52-aa6314c76140"},"source":["!sed -n \"965p\" </content/fairseq/fairseq/models/transformer.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/bin/bash: /content/fairseq/fairseq/models/transformer.py: No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uO3be_pHRJZO","executionInfo":{"status":"ok","timestamp":1622294193118,"user_tz":-420,"elapsed":856,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"446dd72e-9514-45e7-f355-930fc18858df"},"source":["!unzip /content/drive/MyDrive/tools/fast_align.zip -d /"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/tools/fast_align.zip\n","  inflating: /content/fast_align/build/fast_align  \n","  inflating: /content/fast_align/build/atools  \n","  inflating: /content/fast_align/build/force_align.py  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nsq3PhlcROvI"},"source":["%%shell\n","ALIGN=/content/fast_align/build/fast_align\n","paste train.leen train.wovi | awk -F '\\t' '{print $1 \" ||| \" $2}' > train.leen-wovi\n","$ALIGN -i train.leen-wovi -d -o -v > train.align0lw\n","$ALIGN -i train.leen-wovi -d -o -v -r > train.ralign0lw\n","/content/fast_align/build/atools -i train.align0lw -j train.ralign0lw -c grow-diag > train.gdalign0lw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAOu4J_zEpri","executionInfo":{"status":"ok","timestamp":1623417025902,"user_tz":-420,"elapsed":36725,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"fb69647e-8548-469d-b7d1-39a100bea262"},"source":["!time fairseq-preprocess --source-lang \"$SOURCE\" --target-lang \"$TARGET\" \\\n","    --trainpref train --validpref dev --testpref test \\\n","    --align-suffix \"$asuffix\" \\\n","    --joined-dictionary \\\n","    --destdir \"$MODEL\"/binarized"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-11 13:09:53 | INFO | fairseq_cli.preprocess | Namespace(align_suffix='gdalign0lw', alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict=None, target_lang='wovi', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='train', user_dir=None, validpref='dev', workers=1)\n","2021-06-11 13:10:01 | INFO | fairseq_cli.preprocess | [en] Dictionary: 56792 types\n","2021-06-11 13:10:06 | INFO | fairseq_cli.preprocess | [en] train.en: 42026 sents, 848482 tokens, 0.0% replaced by <unk>\n","2021-06-11 13:10:06 | INFO | fairseq_cli.preprocess | [en] Dictionary: 56792 types\n","2021-06-11 13:10:07 | INFO | fairseq_cli.preprocess | [en] dev.en: 1482 sents, 27797 tokens, 3.53% replaced by <unk>\n","2021-06-11 13:10:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 56792 types\n","2021-06-11 13:10:08 | INFO | fairseq_cli.preprocess | [en] test.en: 1527 sents, 33040 tokens, 3.6% replaced by <unk>\n","2021-06-11 13:10:08 | INFO | fairseq_cli.preprocess | [wovi] Dictionary: 56792 types\n","2021-06-11 13:10:13 | INFO | fairseq_cli.preprocess | [wovi] train.wovi: 42026 sents, 908201 tokens, 0.0% replaced by <unk>\n","2021-06-11 13:10:13 | INFO | fairseq_cli.preprocess | [wovi] Dictionary: 56792 types\n","2021-06-11 13:10:15 | INFO | fairseq_cli.preprocess | [wovi] dev.wovi: 1482 sents, 30103 tokens, 2.26% replaced by <unk>\n","2021-06-11 13:10:15 | INFO | fairseq_cli.preprocess | [wovi] Dictionary: 56792 types\n","2021-06-11 13:10:16 | INFO | fairseq_cli.preprocess | [wovi] test.wovi: 1527 sents, 35402 tokens, 2.44% replaced by <unk>\n","2021-06-11 13:10:25 | INFO | fairseq_cli.preprocess | [alignments] train.gdalign0lw: parsed 42026 alignments\n","2021-06-11 13:10:25 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized\n","\n","real\t0m36.639s\n","user\t0m26.166s\n","sys\t0m0.522s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K2I8vuRGiutB"},"source":["##Train baseline with slignment-lambda=0.05 and 1 heads"]},{"cell_type":"code","metadata":{"id":"LDhkscH9Sdmx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623432450979,"user_tz":-420,"elapsed":1306626,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"e20702ae-7ab1-45f7-f437-d866e5b4ec97"},"source":["!time fairseq-train --alignment-heads 1\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.05\\\n","    --fp16"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-11 17:05:48 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, alignment_heads=1, alignment_lambda=0.05, alignment_layer=4, all_gather_list_size=16384, arch='transformer_align', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_with_alignment', cross_self_attention=False, curriculum=0, data='en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, full_context_alignment=False, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=True, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n","2021-06-11 17:05:50 | INFO | fairseq.tasks.translation | [en] dictionary: 56792 types\n","2021-06-11 17:05:50 | INFO | fairseq.tasks.translation | [wovi] dictionary: 56792 types\n","2021-06-11 17:05:50 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized/valid.en-wovi.en\n","2021-06-11 17:05:51 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized/valid.en-wovi.wovi\n","2021-06-11 17:05:51 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized valid en-wovi 1482 examples\n","2021-06-11 17:05:52 | INFO | fairseq_cli.train | TransformerAlignModel(\n","  (encoder): TransformerEncoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=512, out_features=56792, bias=False)\n","  )\n",")\n","2021-06-11 17:05:52 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n","2021-06-11 17:05:52 | INFO | fairseq_cli.train | model: transformer_align (TransformerAlignModel)\n","2021-06-11 17:05:52 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_with_alignment (LabelSmoothedCrossEntropyCriterionWithAlignment)\n","2021-06-11 17:05:52 | INFO | fairseq_cli.train | num. model params: 73216000 (num. trained: 73216000)\n","2021-06-11 17:06:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n","2021-06-11 17:06:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n","2021-06-11 17:06:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-06-11 17:06:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2021-06-11 17:06:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-06-11 17:06:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2021-06-11 17:06:03 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n","2021-06-11 17:06:29 | INFO | fairseq.trainer | loaded checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint_last.pt (epoch 13 @ 3950 updates)\n","2021-06-11 17:06:29 | INFO | fairseq.trainer | loading train data for epoch 13\n","2021-06-11 17:06:30 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized/train.en-wovi.en\n","2021-06-11 17:06:32 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized/train.en-wovi.wovi\n","2021-06-11 17:06:32 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized train en-wovi 42026 examples\n","2021-06-11 17:06:33 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda5p_2/binarized/train.align.en-wovi\n","epoch 013:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:06:33 | INFO | fairseq.trainer | begin training epoch 13\n","epoch 013:  38% 124/330 [00:19<00:31,  6.46it/s, loss=7.59, nll_loss=6.332, alignment_loss=2.362, ppl=80.58, wps=906.3, ups=0.33, wpb=2755.8, bsz=128.5, num_updates=4000, lr=0.0002, gnorm=1.831, loss_scale=0.125, train_wall=2880, wall=0]2021-06-11 17:06:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\n","epoch 013: 100% 329/330 [00:51<00:00,  6.32it/s, loss=7.59, nll_loss=6.332, alignment_loss=2.362, ppl=80.58, wps=906.3, ups=0.33, wpb=2755.8, bsz=128.5, num_updates=4000, lr=0.0002, gnorm=1.831, loss_scale=0.125, train_wall=2880, wall=0]2021-06-11 17:07:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 013 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  9.38it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  20% 3/15 [00:00<00:01, 11.11it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 13.17it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 15.05it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 16.89it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 18.85it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:07:26 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.363 | nll_loss 5.978 | alignment_loss 0 | ppl 63.02 | wps 46777.8 | wpb 2006.9 | bsz 98.8 | num_updates 4279 | best_loss 7.363\n","2021-06-11 17:07:26 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:08:48 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint13.pt (epoch 13 @ 4279 updates, score 7.363) (writing took 82.20154244899999 seconds)\n","2021-06-11 17:08:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2021-06-11 17:08:48 | INFO | train | epoch 013 | loss 7.157 | nll_loss 5.84 | alignment_loss 2.21 | ppl 57.29 | wps 1581.4 | ups 0.57 | wpb 2752.4 | bsz 127.5 | num_updates 4279 | lr 0.00019337 | gnorm 1.781 | loss_scale 0.0625 | train_wall 1044 | wall 0\n","epoch 014:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:08:48 | INFO | fairseq.trainer | begin training epoch 14\n","epoch 014: 100% 329/330 [00:53<00:00,  6.14it/s]2021-06-11 17:09:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 014 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  13% 2/15 [00:00<00:00, 13.47it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  33% 5/15 [00:00<00:00, 15.16it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  53% 8/15 [00:00<00:00, 16.88it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 18.12it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  93% 14/15 [00:00<00:00, 19.20it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:09:42 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.068 | nll_loss 5.64 | alignment_loss 0 | ppl 49.85 | wps 46022.7 | wpb 2006.9 | bsz 98.8 | num_updates 4609 | best_loss 7.068\n","2021-06-11 17:09:42 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:10:41 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint14.pt (epoch 14 @ 4609 updates, score 7.068) (writing took 58.996720857000014 seconds)\n","2021-06-11 17:10:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2021-06-11 17:10:41 | INFO | train | epoch 014 | loss 6.672 | nll_loss 5.289 | alignment_loss 2.027 | ppl 39.08 | wps 8039.5 | ups 2.92 | wpb 2752.1 | bsz 127.4 | num_updates 4609 | lr 0.000186319 | gnorm 1.737 | loss_scale 0.0625 | train_wall 52 | wall 0\n","epoch 015:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:10:41 | INFO | fairseq.trainer | begin training epoch 15\n","epoch 015: 100% 329/330 [00:53<00:00,  6.09it/s]2021-06-11 17:11:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 015 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  9.22it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  20% 3/15 [00:00<00:01, 10.94it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 12.98it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 14.79it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 16.47it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 18.46it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:11:35 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.861 | nll_loss 5.394 | alignment_loss 0 | ppl 42.04 | wps 45786.4 | wpb 2006.9 | bsz 98.8 | num_updates 4939 | best_loss 6.861\n","2021-06-11 17:11:35 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:12:35 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint15.pt (epoch 15 @ 4939 updates, score 6.861) (writing took 59.72055615700003 seconds)\n","2021-06-11 17:12:35 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2021-06-11 17:12:35 | INFO | train | epoch 015 | loss 6.373 | nll_loss 4.948 | alignment_loss 1.906 | ppl 30.87 | wps 7966.5 | ups 2.89 | wpb 2752.1 | bsz 127.4 | num_updates 4939 | lr 0.000179987 | gnorm 1.692 | loss_scale 0.0625 | train_wall 53 | wall 0\n","epoch 016:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:12:35 | INFO | fairseq.trainer | begin training epoch 16\n","epoch 016: 100% 329/330 [00:53<00:00,  6.15it/s, loss=6.633, nll_loss=5.244, alignment_loss=2.011, ppl=37.9, wps=7558.9, ups=2.75, wpb=2749.2, bsz=125.8, num_updates=5000, lr=0.000178885, gnorm=1.731, loss_scale=0.0625, train_wall=158, wall=0]2021-06-11 17:13:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 016 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  9.25it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  27% 4/15 [00:00<00:00, 11.15it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  47% 7/15 [00:00<00:00, 13.06it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 14.89it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  87% 13/15 [00:00<00:00, 16.53it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:13:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.754 | nll_loss 5.243 | alignment_loss 0 | ppl 37.88 | wps 45975.8 | wpb 2006.9 | bsz 98.8 | num_updates 5269 | best_loss 6.754\n","2021-06-11 17:13:30 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:14:28 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint16.pt (epoch 16 @ 5269 updates, score 6.754) (writing took 58.63554824000005 seconds)\n","2021-06-11 17:14:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2021-06-11 17:14:28 | INFO | train | epoch 016 | loss 6.126 | nll_loss 4.666 | alignment_loss 1.825 | ppl 25.39 | wps 8009 | ups 2.91 | wpb 2752.1 | bsz 127.4 | num_updates 5269 | lr 0.000174259 | gnorm 1.717 | loss_scale 0.0625 | train_wall 53 | wall 0\n","epoch 017:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:14:28 | INFO | fairseq.trainer | begin training epoch 17\n","epoch 017:  15% 50/330 [00:08<00:43,  6.37it/s]2021-06-11 17:14:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\n","epoch 017: 100% 329/330 [00:54<00:00,  6.13it/s]2021-06-11 17:15:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 017 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  9.94it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  20% 3/15 [00:00<00:01, 11.64it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 13.61it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 15.31it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 16.91it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 18.83it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:15:24 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.646 | nll_loss 5.108 | alignment_loss 0 | ppl 34.48 | wps 45392.1 | wpb 2006.9 | bsz 98.8 | num_updates 5598 | best_loss 6.646\n","2021-06-11 17:15:24 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:16:22 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint17.pt (epoch 17 @ 5598 updates, score 6.646) (writing took 58.521852775999946 seconds)\n","2021-06-11 17:16:22 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2021-06-11 17:16:22 | INFO | train | epoch 017 | loss 5.896 | nll_loss 4.405 | alignment_loss 1.751 | ppl 21.18 | wps 7958.2 | ups 2.89 | wpb 2753.5 | bsz 127.6 | num_updates 5598 | lr 0.000169061 | gnorm 1.664 | loss_scale 0.0312 | train_wall 53 | wall 0\n","epoch 018:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:16:22 | INFO | fairseq.trainer | begin training epoch 18\n","epoch 018:  18% 59/330 [00:09<00:42,  6.37it/s]2021-06-11 17:16:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625\n","epoch 018: 100% 329/330 [00:54<00:00,  5.93it/s]2021-06-11 17:17:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 018 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.57it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  27% 4/15 [00:00<00:01, 10.44it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  47% 7/15 [00:00<00:00, 12.33it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 14.20it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  87% 13/15 [00:00<00:00, 15.93it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:17:17 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.58 | nll_loss 4.971 | alignment_loss 0 | ppl 31.37 | wps 45524.5 | wpb 2006.9 | bsz 98.8 | num_updates 5927 | best_loss 6.58\n","2021-06-11 17:17:17 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:18:16 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint18.pt (epoch 18 @ 5927 updates, score 6.58) (writing took 58.8430939189999 seconds)\n","2021-06-11 17:18:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2021-06-11 17:18:16 | INFO | train | epoch 018 | loss 5.699 | nll_loss 4.147 | alignment_loss 1.691 | ppl 17.72 | wps 7957.7 | ups 2.89 | wpb 2752.9 | bsz 127.6 | num_updates 5927 | lr 0.000164302 | gnorm 1.637 | loss_scale 0.0156 | train_wall 53 | wall 0\n","epoch 019:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:18:16 | INFO | fairseq.trainer | begin training epoch 19\n","epoch 019:  80% 265/330 [00:43<00:10,  6.04it/s, loss=5.865, nll_loss=4.354, alignment_loss=1.742, ppl=20.45, wps=8030.8, ups=2.92, wpb=2754.3, bsz=128.3, num_updates=6000, lr=0.000163299, gnorm=1.667, loss_scale=0.0156, train_wall=162, wall=0]2021-06-11 17:18:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125\n","epoch 019: 100% 329/330 [00:53<00:00,  6.14it/s, loss=5.865, nll_loss=4.354, alignment_loss=1.742, ppl=20.45, wps=8030.8, ups=2.92, wpb=2754.3, bsz=128.3, num_updates=6000, lr=0.000163299, gnorm=1.667, loss_scale=0.0156, train_wall=162, wall=0]2021-06-11 17:19:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.88it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  20% 3/15 [00:00<00:01, 10.58it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 12.54it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 14.33it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 16.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 18.10it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:19:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.587 | nll_loss 4.994 | alignment_loss 0 | ppl 31.86 | wps 45133.7 | wpb 2006.9 | bsz 98.8 | num_updates 6256 | best_loss 6.58\n","2021-06-11 17:19:11 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:19:42 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint19.pt (epoch 19 @ 6256 updates, score 6.587) (writing took 31.44533724000007 seconds)\n","2021-06-11 17:19:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2021-06-11 17:19:42 | INFO | train | epoch 019 | loss 5.538 | nll_loss 3.937 | alignment_loss 1.644 | ppl 15.32 | wps 10525.8 | ups 3.82 | wpb 2754.1 | bsz 127.6 | num_updates 6256 | lr 0.000159923 | gnorm 1.618 | loss_scale 0.0078 | train_wall 53 | wall 0\n","epoch 020:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:19:43 | INFO | fairseq.trainer | begin training epoch 20\n","epoch 020:  10% 34/330 [00:05<00:46,  6.36it/s]2021-06-11 17:19:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00390625\n","epoch 020:  42% 139/330 [00:22<00:31,  6.07it/s]2021-06-11 17:20:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.001953125\n","epoch 020: 100% 329/330 [00:53<00:00,  6.26it/s]2021-06-11 17:20:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 020 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.12it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.74it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.75it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.58it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.39it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.43it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:20:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.498 | nll_loss 4.91 | alignment_loss 0 | ppl 30.07 | wps 44754 | wpb 2006.9 | bsz 98.8 | num_updates 6584 | best_loss 6.498\n","2021-06-11 17:20:37 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:21:38 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint20.pt (epoch 20 @ 6584 updates, score 6.498) (writing took 60.69220015999986 seconds)\n","2021-06-11 17:21:38 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2021-06-11 17:21:38 | INFO | train | epoch 020 | loss 5.359 | nll_loss 3.752 | alignment_loss 1.599 | ppl 13.47 | wps 7782.4 | ups 2.83 | wpb 2754 | bsz 127.8 | num_updates 6584 | lr 0.000155889 | gnorm 1.619 | loss_scale 0.002 | train_wall 53 | wall 0\n","epoch 021:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:21:38 | INFO | fairseq.trainer | begin training epoch 21\n","epoch 021: 100% 329/330 [00:53<00:00,  6.22it/s]2021-06-11 17:22:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 021 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  7.00it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.65it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.40it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.31it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.24it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.16it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:22:33 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.464 | nll_loss 4.884 | alignment_loss 0 | ppl 29.52 | wps 43334.8 | wpb 2006.9 | bsz 98.8 | num_updates 6914 | best_loss 6.464\n","2021-06-11 17:22:33 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:23:31 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint21.pt (epoch 21 @ 6914 updates, score 6.464) (writing took 58.09664446600004 seconds)\n","2021-06-11 17:23:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2021-06-11 17:23:31 | INFO | train | epoch 021 | loss 5.21 | nll_loss 3.595 | alignment_loss 1.567 | ppl 12.08 | wps 8034 | ups 2.92 | wpb 2752.1 | bsz 127.4 | num_updates 6914 | lr 0.000152123 | gnorm 1.672 | loss_scale 0.002 | train_wall 53 | wall 0\n","epoch 022:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:23:31 | INFO | fairseq.trainer | begin training epoch 22\n","epoch 022:   9% 31/330 [00:05<00:48,  6.13it/s]2021-06-11 17:23:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0009765625\n","epoch 022:  85% 280/330 [00:45<00:08,  5.92it/s, loss=5.326, nll_loss=3.715, alignment_loss=1.594, ppl=13.13, wps=8660.6, ups=3.15, wpb=2749.6, bsz=128, num_updates=7000, lr=0.000151186, gnorm=1.636, loss_scale=0.001, train_wall=162, wall=0]2021-06-11 17:24:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00048828125\n","epoch 022: 100% 329/330 [00:54<00:00,  6.18it/s, loss=5.326, nll_loss=3.715, alignment_loss=1.594, ppl=13.13, wps=8660.6, ups=3.15, wpb=2749.6, bsz=128, num_updates=7000, lr=0.000151186, gnorm=1.636, loss_scale=0.001, train_wall=162, wall=0]2021-06-11 17:24:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 022 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.66it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.25it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.14it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.05it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.03it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.16it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:24:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.656 | nll_loss 5.106 | alignment_loss 0 | ppl 34.45 | wps 44667.4 | wpb 2006.9 | bsz 98.8 | num_updates 7242 | best_loss 6.464\n","2021-06-11 17:24:26 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:25:04 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint22.pt (epoch 22 @ 7242 updates, score 6.656) (writing took 37.414149995000116 seconds)\n","2021-06-11 17:25:04 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2021-06-11 17:25:04 | INFO | train | epoch 022 | loss 5.085 | nll_loss 3.463 | alignment_loss 1.531 | ppl 11.03 | wps 9760.3 | ups 3.54 | wpb 2753.5 | bsz 127.8 | num_updates 7242 | lr 0.000148638 | gnorm 1.684 | loss_scale 0.0005 | train_wall 53 | wall 0\n","epoch 023:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:25:04 | INFO | fairseq.trainer | begin training epoch 23\n","epoch 023: 100% 329/330 [00:53<00:00,  6.01it/s]2021-06-11 17:25:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 023 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.49it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  20% 3/15 [00:00<00:01, 10.26it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 12.19it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.98it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.78it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.73it/s]\u001b[A\n","                                                                        \u001b[A2021-06-11 17:25:59 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.728 | nll_loss 5.166 | alignment_loss 0 | ppl 35.89 | wps 44965.8 | wpb 2006.9 | bsz 98.8 | num_updates 7572 | best_loss 6.464\n","2021-06-11 17:25:59 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-06-11 17:26:35 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda5p_2/checkpoints/checkpoint23.pt (epoch 23 @ 7572 updates, score 6.728) (writing took 36.10700553400011 seconds)\n","2021-06-11 17:26:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2021-06-11 17:26:35 | INFO | train | epoch 023 | loss 5.037 | nll_loss 3.414 | alignment_loss 1.508 | ppl 10.66 | wps 9949.6 | ups 3.62 | wpb 2752.1 | bsz 127.4 | num_updates 7572 | lr 0.000145363 | gnorm 1.776 | loss_scale 0.0005 | train_wall 53 | wall 0\n","epoch 024:   0% 0/330 [00:00<?, ?it/s]2021-06-11 17:26:35 | INFO | fairseq.trainer | begin training epoch 24\n","epoch 024:   2% 5/330 [00:01<01:22,  3.93it/s]2021-06-11 17:26:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.000244140625\n","epoch 024:  54% 178/330 [00:29<00:25,  5.98it/s]2021-06-11 17:27:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0001220703125\n","epoch 024:  95% 314/330 [00:51<00:02,  6.11it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:785: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n","2021-06-11 17:27:27 | INFO | fairseq.nan_detector | Detected nan/inf grad norm, dumping norms...\n","2021-06-11 17:27:27 | INFO | fairseq.nan_detector | norms: {'encoder.embed_tokens.weight': nan, 'encoder.layers.0.self_attn.k_proj.weight': nan, 'encoder.layers.0.self_attn.k_proj.bias': nan, 'encoder.layers.0.self_attn.v_proj.weight': nan, 'encoder.layers.0.self_attn.v_proj.bias': nan, 'encoder.layers.0.self_attn.q_proj.weight': nan, 'encoder.layers.0.self_attn.q_proj.bias': nan, 'encoder.layers.0.self_attn.out_proj.weight': nan, 'encoder.layers.0.self_attn.out_proj.bias': nan, 'encoder.layers.0.self_attn_layer_norm.weight': nan, 'encoder.layers.0.self_attn_layer_norm.bias': nan, 'encoder.layers.0.fc1.weight': nan, 'encoder.layers.0.fc1.bias': nan, 'encoder.layers.0.fc2.weight': nan, 'encoder.layers.0.fc2.bias': nan, 'encoder.layers.0.final_layer_norm.weight': nan, 'encoder.layers.0.final_layer_norm.bias': nan, 'encoder.layers.1.self_attn.k_proj.weight': nan, 'encoder.layers.1.self_attn.k_proj.bias': nan, 'encoder.layers.1.self_attn.v_proj.weight': nan, 'encoder.layers.1.self_attn.v_proj.bias': nan, 'encoder.layers.1.self_attn.q_proj.weight': nan, 'encoder.layers.1.self_attn.q_proj.bias': nan, 'encoder.layers.1.self_attn.out_proj.weight': nan, 'encoder.layers.1.self_attn.out_proj.bias': nan, 'encoder.layers.1.self_attn_layer_norm.weight': nan, 'encoder.layers.1.self_attn_layer_norm.bias': nan, 'encoder.layers.1.fc1.weight': nan, 'encoder.layers.1.fc1.bias': nan, 'encoder.layers.1.fc2.weight': nan, 'encoder.layers.1.fc2.bias': nan, 'encoder.layers.1.final_layer_norm.weight': nan, 'encoder.layers.1.final_layer_norm.bias': nan, 'encoder.layers.2.self_attn.k_proj.weight': nan, 'encoder.layers.2.self_attn.k_proj.bias': nan, 'encoder.layers.2.self_attn.v_proj.weight': nan, 'encoder.layers.2.self_attn.v_proj.bias': nan, 'encoder.layers.2.self_attn.q_proj.weight': nan, 'encoder.layers.2.self_attn.q_proj.bias': nan, 'encoder.layers.2.self_attn.out_proj.weight': nan, 'encoder.layers.2.self_attn.out_proj.bias': nan, 'encoder.layers.2.self_attn_layer_norm.weight': nan, 'encoder.layers.2.self_attn_layer_norm.bias': nan, 'encoder.layers.2.fc1.weight': nan, 'encoder.layers.2.fc1.bias': nan, 'encoder.layers.2.fc2.weight': nan, 'encoder.layers.2.fc2.bias': nan, 'encoder.layers.2.final_layer_norm.weight': nan, 'encoder.layers.2.final_layer_norm.bias': nan, 'encoder.layers.3.self_attn.k_proj.weight': nan, 'encoder.layers.3.self_attn.k_proj.bias': nan, 'encoder.layers.3.self_attn.v_proj.weight': nan, 'encoder.layers.3.self_attn.v_proj.bias': nan, 'encoder.layers.3.self_attn.q_proj.weight': nan, 'encoder.layers.3.self_attn.q_proj.bias': nan, 'encoder.layers.3.self_attn.out_proj.weight': nan, 'encoder.layers.3.self_attn.out_proj.bias': nan, 'encoder.layers.3.self_attn_layer_norm.weight': nan, 'encoder.layers.3.self_attn_layer_norm.bias': nan, 'encoder.layers.3.fc1.weight': nan, 'encoder.layers.3.fc1.bias': nan, 'encoder.layers.3.fc2.weight': nan, 'encoder.layers.3.fc2.bias': nan, 'encoder.layers.3.final_layer_norm.weight': nan, 'encoder.layers.3.final_layer_norm.bias': nan, 'encoder.layers.4.self_attn.k_proj.weight': nan, 'encoder.layers.4.self_attn.k_proj.bias': nan, 'encoder.layers.4.self_attn.v_proj.weight': nan, 'encoder.layers.4.self_attn.v_proj.bias': nan, 'encoder.layers.4.self_attn.q_proj.weight': nan, 'encoder.layers.4.self_attn.q_proj.bias': nan, 'encoder.layers.4.self_attn.out_proj.weight': nan, 'encoder.layers.4.self_attn.out_proj.bias': nan, 'encoder.layers.4.self_attn_layer_norm.weight': nan, 'encoder.layers.4.self_attn_layer_norm.bias': nan, 'encoder.layers.4.fc1.weight': nan, 'encoder.layers.4.fc1.bias': nan, 'encoder.layers.4.fc2.weight': nan, 'encoder.layers.4.fc2.bias': nan, 'encoder.layers.4.final_layer_norm.weight': nan, 'encoder.layers.4.final_layer_norm.bias': nan, 'encoder.layers.5.self_attn.k_proj.weight': nan, 'encoder.layers.5.self_attn.k_proj.bias': nan, 'encoder.layers.5.self_attn.v_proj.weight': nan, 'encoder.layers.5.self_attn.v_proj.bias': nan, 'encoder.layers.5.self_attn.q_proj.weight': nan, 'encoder.layers.5.self_attn.q_proj.bias': nan, 'encoder.layers.5.self_attn.out_proj.weight': nan, 'encoder.layers.5.self_attn.out_proj.bias': nan, 'encoder.layers.5.self_attn_layer_norm.weight': nan, 'encoder.layers.5.self_attn_layer_norm.bias': nan, 'encoder.layers.5.fc1.weight': nan, 'encoder.layers.5.fc1.bias': nan, 'encoder.layers.5.fc2.weight': nan, 'encoder.layers.5.fc2.bias': nan, 'encoder.layers.5.final_layer_norm.weight': nan, 'encoder.layers.5.final_layer_norm.bias': nan, 'decoder.layers.0.self_attn.k_proj.weight': nan, 'decoder.layers.0.self_attn.k_proj.bias': nan, 'decoder.layers.0.self_attn.v_proj.weight': nan, 'decoder.layers.0.self_attn.v_proj.bias': nan, 'decoder.layers.0.self_attn.q_proj.weight': nan, 'decoder.layers.0.self_attn.q_proj.bias': nan, 'decoder.layers.0.self_attn.out_proj.weight': nan, 'decoder.layers.0.self_attn.out_proj.bias': nan, 'decoder.layers.0.self_attn_layer_norm.weight': nan, 'decoder.layers.0.self_attn_layer_norm.bias': nan, 'decoder.layers.0.encoder_attn.k_proj.weight': nan, 'decoder.layers.0.encoder_attn.k_proj.bias': nan, 'decoder.layers.0.encoder_attn.v_proj.weight': nan, 'decoder.layers.0.encoder_attn.v_proj.bias': nan, 'decoder.layers.0.encoder_attn.q_proj.weight': nan, 'decoder.layers.0.encoder_attn.q_proj.bias': nan, 'decoder.layers.0.encoder_attn.out_proj.weight': nan, 'decoder.layers.0.encoder_attn.out_proj.bias': nan, 'decoder.layers.0.encoder_attn_layer_norm.weight': nan, 'decoder.layers.0.encoder_attn_layer_norm.bias': nan, 'decoder.layers.0.fc1.weight': nan, 'decoder.layers.0.fc1.bias': nan, 'decoder.layers.0.fc2.weight': nan, 'decoder.layers.0.fc2.bias': nan, 'decoder.layers.0.final_layer_norm.weight': nan, 'decoder.layers.0.final_layer_norm.bias': nan, 'decoder.layers.1.self_attn.k_proj.weight': nan, 'decoder.layers.1.self_attn.k_proj.bias': nan, 'decoder.layers.1.self_attn.v_proj.weight': nan, 'decoder.layers.1.self_attn.v_proj.bias': nan, 'decoder.layers.1.self_attn.q_proj.weight': nan, 'decoder.layers.1.self_attn.q_proj.bias': nan, 'decoder.layers.1.self_attn.out_proj.weight': nan, 'decoder.layers.1.self_attn.out_proj.bias': nan, 'decoder.layers.1.self_attn_layer_norm.weight': nan, 'decoder.layers.1.self_attn_layer_norm.bias': nan, 'decoder.layers.1.encoder_attn.k_proj.weight': nan, 'decoder.layers.1.encoder_attn.k_proj.bias': nan, 'decoder.layers.1.encoder_attn.v_proj.weight': nan, 'decoder.layers.1.encoder_attn.v_proj.bias': nan, 'decoder.layers.1.encoder_attn.q_proj.weight': nan, 'decoder.layers.1.encoder_attn.q_proj.bias': nan, 'decoder.layers.1.encoder_attn.out_proj.weight': nan, 'decoder.layers.1.encoder_attn.out_proj.bias': nan, 'decoder.layers.1.encoder_attn_layer_norm.weight': nan, 'decoder.layers.1.encoder_attn_layer_norm.bias': nan, 'decoder.layers.1.fc1.weight': nan, 'decoder.layers.1.fc1.bias': nan, 'decoder.layers.1.fc2.weight': nan, 'decoder.layers.1.fc2.bias': nan, 'decoder.layers.1.final_layer_norm.weight': nan, 'decoder.layers.1.final_layer_norm.bias': nan, 'decoder.layers.2.self_attn.k_proj.weight': nan, 'decoder.layers.2.self_attn.k_proj.bias': nan, 'decoder.layers.2.self_attn.v_proj.weight': nan, 'decoder.layers.2.self_attn.v_proj.bias': nan, 'decoder.layers.2.self_attn.q_proj.weight': nan, 'decoder.layers.2.self_attn.q_proj.bias': nan, 'decoder.layers.2.self_attn.out_proj.weight': nan, 'decoder.layers.2.self_attn.out_proj.bias': nan, 'decoder.layers.2.self_attn_layer_norm.weight': nan, 'decoder.layers.2.self_attn_layer_norm.bias': nan, 'decoder.layers.2.encoder_attn.k_proj.weight': nan, 'decoder.layers.2.encoder_attn.k_proj.bias': nan, 'decoder.layers.2.encoder_attn.v_proj.weight': nan, 'decoder.layers.2.encoder_attn.v_proj.bias': nan, 'decoder.layers.2.encoder_attn.q_proj.weight': nan, 'decoder.layers.2.encoder_attn.q_proj.bias': nan, 'decoder.layers.2.encoder_attn.out_proj.weight': nan, 'decoder.layers.2.encoder_attn.out_proj.bias': nan, 'decoder.layers.2.encoder_attn_layer_norm.weight': nan, 'decoder.layers.2.encoder_attn_layer_norm.bias': nan, 'decoder.layers.2.fc1.weight': nan, 'decoder.layers.2.fc1.bias': nan, 'decoder.layers.2.fc2.weight': nan, 'decoder.layers.2.fc2.bias': nan, 'decoder.layers.2.final_layer_norm.weight': nan, 'decoder.layers.2.final_layer_norm.bias': nan, 'decoder.layers.3.self_attn.k_proj.weight': nan, 'decoder.layers.3.self_attn.k_proj.bias': nan, 'decoder.layers.3.self_attn.v_proj.weight': nan, 'decoder.layers.3.self_attn.v_proj.bias': nan, 'decoder.layers.3.self_attn.q_proj.weight': nan, 'decoder.layers.3.self_attn.q_proj.bias': nan, 'decoder.layers.3.self_attn.out_proj.weight': nan, 'decoder.layers.3.self_attn.out_proj.bias': nan, 'decoder.layers.3.self_attn_layer_norm.weight': nan, 'decoder.layers.3.self_attn_layer_norm.bias': nan, 'decoder.layers.3.encoder_attn.k_proj.weight': nan, 'decoder.layers.3.encoder_attn.k_proj.bias': nan, 'decoder.layers.3.encoder_attn.v_proj.weight': nan, 'decoder.layers.3.encoder_attn.v_proj.bias': nan, 'decoder.layers.3.encoder_attn.q_proj.weight': nan, 'decoder.layers.3.encoder_attn.q_proj.bias': nan, 'decoder.layers.3.encoder_attn.out_proj.weight': nan, 'decoder.layers.3.encoder_attn.out_proj.bias': nan, 'decoder.layers.3.encoder_attn_layer_norm.weight': nan, 'decoder.layers.3.encoder_attn_layer_norm.bias': nan, 'decoder.layers.3.fc1.weight': nan, 'decoder.layers.3.fc1.bias': nan, 'decoder.layers.3.fc2.weight': nan, 'decoder.layers.3.fc2.bias': nan, 'decoder.layers.3.final_layer_norm.weight': nan, 'decoder.layers.3.final_layer_norm.bias': nan, 'decoder.layers.4.self_attn.k_proj.weight': nan, 'decoder.layers.4.self_attn.k_proj.bias': nan, 'decoder.layers.4.self_attn.v_proj.weight': nan, 'decoder.layers.4.self_attn.v_proj.bias': nan, 'decoder.layers.4.self_attn.q_proj.weight': nan, 'decoder.layers.4.self_attn.q_proj.bias': nan, 'decoder.layers.4.self_attn.out_proj.weight': nan, 'decoder.layers.4.self_attn.out_proj.bias': nan, 'decoder.layers.4.self_attn_layer_norm.weight': nan, 'decoder.layers.4.self_attn_layer_norm.bias': nan, 'decoder.layers.4.encoder_attn.k_proj.weight': nan, 'decoder.layers.4.encoder_attn.k_proj.bias': nan, 'decoder.layers.4.encoder_attn.v_proj.weight': 0.18691831827163696, 'decoder.layers.4.encoder_attn.v_proj.bias': 0.019554588943719864, 'decoder.layers.4.encoder_attn.q_proj.weight': nan, 'decoder.layers.4.encoder_attn.q_proj.bias': nan, 'decoder.layers.4.encoder_attn.out_proj.weight': 0.0985485315322876, 'decoder.layers.4.encoder_attn.out_proj.bias': 0.016326794400811195, 'decoder.layers.4.encoder_attn_layer_norm.weight': 0.011261203326284885, 'decoder.layers.4.encoder_attn_layer_norm.bias': 0.02014175057411194, 'decoder.layers.4.fc1.weight': 0.12026078999042511, 'decoder.layers.4.fc1.bias': 0.0074996971525251865, 'decoder.layers.4.fc2.weight': 0.16068731248378754, 'decoder.layers.4.fc2.bias': 0.018588844686746597, 'decoder.layers.4.final_layer_norm.weight': 0.011221924796700478, 'decoder.layers.4.final_layer_norm.bias': 0.021160122007131577, 'decoder.layers.5.self_attn.k_proj.weight': 0.03814530372619629, 'decoder.layers.5.self_attn.k_proj.bias': 4.421648191055283e-05, 'decoder.layers.5.self_attn.v_proj.weight': 0.17102977633476257, 'decoder.layers.5.self_attn.v_proj.bias': 0.016440128907561302, 'decoder.layers.5.self_attn.q_proj.weight': 0.0357913039624691, 'decoder.layers.5.self_attn.q_proj.bias': 0.0019824139308184385, 'decoder.layers.5.self_attn.out_proj.weight': 0.09690359979867935, 'decoder.layers.5.self_attn.out_proj.bias': 0.01776825450360775, 'decoder.layers.5.self_attn_layer_norm.weight': 0.011375544592738152, 'decoder.layers.5.self_attn_layer_norm.bias': 0.018900737166404724, 'decoder.layers.5.encoder_attn.k_proj.weight': 0.061422742903232574, 'decoder.layers.5.encoder_attn.k_proj.bias': 3.9015052607282996e-05, 'decoder.layers.5.encoder_attn.v_proj.weight': 0.19952209293842316, 'decoder.layers.5.encoder_attn.v_proj.bias': 0.019691485911607742, 'decoder.layers.5.encoder_attn.q_proj.weight': 0.06781457364559174, 'decoder.layers.5.encoder_attn.q_proj.bias': 0.0037631173618137836, 'decoder.layers.5.encoder_attn.out_proj.weight': 0.10856957733631134, 'decoder.layers.5.encoder_attn.out_proj.bias': 0.01762216165661812, 'decoder.layers.5.encoder_attn_layer_norm.weight': 0.013826264999806881, 'decoder.layers.5.encoder_attn_layer_norm.bias': 0.022474119439721107, 'decoder.layers.5.fc1.weight': 0.2001929134130478, 'decoder.layers.5.fc1.bias': 0.012994270771741867, 'decoder.layers.5.fc2.weight': 0.27614155411720276, 'decoder.layers.5.fc2.bias': 0.022856593132019043, 'decoder.layers.5.final_layer_norm.weight': 0.0302854236215353, 'decoder.layers.5.final_layer_norm.bias': 0.027326596900820732}\n","2021-06-11 17:27:28 | INFO | fairseq.nan_detector | gradients: {'encoder.embed_tokens.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), 'encoder.layers.0.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.encoder_attn.k_proj.weight': tensor([[        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        [        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        [        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        ...,\n","        [-6.7830e-05, -1.7548e-04,  6.0201e-05,  ..., -8.3447e-07,\n","          1.2362e-04,  1.1384e-05],\n","        [-3.6180e-05, -2.6286e-05, -1.0490e-04,  ..., -1.3554e-04,\n","         -3.5286e-05, -8.9049e-05],\n","        [-9.4175e-05, -1.9860e-04,  9.5189e-05,  ..., -1.7297e-04,\n","          5.8651e-05, -4.8459e-05]], device='cuda:0', dtype=torch.float16), 'decoder.layers.4.encoder_attn.k_proj.bias': tensor([        nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan, -1.1921e-06,\n","        -4.4703e-06, -1.1921e-07, -1.6093e-06, -2.6822e-06, -3.5763e-07,\n","         2.3842e-07,  8.9407e-07,  1.7881e-06, -2.0862e-06,  2.4438e-06,\n","        -5.3644e-07, -2.2650e-06, -2.2054e-06,  1.8477e-06,  3.2187e-06,\n","        -3.5763e-07, -3.5763e-07, -4.1723e-07, -4.0531e-06,  2.9802e-07,\n","         3.0398e-06, -1.5497e-06,  1.6093e-06,  5.9605e-08,  2.2054e-06,\n","         1.0133e-06,  1.3113e-06, -9.5367e-07,  1.4901e-06, -2.6226e-06,\n","        -1.7881e-07, -1.7881e-06, -2.6226e-06,  2.0266e-06,  1.1325e-06,\n","        -1.9670e-06, -2.9802e-07,  1.9073e-06,  4.1723e-07,  2.5034e-06,\n","        -1.4901e-06,  1.9073e-06, -2.8610e-06,  1.1921e-06, -1.6689e-06,\n","         9.5367e-07,  4.7684e-07,  1.7285e-06,  2.5034e-06,  5.9605e-07,\n","         1.7881e-07, -1.1921e-07, -2.6822e-06,  8.3447e-07, -2.4438e-06,\n","        -1.2517e-06,  3.3379e-06,  5.0664e-06, -6.5565e-07, -9.5367e-07,\n","         4.7684e-07,  4.7684e-07, -5.3644e-07,  1.1921e-06,  1.8477e-06,\n","        -3.5763e-07, -1.1921e-07, -7.7486e-07, -5.9605e-07, -1.3113e-06,\n","        -1.0729e-06,  1.0729e-06,  1.8477e-06, -3.5763e-07, -4.7684e-07,\n","        -4.1723e-07, -1.0133e-06, -2.6226e-06, -1.1921e-07, -3.1590e-06,\n","        -5.3644e-07,  5.9605e-07, -1.2517e-06, -1.4901e-06, -2.9802e-07,\n","         2.9802e-07, -8.3447e-07,  1.5497e-06,  3.3379e-06,  0.0000e+00,\n","         1.5497e-06, -1.0133e-06, -1.4305e-06,  1.0729e-06,  2.5034e-06,\n","        -1.3113e-06,  5.9605e-08, -4.1723e-07,  1.1921e-06, -2.3842e-07,\n","         5.9605e-07, -1.3113e-06, -3.5763e-07,  1.3709e-06,  0.0000e+00,\n","        -2.0862e-06,  1.0729e-06, -5.9605e-07,  1.1921e-07, -5.3644e-07,\n","         7.1526e-07, -1.5497e-06,  1.7285e-06, -2.3842e-07,  1.0729e-06,\n","        -2.9802e-07,  2.5630e-06, -7.7486e-07,  1.2517e-06,  2.9206e-06,\n","        -5.9605e-07,  2.5034e-06, -1.1921e-06,  9.5367e-07,  2.3246e-06,\n","         3.5763e-07,  3.5763e-07, -2.0266e-06,  1.4305e-06,  2.9802e-07,\n","         5.9605e-08,  1.1921e-07,  3.0398e-06, -7.1526e-07, -5.9605e-07,\n","        -8.9407e-07,  7.7486e-07, -1.0729e-06,  1.7881e-06,  8.3447e-07,\n","         3.5763e-07,  1.2517e-06, -1.7285e-06,  1.9073e-06, -1.3709e-06,\n","         7.1526e-07, -1.4901e-06, -1.6689e-06,  6.5565e-07, -1.7881e-06,\n","         2.8014e-06, -5.9605e-07, -1.9073e-06,  2.9206e-06,  4.7684e-07,\n","         5.3644e-07, -4.7684e-07, -1.4305e-06, -3.5763e-07,  1.4901e-06,\n","        -2.9802e-07,  2.3246e-06, -1.9670e-06,  5.9605e-07,  1.6689e-06,\n","         2.3246e-06,  4.5300e-06, -4.7684e-07, -8.3447e-07, -2.1458e-06,\n","         2.0862e-06,  1.1325e-06, -4.7684e-07, -2.4438e-06,  1.7881e-07,\n","         2.0862e-06, -9.5367e-07,  1.9670e-06, -1.3113e-06, -2.6226e-06,\n","         1.4901e-06,  1.7881e-07,  1.4305e-06,  2.1458e-06, -4.1723e-06,\n","         3.3975e-06,  1.9073e-06, -1.3709e-06,  8.3447e-07,  1.2517e-06,\n","        -2.1458e-06, -3.0994e-06,  2.2054e-06, -4.3511e-06, -4.1723e-07,\n","        -3.0398e-06,  5.4836e-06,  1.1921e-07, -8.3447e-07, -5.3644e-07,\n","         1.9670e-06, -3.2187e-06,  2.9206e-06,  1.3113e-06,  2.3842e-07,\n","        -1.6689e-06,  1.1921e-07,  2.9802e-06,  1.1921e-06, -1.9073e-06,\n","        -5.3644e-07,  1.2517e-06,  3.2783e-06, -1.2517e-06, -2.2054e-06,\n","         2.9802e-07, -5.3644e-07,  0.0000e+00, -6.5565e-07, -1.5497e-06,\n","        -3.2783e-06,  3.2783e-06,  1.0729e-06,  4.1723e-07,  2.3246e-06,\n","        -4.1723e-07,  2.6226e-06,  2.2650e-06, -3.5763e-07,  3.5763e-07,\n","         3.5763e-06, -2.3842e-07, -9.5367e-07,  5.9605e-07,  5.3644e-07,\n","         1.4305e-06,  2.1458e-06, -6.5565e-07,  1.2517e-06,  1.2517e-06,\n","         0.0000e+00,  2.2054e-06,  3.0994e-06, -8.3447e-07, -6.5565e-07,\n","         2.6226e-06,  8.9407e-07, -8.9407e-07,  1.4305e-06,  4.0531e-06,\n","        -2.5034e-06,  1.6689e-06,  4.1723e-07,  1.9670e-06,  4.7684e-07,\n","        -5.9605e-07, -1.7881e-07,  1.9073e-06,  8.3447e-07, -2.4438e-06,\n","         1.3113e-06,  1.8477e-06, -2.5630e-06,  3.6359e-06, -2.3842e-07,\n","         5.9605e-08,  1.3113e-06, -1.8477e-06, -5.9605e-07, -1.1921e-06,\n","        -1.3709e-06,  1.2517e-06, -7.7486e-07, -2.3842e-07, -2.3842e-07,\n","        -9.5367e-07, -1.9073e-06, -3.3975e-06, -2.3842e-07, -1.3113e-06,\n","         1.3113e-06, -1.0133e-06,  1.6093e-06,  4.7684e-07,  3.0994e-06,\n","         2.3842e-07, -9.5367e-07, -1.9670e-06, -1.1921e-06,  2.3246e-06,\n","         2.5034e-06,  1.7881e-07,  3.5763e-06, -8.9407e-07, -2.3246e-06,\n","         4.1723e-07,  5.3644e-07,  2.3842e-06, -4.1723e-07,  5.3644e-07,\n","         7.1526e-07,  1.1921e-07,  9.5367e-07,  1.3709e-06, -1.4305e-06,\n","         9.5367e-07,  1.1921e-06,  8.9407e-07,  4.1723e-07, -5.3644e-07,\n","        -1.2517e-06,  2.5034e-06,  1.7881e-06,  2.0862e-06,  1.1325e-06,\n","        -1.6093e-06,  2.0862e-06,  1.3113e-06, -2.8014e-06,  7.7486e-07,\n","        -1.3709e-06,  4.1723e-07,  1.2517e-06, -2.0862e-06, -1.4901e-06,\n","        -4.2319e-06,  4.7684e-07,  4.1723e-07,  7.1526e-07, -1.3113e-06,\n","        -5.3644e-07,  2.8610e-06,  3.1590e-06,  1.2517e-06, -2.0266e-06,\n","         2.4438e-06, -1.0133e-06, -2.0266e-06,  2.9802e-06, -1.0729e-06,\n","        -1.4901e-06,  1.2517e-06,  2.6822e-06, -3.7551e-06,  1.5497e-06,\n","        -1.0729e-06, -1.4305e-06,  1.4901e-06,  3.1590e-06, -3.3975e-06,\n","         9.5367e-07,  0.0000e+00, -2.5630e-06,  3.2783e-06, -5.9605e-08,\n","        -2.2650e-06, -1.4305e-06, -2.9802e-07, -1.0729e-06, -3.0994e-06,\n","         1.9073e-06, -1.1921e-07,  1.7285e-06, -1.6689e-06,  1.7285e-06,\n","         1.8477e-06, -1.1325e-06,  1.6093e-06, -2.1458e-06, -3.0398e-06,\n","        -2.7418e-06,  3.8147e-06,  7.7486e-07, -1.3709e-06,  1.6093e-06,\n","         1.7285e-06, -2.9802e-07,  3.5763e-07,  2.0862e-06, -3.4571e-06,\n","         2.9206e-06,  1.5497e-06,  4.1723e-07,  1.0729e-06,  1.8477e-06,\n","         1.9073e-06, -1.4305e-06,  9.5367e-07,  1.1325e-06, -2.4438e-06,\n","        -8.9407e-07, -2.1458e-06,  2.6822e-06, -5.3644e-07,  1.1325e-06,\n","        -1.7285e-06, -2.6822e-06, -3.0994e-06, -1.7881e-07, -1.2517e-06,\n","         0.0000e+00,  1.9073e-06,  5.9605e-07,  2.3842e-06, -2.2650e-06,\n","        -4.2319e-06,  1.5497e-06, -1.7881e-07, -2.2054e-06,  2.7418e-06,\n","         5.3048e-06, -4.1723e-07, -5.9605e-08, -3.6359e-06,  2.5034e-06,\n","         1.6689e-06, -1.7881e-07,  5.9605e-08, -8.9407e-07, -1.6093e-06,\n","        -1.9670e-06, -1.9073e-06,  4.7684e-07,  3.3379e-06,  2.0862e-06,\n","         7.1526e-07,  3.3379e-06, -6.5565e-07, -2.9802e-06,  4.7684e-07,\n","         2.9802e-07,  1.4305e-06, -2.1458e-06, -2.9802e-07,  5.9605e-08,\n","         2.2054e-06,  1.9073e-06, -2.5034e-06, -1.6093e-06,  2.5630e-06,\n","        -6.5565e-07, -3.5763e-07,  8.3447e-07, -1.3113e-06, -3.0398e-06,\n","        -5.3644e-07, -2.2054e-06], device='cuda:0', dtype=torch.float16), 'decoder.layers.4.encoder_attn.q_proj.weight': tensor([[        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        [        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        [        nan,         nan,         nan,  ...,         nan,\n","                 nan,         nan],\n","        ...,\n","        [-2.9206e-05,  1.3399e-04,  4.8161e-05,  ...,  1.9193e-05,\n","         -5.2452e-06,  5.4002e-05],\n","        [ 2.1601e-04, -4.2260e-05,  1.7881e-06,  ..., -9.7752e-05,\n","         -9.8646e-05,  4.6790e-05],\n","        [ 3.4869e-05, -1.0419e-04, -4.6909e-05,  ...,  4.4644e-05,\n","          1.5342e-04,  1.5068e-04]], device='cuda:0', dtype=torch.float16), 'decoder.layers.4.encoder_attn.q_proj.bias': tensor([        nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,         nan,\n","                nan,         nan,         nan,         nan,  5.4479e-05,\n","        -1.3113e-04, -1.1635e-04, -2.5225e-04,  9.0003e-06, -1.5521e-04,\n","         3.0661e-04,  2.6107e-05,  7.4208e-05,  1.4472e-04,  6.1572e-05,\n","         2.7597e-05,  1.7798e-04,  2.1398e-05,  2.9564e-05,  5.1856e-06,\n","        -1.0341e-04,  1.2922e-04, -2.9039e-04,  8.7142e-05, -1.3351e-04,\n","         8.8215e-06,  7.4327e-05, -2.4104e-04,  1.2124e-04,  1.4138e-04,\n","        -6.5982e-05, -6.5386e-05,  2.5272e-04, -3.1805e-04, -1.8454e-04,\n","         2.7108e-04,  1.6046e-04, -3.4273e-05, -4.5061e-05, -3.0589e-04,\n","         1.8048e-04, -2.8276e-04,  4.3333e-05,  2.8586e-04, -4.4465e-05,\n","         7.8976e-05, -4.6492e-06, -2.4438e-06,  3.3998e-04, -2.0361e-04,\n","         1.7619e-04, -3.4618e-04, -1.0872e-04,  1.2517e-04,  2.3282e-04,\n","         1.9836e-04, -7.0930e-06, -6.5207e-05,  2.7776e-05, -4.9412e-05,\n","        -9.5546e-05, -5.1320e-05, -1.7381e-04,  2.3067e-05,  1.6284e-04,\n","         2.2066e-04,  2.4867e-04,  3.5095e-04,  1.6260e-04, -2.4891e-04,\n","        -4.9412e-05, -4.1556e-04,  2.1446e-04, -1.6880e-04, -3.7074e-04,\n","        -1.7667e-04,  6.5446e-05,  1.0729e-04,  9.2745e-05,  5.1022e-05,\n","         1.6928e-04,  1.3137e-04, -1.9073e-04, -1.3292e-05,  2.6894e-04,\n","         2.0838e-04, -3.7575e-04,  8.0943e-05,  3.0756e-04,  1.7810e-04,\n","         2.2435e-04,  1.0055e-04,  2.5928e-05, -2.9874e-04,  1.2863e-04,\n","        -1.0335e-04,  2.2912e-04, -1.6463e-04, -9.3102e-05, -1.3006e-04,\n","         1.3554e-04, -1.4102e-04, -4.2343e-04,  2.9254e-04,  2.4915e-04,\n","         3.5107e-05,  9.7811e-05,  2.9182e-04, -2.2411e-04, -7.9215e-05,\n","         1.6725e-04,  3.4571e-04, -9.4712e-05,  3.6061e-05,  4.0770e-05,\n","         1.0133e-06,  2.9397e-04, -3.3498e-05,  2.3317e-04, -2.6369e-04,\n","         7.3373e-05, -2.6107e-04, -2.1482e-04,  2.0862e-06, -3.4511e-05,\n","        -7.3910e-06, -1.1092e-04, -1.0473e-04, -2.7323e-04,  9.5904e-05,\n","        -1.2803e-04,  4.5717e-05, -3.0303e-04, -9.4056e-05,  3.3283e-04,\n","         1.4246e-05,  6.1452e-05,  2.5892e-04, -1.5283e-04,  1.7643e-04,\n","        -1.7202e-04, -1.4532e-04,  3.8314e-04,  5.5134e-05,  2.4331e-04,\n","        -1.3828e-04, -3.6120e-05, -1.4102e-04,  1.2052e-04, -1.2720e-04,\n","        -8.1301e-05,  2.4819e-04, -2.5821e-04, -2.2054e-05,  3.0851e-04,\n","         1.0788e-05, -1.6928e-04, -7.4863e-05, -5.4121e-05, -8.0943e-05,\n","        -2.3735e-04, -2.1124e-04, -1.9777e-04,  2.6846e-04, -3.9101e-05,\n","        -8.7738e-05,  9.9540e-06, -7.1883e-05, -1.2290e-04,  3.6359e-05,\n","        -1.7571e-04, -8.6725e-05, -3.3975e-06, -8.0705e-05,  4.4441e-04,\n","         3.7742e-04, -2.4652e-04, -2.2244e-04,  7.9155e-05,  1.4424e-04,\n","         4.5824e-04,  1.1939e-04,  3.4213e-04,  2.6846e-04, -4.0793e-04,\n","        -1.1849e-04,  1.0407e-04,  2.7752e-04,  3.9077e-04, -2.9087e-05,\n","         3.7599e-04,  2.2042e-04, -4.4918e-04,  2.8324e-04, -7.1526e-05,\n","        -1.2040e-05,  6.5029e-05, -5.3287e-05, -1.8167e-04, -7.0393e-05,\n","         7.9691e-05,  1.2684e-04, -9.2506e-05, -2.4557e-05,  3.3975e-06,\n","        -8.1062e-05,  9.1195e-05, -1.4424e-04,  1.9014e-05,  9.6738e-05,\n","         1.5283e-04,  2.2531e-05, -1.0157e-04,  2.9445e-05, -1.0663e-04,\n","        -1.3101e-04, -5.0664e-06, -1.3709e-06, -1.6403e-04,  1.4246e-04,\n","         2.5296e-04, -2.4152e-04, -1.7643e-04, -3.9101e-05, -1.9574e-04,\n","        -4.1902e-05, -1.1426e-04, -1.5974e-05, -5.0664e-05, -3.0518e-05,\n","        -2.5749e-04,  1.7583e-05,  8.6427e-06,  1.2326e-04,  1.7881e-04,\n","        -2.8133e-05,  3.4273e-05,  1.0675e-04,  1.1379e-04, -1.3590e-04,\n","        -4.5598e-05,  9.4652e-05, -4.5896e-05, -2.1243e-04, -1.9753e-04,\n","         1.7023e-04, -1.3816e-04,  6.8247e-05, -5.8293e-05, -4.9412e-05,\n","        -9.3997e-05, -1.1444e-05, -6.7055e-05,  7.9036e-05,  6.9380e-05,\n","         7.6652e-05,  1.5616e-05,  7.4089e-05,  8.7082e-05,  6.5684e-05,\n","         1.0151e-04, -6.2823e-05, -1.6332e-04,  1.4329e-04, -3.6001e-05,\n","         2.3949e-04, -1.9574e-04,  1.4615e-04,  3.6478e-04,  2.6226e-06,\n","        -1.6344e-04,  7.9095e-05,  2.1005e-04, -6.2108e-05,  7.6652e-05,\n","        -1.5235e-04,  7.8499e-05, -1.6034e-04,  3.7599e-04,  6.5804e-05,\n","        -1.2231e-04, -2.9135e-04,  4.1366e-05, -4.8339e-05,  2.8670e-05,\n","        -1.1766e-04, -1.5736e-04,  2.3305e-05,  1.7560e-04,  1.0020e-04,\n","        -3.8028e-05, -1.1164e-04, -1.9133e-04, -3.7265e-04,  2.2292e-04,\n","         3.0255e-04,  1.5616e-04,  1.7393e-04, -8.8990e-05, -1.3280e-04,\n","         1.5450e-04,  5.9009e-05,  3.2330e-04,  1.4281e-04,  1.8001e-04,\n","        -2.0266e-06,  1.0324e-04,  1.8764e-04,  1.1063e-04, -9.5665e-05,\n","         1.4806e-04, -1.2219e-05, -2.5988e-04,  3.1233e-04,  1.2457e-05,\n","        -2.5678e-04,  1.3733e-04,  1.8775e-04, -4.0174e-05, -2.8610e-05,\n","        -1.9407e-04,  8.9705e-05, -1.3816e-04,  3.1710e-05,  1.8239e-04,\n","         1.0252e-05,  1.5628e-04, -3.2842e-05, -3.5763e-05, -2.4271e-04,\n","        -1.6212e-04, -3.0458e-05, -7.9215e-05, -8.2374e-05, -4.2379e-05,\n","        -1.3363e-04,  2.0790e-04,  2.2686e-04,  1.0395e-04, -1.8919e-04,\n","        -9.1791e-05, -2.4509e-04,  1.4853e-04,  4.4107e-05, -6.4969e-05,\n","        -9.9599e-05,  1.1212e-04,  8.4639e-06, -5.7876e-05, -1.2165e-04,\n","        -7.9036e-05, -5.6505e-05,  4.5943e-04,  2.4116e-04, -2.9063e-04,\n","        -5.7995e-05,  4.2975e-05, -1.6117e-04,  2.0409e-04, -1.8573e-04,\n","         3.2234e-04, -1.7548e-04, -1.2684e-04,  3.1233e-04, -2.6584e-04,\n","         8.6963e-05,  1.7309e-04,  3.2973e-04, -2.6965e-04,  1.2165e-04,\n","        -1.4019e-04,  1.3244e-04,  2.3723e-04, -2.4462e-04, -5.9903e-05,\n","         1.6451e-05,  1.0157e-04, -7.2658e-05,  1.5247e-04,  9.1732e-05,\n","         1.8048e-04, -1.3721e-04, -1.5020e-04,  9.4652e-05, -4.8459e-05,\n","         6.0678e-05,  2.6226e-05,  2.4700e-04,  3.4153e-05, -1.8644e-04,\n","        -3.6657e-05,  2.2602e-04,  1.4174e-04, -1.0133e-04, -1.0204e-04,\n","        -2.2399e-04, -1.7166e-05, -1.6820e-04, -1.4472e-04, -2.1183e-04,\n","        -2.1648e-04,  4.1962e-05,  5.9426e-05,  7.1645e-05, -2.1505e-04,\n","        -6.1631e-05,  4.7684e-06,  2.7823e-04, -2.5451e-05, -1.3137e-04,\n","         1.1277e-04,  2.9147e-05,  3.0518e-05,  1.4353e-04,  9.7573e-05,\n","        -9.6738e-05,  7.0274e-05, -2.8992e-04, -1.1766e-04,  5.2452e-06,\n","         2.2149e-04,  2.0266e-05,  3.7849e-05,  5.4538e-05, -8.9407e-06,\n","         1.3971e-04, -4.5836e-05, -9.6619e-05,  8.1003e-05,  4.9293e-05,\n","        -7.1824e-05,  3.8087e-05, -2.5201e-04,  5.2691e-05, -7.3910e-06,\n","         9.4950e-05, -3.2008e-05,  6.4433e-05, -1.8680e-04,  1.5020e-04,\n","        -1.4353e-04, -7.9095e-05,  1.1986e-04,  3.1650e-05,  4.2498e-05,\n","         2.0456e-04, -4.9889e-05, -7.6056e-05,  5.3763e-05,  1.2970e-04,\n","         1.2398e-04,  1.8251e-04], device='cuda:0', dtype=torch.float16)}\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n","    sys.exit(cli_main())\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n","    distributed_utils.call_main(args, main)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n","    main(args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 125, in main\n","    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\n","  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n","    return func(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 208, in train\n","    log_output = trainer.train_step(samples)\n","  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n","    return func(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 580, in train_step\n","    grad_norm = self.clip_grad_norm(self.args.clip_norm)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 855, in clip_grad_norm\n","    return self.optimizer.clip_grad_norm(clip_norm, aggregate_norm_fn=None)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/fp16_optimizer.py\", line 181, in clip_grad_norm\n","    self.scaler.check_overflow(grad_norm)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/dynamic_loss_scaler.py\", line 66, in check_overflow\n","    ).format(self.min_loss_scale)\n","FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\n","\n","real\t21m45.958s\n","user\t11m48.070s\n","sys\t1m6.958s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qHBIXSFPi6g-"},"source":["##Train baseline with slignment-lambda=0.5 and 1 heads"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RlRv_-3OegFh","executionInfo":{"status":"ok","timestamp":1622302125181,"user_tz":-420,"elapsed":3378653,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"83ac8eb8-8bf9-4eb0-8b71-58020ac0412d"},"source":["!time fairseq-train --alignment-heads 1\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.5\\\n","    --fp16"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-29 14:32:32 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, alignment_heads=1, alignment_lambda=0.5, alignment_layer=4, all_gather_list_size=16384, arch='transformer_align', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_with_alignment', cross_self_attention=False, curriculum=0, data='en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, full_context_alignment=False, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=True, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n","2021-05-29 14:32:33 | INFO | fairseq.tasks.translation | [en] dictionary: 56792 types\n","2021-05-29 14:32:33 | INFO | fairseq.tasks.translation | [wovi] dictionary: 56792 types\n","2021-05-29 14:32:33 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized/valid.en-wovi.en\n","2021-05-29 14:32:33 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized/valid.en-wovi.wovi\n","2021-05-29 14:32:33 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized valid en-wovi 1482 examples\n","2021-05-29 14:32:34 | INFO | fairseq_cli.train | TransformerAlignModel(\n","  (encoder): TransformerEncoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=512, out_features=56792, bias=False)\n","  )\n",")\n","2021-05-29 14:32:34 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n","2021-05-29 14:32:34 | INFO | fairseq_cli.train | model: transformer_align (TransformerAlignModel)\n","2021-05-29 14:32:34 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_with_alignment (LabelSmoothedCrossEntropyCriterionWithAlignment)\n","2021-05-29 14:32:35 | INFO | fairseq_cli.train | num. model params: 73216000 (num. trained: 73216000)\n","2021-05-29 14:32:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n","2021-05-29 14:32:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n","2021-05-29 14:32:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 14:32:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2021-05-29 14:32:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 14:32:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2021-05-29 14:32:45 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n","2021-05-29 14:32:45 | INFO | fairseq.trainer | no existing checkpoint found en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint_last.pt\n","2021-05-29 14:32:45 | INFO | fairseq.trainer | loading train data for epoch 1\n","2021-05-29 14:32:45 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized/train.en-wovi.en\n","2021-05-29 14:32:45 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized/train.en-wovi.wovi\n","2021-05-29 14:32:45 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized train en-wovi 42026 examples\n","2021-05-29 14:32:46 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h1-l4-alignment-lambda50p/binarized/train.align.en-wovi\n","epoch 001:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:32:46 | INFO | fairseq.trainer | begin training epoch 1\n","epoch 001:   1% 2/330 [00:00<01:37,  3.35it/s]2021-05-29 14:32:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n","epoch 001:  58% 193/330 [00:31<00:22,  6.12it/s]2021-05-29 14:33:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n","epoch 001: 100% 329/330 [00:54<00:00,  6.00it/s]2021-05-29 14:33:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.75it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.31it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.20it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.85it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:33:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.476 | nll_loss 11.992 | alignment_loss 0 | ppl 4073.98 | wps 43354 | wpb 2006.9 | bsz 98.8 | num_updates 328\n","2021-05-29 14:33:41 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:34:47 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint1.pt (epoch 1 @ 328 updates, score 12.476) (writing took 65.82044492099976 seconds)\n","2021-05-29 14:34:47 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2021-05-29 14:34:47 | INFO | train | epoch 001 | loss 14.234 | nll_loss 13.977 | alignment_loss 3.853 | ppl 16125.1 | wps 7453.7 | ups 2.71 | wpb 2754.1 | bsz 124.9 | num_updates 328 | lr 1.64e-05 | gnorm 2.664 | loss_scale 32 | train_wall 54 | wall 122\n","epoch 002:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:34:47 | INFO | fairseq.trainer | begin training epoch 2\n","epoch 002:  23% 76/330 [00:13<00:42,  5.92it/s]2021-05-29 14:35:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n","epoch 002:  43% 141/330 [00:24<00:32,  5.84it/s]2021-05-29 14:35:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n","epoch 002:  45% 147/330 [00:25<00:31,  5.88it/s]2021-05-29 14:35:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n","epoch 002:  47% 155/330 [00:27<00:30,  5.74it/s]2021-05-29 14:35:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0\n","epoch 002:  50% 164/330 [00:28<00:27,  5.96it/s]2021-05-29 14:35:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0\n","epoch 002:  62% 206/330 [00:35<00:20,  5.95it/s]2021-05-29 14:35:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5\n","epoch 002:  63% 208/330 [00:36<00:20,  6.09it/s]2021-05-29 14:35:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25\n","epoch 002:  93% 307/330 [00:52<00:03,  6.17it/s]2021-05-29 14:35:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125\n","epoch 002: 100% 329/330 [00:56<00:00,  6.04it/s]2021-05-29 14:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.91it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.92it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:35:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.647 | nll_loss 9.786 | alignment_loss 0 | ppl 882.91 | wps 44986.4 | wpb 2006.9 | bsz 98.8 | num_updates 650 | best_loss 10.647\n","2021-05-29 14:35:44 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:36:42 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint2.pt (epoch 2 @ 650 updates, score 10.647) (writing took 58.07064286199966 seconds)\n","2021-05-29 14:36:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2021-05-29 14:36:42 | INFO | train | epoch 002 | loss 11.318 | nll_loss 10.664 | alignment_loss 3.231 | ppl 1621.95 | wps 7676.4 | ups 2.79 | wpb 2749.9 | bsz 128.6 | num_updates 650 | lr 3.25e-05 | gnorm 2.91 | loss_scale 0.125 | train_wall 55 | wall 237\n","epoch 003:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:36:42 | INFO | fairseq.trainer | begin training epoch 3\n","epoch 003: 100% 329/330 [00:56<00:00,  6.02it/s]2021-05-29 14:37:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.75it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.60it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:37:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.539 | nll_loss 9.618 | alignment_loss 0 | ppl 785.64 | wps 44453.4 | wpb 2006.9 | bsz 98.8 | num_updates 980 | best_loss 10.539\n","2021-05-29 14:37:40 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:38:42 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint3.pt (epoch 3 @ 980 updates, score 10.539) (writing took 62.06028383600005 seconds)\n","2021-05-29 14:38:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2021-05-29 14:38:42 | INFO | train | epoch 003 | loss 10.561 | nll_loss 9.703 | alignment_loss 2.955 | ppl 833.21 | wps 7608.1 | ups 2.76 | wpb 2752.1 | bsz 127.4 | num_updates 980 | lr 4.9e-05 | gnorm 2.905 | loss_scale 0.125 | train_wall 55 | wall 356\n","epoch 004:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:38:42 | INFO | fairseq.trainer | begin training epoch 4\n","epoch 004: 100% 329/330 [00:56<00:00,  6.30it/s, loss=12.008, nll_loss=11.412, alignment_loss=3.336, ppl=2725.06, wps=7647.5, ups=2.78, wpb=2752.5, bsz=126.8, num_updates=1000, lr=5e-05, gnorm=2.815, loss_scale=0.125, train_wall=168, wall=360]2021-05-29 14:39:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.99it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.08it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:39:39 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.26 | nll_loss 9.332 | alignment_loss 0 | ppl 644.46 | wps 45434.2 | wpb 2006.9 | bsz 98.8 | num_updates 1310 | best_loss 10.26\n","2021-05-29 14:39:39 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:40:36 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint4.pt (epoch 4 @ 1310 updates, score 10.26) (writing took 57.453343424000195 seconds)\n","2021-05-29 14:40:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2021-05-29 14:40:36 | INFO | train | epoch 004 | loss 10.365 | nll_loss 9.466 | alignment_loss 2.798 | ppl 707 | wps 7906.5 | ups 2.87 | wpb 2752.1 | bsz 127.4 | num_updates 1310 | lr 6.55e-05 | gnorm 2.553 | loss_scale 0.125 | train_wall 55 | wall 471\n","epoch 005:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:40:37 | INFO | fairseq.trainer | begin training epoch 5\n","epoch 005:  61% 200/330 [00:34<00:22,  5.69it/s]2021-05-29 14:41:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\n","epoch 005: 100% 329/330 [00:56<00:00,  6.36it/s]2021-05-29 14:41:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.04it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.75it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.74it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.63it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.41it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.46it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:41:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.89 | nll_loss 8.901 | alignment_loss 0 | ppl 478.04 | wps 45270.2 | wpb 2006.9 | bsz 98.8 | num_updates 1639 | best_loss 9.89\n","2021-05-29 14:41:34 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:42:35 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint5.pt (epoch 5 @ 1639 updates, score 9.89) (writing took 61.08873804399991 seconds)\n","2021-05-29 14:42:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2021-05-29 14:42:35 | INFO | train | epoch 005 | loss 10.079 | nll_loss 9.143 | alignment_loss 2.698 | ppl 565.54 | wps 7619.8 | ups 2.77 | wpb 2751.5 | bsz 127.5 | num_updates 1639 | lr 8.195e-05 | gnorm 2.392 | loss_scale 0.0625 | train_wall 55 | wall 590\n","epoch 006:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:42:36 | INFO | fairseq.trainer | begin training epoch 6\n","epoch 006: 100% 329/330 [00:56<00:00,  6.09it/s]2021-05-29 14:43:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 006 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.82it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.54it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.46it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.37it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.18it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.26it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:43:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.507 | nll_loss 8.45 | alignment_loss 0 | ppl 349.61 | wps 45045.8 | wpb 2006.9 | bsz 98.8 | num_updates 1969 | best_loss 9.507\n","2021-05-29 14:43:33 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:44:33 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint6.pt (epoch 6 @ 1969 updates, score 9.507) (writing took 59.41746082499958 seconds)\n","2021-05-29 14:44:33 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2021-05-29 14:44:33 | INFO | train | epoch 006 | loss 9.683 | nll_loss 8.701 | alignment_loss 2.592 | ppl 416.24 | wps 7724.6 | ups 2.81 | wpb 2752.1 | bsz 127.4 | num_updates 1969 | lr 9.845e-05 | gnorm 2.352 | loss_scale 0.0625 | train_wall 55 | wall 708\n","epoch 007:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:44:33 | INFO | fairseq.trainer | begin training epoch 7\n","epoch 007: 100% 329/330 [00:55<00:00,  6.11it/s, loss=10.014, nll_loss=9.072, alignment_loss=2.685, ppl=538.19, wps=7800.8, ups=2.84, wpb=2751.1, bsz=127.4, num_updates=2000, lr=0.0001, gnorm=2.427, loss_scale=0.0625, train_wall=168, wall=713]2021-05-29 14:45:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 007 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.74it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.45it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.43it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.28it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.15it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.20it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:45:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.034 | nll_loss 7.942 | alignment_loss 0 | ppl 245.84 | wps 45202.6 | wpb 2006.9 | bsz 98.8 | num_updates 2299 | best_loss 9.034\n","2021-05-29 14:45:30 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:46:27 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint7.pt (epoch 7 @ 2299 updates, score 9.034) (writing took 57.03666586300005 seconds)\n","2021-05-29 14:46:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2021-05-29 14:46:27 | INFO | train | epoch 007 | loss 9.262 | nll_loss 8.228 | alignment_loss 2.466 | ppl 299.73 | wps 7969.5 | ups 2.9 | wpb 2752.1 | bsz 127.4 | num_updates 2299 | lr 0.00011495 | gnorm 2.233 | loss_scale 0.0625 | train_wall 55 | wall 821\n","epoch 008:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:46:27 | INFO | fairseq.trainer | begin training epoch 8\n","epoch 008: 100% 329/330 [00:56<00:00,  6.49it/s]2021-05-29 14:47:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 008 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.28it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.72it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.49it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.36it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 12.35it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 14.57it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:47:25 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.735 | nll_loss 7.575 | alignment_loss 0 | ppl 190.66 | wps 44595 | wpb 2006.9 | bsz 98.8 | num_updates 2629 | best_loss 8.735\n","2021-05-29 14:47:25 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:48:23 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint8.pt (epoch 8 @ 2629 updates, score 8.735) (writing took 58.528295389999585 seconds)\n","2021-05-29 14:48:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2021-05-29 14:48:23 | INFO | train | epoch 008 | loss 8.889 | nll_loss 7.804 | alignment_loss 2.364 | ppl 223.54 | wps 7798.7 | ups 2.83 | wpb 2752.1 | bsz 127.4 | num_updates 2629 | lr 0.00013145 | gnorm 2.287 | loss_scale 0.0625 | train_wall 55 | wall 938\n","epoch 009:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:48:23 | INFO | fairseq.trainer | begin training epoch 9\n","epoch 009: 100% 329/330 [00:56<00:00,  6.03it/s]2021-05-29 14:49:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 009 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.44it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.12it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.03it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.93it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.84it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.89it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:49:21 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.376 | nll_loss 7.189 | alignment_loss 0 | ppl 145.94 | wps 44848.9 | wpb 2006.9 | bsz 98.8 | num_updates 2959 | best_loss 8.376\n","2021-05-29 14:49:21 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:50:18 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint9.pt (epoch 9 @ 2959 updates, score 8.376) (writing took 57.09697258200049 seconds)\n","2021-05-29 14:50:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2021-05-29 14:50:18 | INFO | train | epoch 009 | loss 8.499 | nll_loss 7.364 | alignment_loss 2.251 | ppl 164.71 | wps 7937.8 | ups 2.88 | wpb 2752.1 | bsz 127.4 | num_updates 2959 | lr 0.00014795 | gnorm 2.253 | loss_scale 0.0625 | train_wall 55 | wall 1052\n","epoch 010:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:50:18 | INFO | fairseq.trainer | begin training epoch 10\n","epoch 010: 100% 329/330 [00:56<00:00,  5.80it/s, loss=8.844, nll_loss=7.754, alignment_loss=2.354, ppl=215.86, wps=7927.9, ups=2.88, wpb=2751.1, bsz=126.9, num_updates=3000, lr=0.00015, gnorm=2.262, loss_scale=0.0625, train_wall=168, wall=1060]2021-05-29 14:51:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.96it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.47it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.40it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.32it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.26it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.38it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:51:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.001 | nll_loss 6.742 | alignment_loss 0 | ppl 107.08 | wps 44240.2 | wpb 2006.9 | bsz 98.8 | num_updates 3289 | best_loss 8.001\n","2021-05-29 14:51:16 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:52:15 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint10.pt (epoch 10 @ 3289 updates, score 8.001) (writing took 58.88299413799996 seconds)\n","2021-05-29 14:52:15 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2021-05-29 14:52:15 | INFO | train | epoch 010 | loss 8.143 | nll_loss 6.96 | alignment_loss 2.146 | ppl 124.49 | wps 7772.6 | ups 2.82 | wpb 2752.1 | bsz 127.4 | num_updates 3289 | lr 0.00016445 | gnorm 2.205 | loss_scale 0.0625 | train_wall 56 | wall 1169\n","epoch 011:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:52:15 | INFO | fairseq.trainer | begin training epoch 11\n","epoch 011:   2% 6/330 [00:01<01:28,  3.66it/s]2021-05-29 14:52:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\n","epoch 011: 100% 329/330 [00:56<00:00,  6.18it/s]2021-05-29 14:53:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 011 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.16it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.69it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.61it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.49it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.41it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.51it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:53:12 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.756 | nll_loss 6.447 | alignment_loss 0 | ppl 87.22 | wps 43970.7 | wpb 2006.9 | bsz 98.8 | num_updates 3618 | best_loss 7.756\n","2021-05-29 14:53:12 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:54:13 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint11.pt (epoch 11 @ 3618 updates, score 7.756) (writing took 60.59654477000004 seconds)\n","2021-05-29 14:54:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n","2021-05-29 14:54:13 | INFO | train | epoch 011 | loss 7.78 | nll_loss 6.548 | alignment_loss 2.024 | ppl 93.56 | wps 7647.3 | ups 2.78 | wpb 2751.7 | bsz 127.5 | num_updates 3618 | lr 0.0001809 | gnorm 2.154 | loss_scale 0.0312 | train_wall 55 | wall 1288\n","epoch 012:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:54:13 | INFO | fairseq.trainer | begin training epoch 12\n","epoch 012: 100% 329/330 [00:56<00:00,  6.36it/s]2021-05-29 14:55:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 012 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.01it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.67it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.59it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.47it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.37it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.49it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:55:11 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.515 | nll_loss 6.149 | alignment_loss 0 | ppl 70.95 | wps 44768 | wpb 2006.9 | bsz 98.8 | num_updates 3948 | best_loss 7.515\n","2021-05-29 14:55:11 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:56:05 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint12.pt (epoch 12 @ 3948 updates, score 7.515) (writing took 53.998900094999954 seconds)\n","2021-05-29 14:56:05 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n","2021-05-29 14:56:05 | INFO | train | epoch 012 | loss 7.412 | nll_loss 6.13 | alignment_loss 1.891 | ppl 70.06 | wps 8132.9 | ups 2.96 | wpb 2752.1 | bsz 127.4 | num_updates 3948 | lr 0.0001974 | gnorm 2.104 | loss_scale 0.0312 | train_wall 55 | wall 1399\n","epoch 013:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:56:05 | INFO | fairseq.trainer | begin training epoch 13\n","epoch 013: 100% 329/330 [00:56<00:00,  6.12it/s, loss=7.72, nll_loss=6.48, alignment_loss=1.998, ppl=89.25, wps=7907.5, ups=2.87, wpb=2755.3, bsz=128.3, num_updates=4000, lr=0.0002, gnorm=2.141, loss_scale=0.0312, train_wall=168, wall=1409]2021-05-29 14:57:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 013 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.82it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.57it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.46it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.52it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:57:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.218 | nll_loss 5.808 | alignment_loss 0 | ppl 56.03 | wps 43990.9 | wpb 2006.9 | bsz 98.8 | num_updates 4278 | best_loss 7.218\n","2021-05-29 14:57:02 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:58:00 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint13.pt (epoch 13 @ 4278 updates, score 7.218) (writing took 57.88110281900026 seconds)\n","2021-05-29 14:58:00 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2021-05-29 14:58:00 | INFO | train | epoch 013 | loss 7.038 | nll_loss 5.706 | alignment_loss 1.769 | ppl 52.19 | wps 7858.8 | ups 2.86 | wpb 2752.1 | bsz 127.4 | num_updates 4278 | lr 0.000193392 | gnorm 2.024 | loss_scale 0.0312 | train_wall 56 | wall 1515\n","epoch 014:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:58:01 | INFO | fairseq.trainer | begin training epoch 14\n","epoch 014: 100% 329/330 [00:56<00:00,  6.01it/s]2021-05-29 14:58:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 014 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.29it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.95it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.82it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.69it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.57it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.72it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 14:58:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.047 | nll_loss 5.624 | alignment_loss 0 | ppl 49.3 | wps 44395.9 | wpb 2006.9 | bsz 98.8 | num_updates 4608 | best_loss 7.047\n","2021-05-29 14:58:58 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 14:59:57 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint14.pt (epoch 14 @ 4608 updates, score 7.047) (writing took 58.987497025000266 seconds)\n","2021-05-29 14:59:57 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2021-05-29 14:59:57 | INFO | train | epoch 014 | loss 6.7 | nll_loss 5.322 | alignment_loss 1.67 | ppl 39.99 | wps 7767.3 | ups 2.82 | wpb 2752.1 | bsz 127.4 | num_updates 4608 | lr 0.000186339 | gnorm 1.989 | loss_scale 0.0312 | train_wall 56 | wall 1632\n","epoch 015:   0% 0/330 [00:00<?, ?it/s]2021-05-29 14:59:57 | INFO | fairseq.trainer | begin training epoch 15\n","epoch 015: 100% 329/330 [00:56<00:00,  6.01it/s]2021-05-29 15:00:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 015 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.86it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.46it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.37it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.26it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.09it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.18it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:00:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.846 | nll_loss 5.359 | alignment_loss 0 | ppl 41.04 | wps 44244.1 | wpb 2006.9 | bsz 98.8 | num_updates 4938 | best_loss 6.846\n","2021-05-29 15:00:55 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:01:54 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint15.pt (epoch 15 @ 4938 updates, score 6.846) (writing took 59.67757466300009 seconds)\n","2021-05-29 15:01:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2021-05-29 15:01:54 | INFO | train | epoch 015 | loss 6.407 | nll_loss 4.989 | alignment_loss 1.588 | ppl 31.76 | wps 7735.2 | ups 2.81 | wpb 2752.1 | bsz 127.4 | num_updates 4938 | lr 0.000180005 | gnorm 1.91 | loss_scale 0.0312 | train_wall 55 | wall 1749\n","epoch 016:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:01:55 | INFO | fairseq.trainer | begin training epoch 16\n","epoch 016:  35% 115/330 [00:19<00:36,  5.96it/s, loss=6.666, nll_loss=5.283, alignment_loss=1.663, ppl=38.93, wps=7816.5, ups=2.84, wpb=2748.6, bsz=125.8, num_updates=5000, lr=0.000178885, gnorm=1.966, loss_scale=0.0312, train_wall=168, wall=1760]2021-05-29 15:02:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625\n","epoch 016: 100% 329/330 [00:56<00:00,  6.14it/s, loss=6.666, nll_loss=5.283, alignment_loss=1.663, ppl=38.93, wps=7816.5, ups=2.84, wpb=2748.6, bsz=125.8, num_updates=5000, lr=0.000178885, gnorm=1.966, loss_scale=0.0312, train_wall=168, wall=1760]2021-05-29 15:02:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 016 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.55it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.20it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.15it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.03it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.88it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.97it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:02:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.738 | nll_loss 5.178 | alignment_loss 0 | ppl 36.21 | wps 44746.9 | wpb 2006.9 | bsz 98.8 | num_updates 5267 | best_loss 6.738\n","2021-05-29 15:02:52 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:03:50 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint16.pt (epoch 16 @ 5267 updates, score 6.738) (writing took 58.442134251000425 seconds)\n","2021-05-29 15:03:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2021-05-29 15:03:50 | INFO | train | epoch 016 | loss 6.153 | nll_loss 4.677 | alignment_loss 1.519 | ppl 25.58 | wps 7833.3 | ups 2.85 | wpb 2752.9 | bsz 127.6 | num_updates 5267 | lr 0.000174292 | gnorm 1.857 | loss_scale 0.0156 | train_wall 55 | wall 1865\n","epoch 017:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:03:51 | INFO | fairseq.trainer | begin training epoch 17\n","epoch 017:   9% 30/330 [00:05<00:49,  6.01it/s]2021-05-29 15:03:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125\n","epoch 017: 100% 329/330 [00:55<00:00,  6.04it/s]2021-05-29 15:04:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 017 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.13it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  9.94it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  47% 7/15 [00:00<00:00, 11.81it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 13.70it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  87% 13/15 [00:00<00:00, 15.52it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:04:47 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.642 | nll_loss 5.066 | alignment_loss 0 | ppl 33.49 | wps 45102 | wpb 2006.9 | bsz 98.8 | num_updates 5596 | best_loss 6.642\n","2021-05-29 15:04:47 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:05:46 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint17.pt (epoch 17 @ 5596 updates, score 6.642) (writing took 58.28691148300004 seconds)\n","2021-05-29 15:05:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2021-05-29 15:05:46 | INFO | train | epoch 017 | loss 5.946 | nll_loss 4.417 | alignment_loss 1.468 | ppl 21.36 | wps 7845.5 | ups 2.85 | wpb 2752.9 | bsz 127.6 | num_updates 5596 | lr 0.000169091 | gnorm 1.801 | loss_scale 0.0078 | train_wall 55 | wall 1980\n","epoch 018:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:05:46 | INFO | fairseq.trainer | begin training epoch 18\n","epoch 018: 100% 329/330 [00:55<00:00,  5.88it/s]2021-05-29 15:06:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 018 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.43it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.08it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.98it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.86it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.77it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.75it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:06:43 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.566 | nll_loss 4.941 | alignment_loss 0 | ppl 30.72 | wps 44272.9 | wpb 2006.9 | bsz 98.8 | num_updates 5926 | best_loss 6.566\n","2021-05-29 15:06:43 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:07:42 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint18.pt (epoch 18 @ 5926 updates, score 6.566) (writing took 59.15809544399963 seconds)\n","2021-05-29 15:07:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2021-05-29 15:07:42 | INFO | train | epoch 018 | loss 5.75 | nll_loss 4.184 | alignment_loss 1.416 | ppl 18.18 | wps 7807.1 | ups 2.84 | wpb 2752.1 | bsz 127.4 | num_updates 5926 | lr 0.000164316 | gnorm 1.768 | loss_scale 0.0078 | train_wall 55 | wall 2097\n","epoch 019:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:07:42 | INFO | fairseq.trainer | begin training epoch 19\n","epoch 019: 100% 329/330 [00:56<00:00,  6.16it/s, loss=5.908, nll_loss=4.374, alignment_loss=1.457, ppl=20.73, wps=7870.3, ups=2.86, wpb=2754.6, bsz=128.4, num_updates=6000, lr=0.000163299, gnorm=1.804, loss_scale=0.0078, train_wall=167, wall=2110]2021-05-29 15:08:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.22it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.90it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.81it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.73it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.63it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.63it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:08:40 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.555 | nll_loss 4.939 | alignment_loss 0 | ppl 30.67 | wps 44700.3 | wpb 2006.9 | bsz 98.8 | num_updates 6256 | best_loss 6.555\n","2021-05-29 15:08:40 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:09:39 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint19.pt (epoch 19 @ 6256 updates, score 6.555) (writing took 59.40453899100066 seconds)\n","2021-05-29 15:09:39 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2021-05-29 15:09:39 | INFO | train | epoch 019 | loss 5.586 | nll_loss 3.988 | alignment_loss 1.375 | ppl 15.87 | wps 7755 | ups 2.82 | wpb 2752.1 | bsz 127.4 | num_updates 6256 | lr 0.000159923 | gnorm 1.765 | loss_scale 0.0078 | train_wall 55 | wall 2214\n","epoch 020:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:09:39 | INFO | fairseq.trainer | begin training epoch 20\n","epoch 020:  86% 284/330 [00:48<00:07,  5.80it/s]2021-05-29 15:10:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00390625\n","epoch 020: 100% 329/330 [00:56<00:00,  6.20it/s]2021-05-29 15:10:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 020 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.91it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  9.75it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  47% 7/15 [00:00<00:00, 11.62it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 13.57it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  87% 13/15 [00:00<00:00, 15.39it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:10:36 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.474 | nll_loss 4.846 | alignment_loss 0 | ppl 28.76 | wps 45519.6 | wpb 2006.9 | bsz 98.8 | num_updates 6585 | best_loss 6.474\n","2021-05-29 15:10:36 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:11:34 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint20.pt (epoch 20 @ 6585 updates, score 6.474) (writing took 57.80160870200052 seconds)\n","2021-05-29 15:11:34 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2021-05-29 15:11:34 | INFO | train | epoch 020 | loss 5.434 | nll_loss 3.809 | alignment_loss 1.338 | ppl 14.02 | wps 7878.7 | ups 2.86 | wpb 2755.9 | bsz 127.5 | num_updates 6585 | lr 0.000155877 | gnorm 1.753 | loss_scale 0.0039 | train_wall 55 | wall 2329\n","epoch 021:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:11:35 | INFO | fairseq.trainer | begin training epoch 21\n","epoch 021: 100% 329/330 [00:56<00:00,  6.15it/s]2021-05-29 15:12:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 021 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.60it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.28it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.28it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.17it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.99it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.07it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:12:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.444 | nll_loss 4.81 | alignment_loss 0 | ppl 28.05 | wps 45141.2 | wpb 2006.9 | bsz 98.8 | num_updates 6915 | best_loss 6.444\n","2021-05-29 15:12:32 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:13:30 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint21.pt (epoch 21 @ 6915 updates, score 6.444) (writing took 58.249427396999636 seconds)\n","2021-05-29 15:13:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2021-05-29 15:13:30 | INFO | train | epoch 021 | loss 5.282 | nll_loss 3.647 | alignment_loss 1.305 | ppl 12.53 | wps 7817.8 | ups 2.84 | wpb 2752.1 | bsz 127.4 | num_updates 6915 | lr 0.000152112 | gnorm 1.775 | loss_scale 0.0039 | train_wall 55 | wall 2445\n","epoch 022:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:13:31 | INFO | fairseq.trainer | begin training epoch 22\n","epoch 022: 100% 329/330 [00:55<00:00,  6.18it/s, loss=5.393, nll_loss=3.769, alignment_loss=1.331, ppl=13.63, wps=7862.1, ups=2.86, wpb=2749.1, bsz=127.8, num_updates=7000, lr=0.000151186, gnorm=1.755, loss_scale=0.0039, train_wall=167, wall=2460]2021-05-29 15:14:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 022 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.05it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.66it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.57it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.46it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.40it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.50it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:14:27 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.401 | nll_loss 4.763 | alignment_loss 0 | ppl 27.15 | wps 44517.7 | wpb 2006.9 | bsz 98.8 | num_updates 7245 | best_loss 6.401\n","2021-05-29 15:14:27 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:15:22 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint22.pt (epoch 22 @ 7245 updates, score 6.401) (writing took 55.021314036999684 seconds)\n","2021-05-29 15:15:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2021-05-29 15:15:22 | INFO | train | epoch 022 | loss 5.135 | nll_loss 3.483 | alignment_loss 1.268 | ppl 11.18 | wps 8098.7 | ups 2.94 | wpb 2752.1 | bsz 127.4 | num_updates 7245 | lr 0.000148608 | gnorm 1.713 | loss_scale 0.0039 | train_wall 55 | wall 2557\n","epoch 023:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:15:23 | INFO | fairseq.trainer | begin training epoch 23\n","epoch 023: 100% 329/330 [00:56<00:00,  5.92it/s]2021-05-29 15:16:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 023 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.51it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.13it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.04it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.91it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.81it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.88it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:16:20 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.352 | nll_loss 4.71 | alignment_loss 0 | ppl 26.18 | wps 44317 | wpb 2006.9 | bsz 98.8 | num_updates 7575 | best_loss 6.352\n","2021-05-29 15:16:20 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:17:16 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint23.pt (epoch 23 @ 7575 updates, score 6.352) (writing took 56.198146349000126 seconds)\n","2021-05-29 15:17:16 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2021-05-29 15:17:16 | INFO | train | epoch 023 | loss 5.018 | nll_loss 3.351 | alignment_loss 1.242 | ppl 10.21 | wps 7989.9 | ups 2.9 | wpb 2752.1 | bsz 127.4 | num_updates 7575 | lr 0.000145334 | gnorm 1.742 | loss_scale 0.0039 | train_wall 55 | wall 2671\n","epoch 024:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:17:16 | INFO | fairseq.trainer | begin training epoch 24\n","epoch 024: 100% 329/330 [00:57<00:00,  6.17it/s]2021-05-29 15:18:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 024 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.81it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.55it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.48it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.34it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.17it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.23it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:18:15 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.364 | nll_loss 4.711 | alignment_loss 0 | ppl 26.2 | wps 45013.1 | wpb 2006.9 | bsz 98.8 | num_updates 7905 | best_loss 6.352\n","2021-05-29 15:18:15 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:18:53 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint24.pt (epoch 24 @ 7905 updates, score 6.364) (writing took 38.14268196299963 seconds)\n","2021-05-29 15:18:53 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n","2021-05-29 15:18:53 | INFO | train | epoch 024 | loss 4.908 | nll_loss 3.226 | alignment_loss 1.216 | ppl 9.36 | wps 9386.9 | ups 3.41 | wpb 2752.1 | bsz 127.4 | num_updates 7905 | lr 0.000142269 | gnorm 1.751 | loss_scale 0.0039 | train_wall 55 | wall 2767\n","epoch 025:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:18:53 | INFO | fairseq.trainer | begin training epoch 25\n","epoch 025: 100% 329/330 [00:55<00:00,  6.03it/s, loss=4.987, nll_loss=3.316, alignment_loss=1.231, ppl=9.96, wps=8520.8, ups=3.08, wpb=2764.4, bsz=127.6, num_updates=8000, lr=0.000141421, gnorm=1.732, loss_scale=0.0039, train_wall=167, wall=2784]2021-05-29 15:19:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 025 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.08it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.83it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.76it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.60it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.40it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.42it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:19:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.354 | nll_loss 4.689 | alignment_loss 0 | ppl 25.79 | wps 44881.6 | wpb 2006.9 | bsz 98.8 | num_updates 8235 | best_loss 6.352\n","2021-05-29 15:19:50 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:20:23 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint25.pt (epoch 25 @ 8235 updates, score 6.354) (writing took 33.4461232270005 seconds)\n","2021-05-29 15:20:23 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n","2021-05-29 15:20:23 | INFO | train | epoch 025 | loss 4.797 | nll_loss 3.101 | alignment_loss 1.188 | ppl 8.58 | wps 10059.8 | ups 3.66 | wpb 2752.1 | bsz 127.4 | num_updates 8235 | lr 0.000139389 | gnorm 1.722 | loss_scale 0.0039 | train_wall 55 | wall 2858\n","epoch 026:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:20:23 | INFO | fairseq.trainer | begin training epoch 26\n","epoch 026: 100% 329/330 [00:55<00:00,  6.13it/s]2021-05-29 15:21:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 026 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.55it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.28it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.23it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.11it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.95it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.03it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:21:20 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.371 | nll_loss 4.721 | alignment_loss 0 | ppl 26.37 | wps 45142.7 | wpb 2006.9 | bsz 98.8 | num_updates 8565 | best_loss 6.352\n","2021-05-29 15:21:20 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:22:00 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint26.pt (epoch 26 @ 8565 updates, score 6.371) (writing took 40.11533442600012 seconds)\n","2021-05-29 15:22:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n","2021-05-29 15:22:00 | INFO | train | epoch 026 | loss 4.701 | nll_loss 2.99 | alignment_loss 1.166 | ppl 7.94 | wps 9395 | ups 3.41 | wpb 2752.1 | bsz 127.4 | num_updates 8565 | lr 0.000136677 | gnorm 1.742 | loss_scale 0.0039 | train_wall 55 | wall 2954\n","epoch 027:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:22:00 | INFO | fairseq.trainer | begin training epoch 27\n","epoch 027: 100% 329/330 [00:55<00:00,  6.16it/s]2021-05-29 15:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 027 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.99it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.66it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.60it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.46it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.30it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.21it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:22:56 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.396 | nll_loss 4.731 | alignment_loss 0 | ppl 26.56 | wps 44378.6 | wpb 2006.9 | bsz 98.8 | num_updates 8895 | best_loss 6.352\n","2021-05-29 15:22:56 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:23:32 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint27.pt (epoch 27 @ 8895 updates, score 6.396) (writing took 35.64100445300028 seconds)\n","2021-05-29 15:23:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n","2021-05-29 15:23:32 | INFO | train | epoch 027 | loss 4.624 | nll_loss 2.901 | alignment_loss 1.149 | ppl 7.47 | wps 9874.4 | ups 3.59 | wpb 2752.1 | bsz 127.4 | num_updates 8895 | lr 0.000134118 | gnorm 1.796 | loss_scale 0.0039 | train_wall 54 | wall 3046\n","epoch 028:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:23:32 | INFO | fairseq.trainer | begin training epoch 28\n","epoch 028:  19% 63/330 [00:11<00:44,  5.99it/s]2021-05-29 15:23:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.001953125\n","epoch 028:  54% 177/330 [00:30<00:25,  5.96it/s, loss=4.681, nll_loss=2.966, alignment_loss=1.163, ppl=7.82, wps=9796.9, ups=3.57, wpb=2747.3, bsz=127.7, num_updates=9000, lr=0.000133333, gnorm=1.762, loss_scale=0.002, train_wall=166, wall=3065]2021-05-29 15:24:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0009765625\n","epoch 028: 100% 329/330 [00:55<00:00,  6.06it/s, loss=4.681, nll_loss=2.966, alignment_loss=1.163, ppl=7.82, wps=9796.9, ups=3.57, wpb=2747.3, bsz=127.7, num_updates=9000, lr=0.000133333, gnorm=1.762, loss_scale=0.002, train_wall=166, wall=3065]2021-05-29 15:24:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 028 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.89it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.59it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.57it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.27it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.31it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:24:28 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.653 | nll_loss 5.03 | alignment_loss 0 | ppl 32.66 | wps 45107.5 | wpb 2006.9 | bsz 98.8 | num_updates 9223 | best_loss 6.352\n","2021-05-29 15:24:28 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:25:07 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint28.pt (epoch 28 @ 9223 updates, score 6.653) (writing took 38.958952859999954 seconds)\n","2021-05-29 15:25:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n","2021-05-29 15:25:07 | INFO | train | epoch 028 | loss 4.527 | nll_loss 2.789 | alignment_loss 1.125 | ppl 6.91 | wps 9460.3 | ups 3.44 | wpb 2753.6 | bsz 127.8 | num_updates 9223 | lr 0.000131712 | gnorm 1.779 | loss_scale 0.001 | train_wall 55 | wall 3142\n","epoch 029:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:25:07 | INFO | fairseq.trainer | begin training epoch 29\n","epoch 029:  32% 106/330 [00:18<00:36,  6.20it/s]2021-05-29 15:25:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00048828125\n","epoch 029: 100% 329/330 [00:55<00:00,  6.28it/s]2021-05-29 15:26:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 029 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.58it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.24it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.20it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.05it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.93it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.96it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:26:04 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.689 | nll_loss 5.053 | alignment_loss 0 | ppl 33.21 | wps 44657.2 | wpb 2006.9 | bsz 98.8 | num_updates 9552 | best_loss 6.352\n","2021-05-29 15:26:04 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:26:40 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint29.pt (epoch 29 @ 9552 updates, score 6.689) (writing took 35.92346756900042 seconds)\n","2021-05-29 15:26:40 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n","2021-05-29 15:26:40 | INFO | train | epoch 029 | loss 4.494 | nll_loss 2.748 | alignment_loss 1.105 | ppl 6.72 | wps 9801.7 | ups 3.56 | wpb 2752.9 | bsz 127.6 | num_updates 9552 | lr 0.000129423 | gnorm 1.841 | loss_scale 0.0005 | train_wall 55 | wall 3234\n","epoch 030:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:26:40 | INFO | fairseq.trainer | begin training epoch 30\n","epoch 030:  49% 161/330 [00:26<00:26,  6.29it/s]2021-05-29 15:27:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.000244140625\n","epoch 030: 100% 329/330 [00:55<00:00,  6.01it/s]2021-05-29 15:27:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 030 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.60it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.17it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.04it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 11.95it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 13.85it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.00it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:27:36 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.961 | nll_loss 5.346 | alignment_loss 0 | ppl 40.68 | wps 44199.9 | wpb 2006.9 | bsz 98.8 | num_updates 9881 | best_loss 6.352\n","2021-05-29 15:27:36 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:28:14 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint30.pt (epoch 30 @ 9881 updates, score 6.961) (writing took 37.6688520509997 seconds)\n","2021-05-29 15:28:14 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n","2021-05-29 15:28:14 | INFO | train | epoch 030 | loss 4.488 | nll_loss 2.732 | alignment_loss 1.086 | ppl 6.65 | wps 9636.5 | ups 3.5 | wpb 2752.9 | bsz 127.6 | num_updates 9881 | lr 0.000127251 | gnorm 1.9 | loss_scale 0.0002 | train_wall 55 | wall 3328\n","epoch 031:   0% 0/330 [00:00<?, ?it/s]2021-05-29 15:28:14 | INFO | fairseq.trainer | begin training epoch 31\n","epoch 031:  36% 118/330 [00:20<00:36,  5.86it/s]2021-05-29 15:28:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 031 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.72it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.12it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.84it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.69it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 12.60it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 14.63it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 15:28:35 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.822 | nll_loss 5.205 | alignment_loss 0 | ppl 36.9 | wps 41803.5 | wpb 2006.9 | bsz 98.8 | num_updates 10000 | best_loss 6.352\n","2021-05-29 15:28:35 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 15:28:41 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h1-l4-alignment-lambda50p/checkpoints/checkpoint_last.pt (epoch 31 @ 10000 updates, score 6.822) (writing took 6.6615484729991294 seconds)\n","2021-05-29 15:28:41 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n","2021-05-29 15:28:41 | INFO | train | epoch 031 | loss 4.437 | nll_loss 2.667 | alignment_loss 1.07 | ppl 6.35 | wps 11895.6 | ups 4.26 | wpb 2789.9 | bsz 129.1 | num_updates 10000 | lr 0.000126491 | gnorm 1.931 | loss_scale 0.0002 | train_wall 20 | wall 3356\n","2021-05-29 15:28:41 | INFO | fairseq_cli.train | done training in 3355.8 seconds\n","\n","real\t56m18.126s\n","user\t30m53.168s\n","sys\t3m26.747s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aRzi2usWjHDG"},"source":["##Train baseline with slignment-lambda=0.05 and 8 heads"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Z-fKF8ZxwZZ1","executionInfo":{"status":"ok","timestamp":1622309346294,"user_tz":-420,"elapsed":351,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"1cb361d4-906b-4a8c-9a4b-ea86ece3aec0"},"source":["MODEL"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'en2wovi-align0lw-h8average-l4-alignment-lambda5p'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"Kl1w0S9_sy-M"},"source":["!time fairseq-train --alignment-heads 8\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.05\\\n","    --fp16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h00X4nyUKl9A","executionInfo":{"status":"ok","timestamp":1622309756092,"user_tz":-420,"elapsed":104859,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"4fd216d8-a6df-49f0-bd35-5d9987a8a34c"},"source":["!time fairseq-train --alignment-heads 8\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.02 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.05\\\n","    --fp16"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-29 17:34:12 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, alignment_heads=8, alignment_lambda=0.05, alignment_layer=4, all_gather_list_size=16384, arch='transformer_align', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_with_alignment', cross_self_attention=False, curriculum=0, data='en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, full_context_alignment=False, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=True, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.02], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='en2wovi-align0lw-h8average-l4-alignment-lambda5p/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n","2021-05-29 17:34:12 | INFO | fairseq.tasks.translation | [en] dictionary: 56792 types\n","2021-05-29 17:34:12 | INFO | fairseq.tasks.translation | [wovi] dictionary: 56792 types\n","2021-05-29 17:34:12 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized/valid.en-wovi.en\n","2021-05-29 17:34:12 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized/valid.en-wovi.wovi\n","2021-05-29 17:34:12 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized valid en-wovi 1482 examples\n","2021-05-29 17:34:13 | INFO | fairseq_cli.train | TransformerAlignModel(\n","  (encoder): TransformerEncoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=512, out_features=56792, bias=False)\n","  )\n",")\n","2021-05-29 17:34:13 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n","2021-05-29 17:34:13 | INFO | fairseq_cli.train | model: transformer_align (TransformerAlignModel)\n","2021-05-29 17:34:13 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_with_alignment (LabelSmoothedCrossEntropyCriterionWithAlignment)\n","2021-05-29 17:34:13 | INFO | fairseq_cli.train | num. model params: 73216000 (num. trained: 73216000)\n","2021-05-29 17:34:17 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n","2021-05-29 17:34:17 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n","2021-05-29 17:34:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 17:34:17 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2021-05-29 17:34:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 17:34:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2021-05-29 17:34:17 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n","2021-05-29 17:34:20 | INFO | fairseq.trainer | loaded checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda5p/checkpoints/checkpoint_last.pt (epoch 20 @ 6251 updates)\n","2021-05-29 17:34:20 | INFO | fairseq.trainer | loading train data for epoch 20\n","2021-05-29 17:34:20 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized/train.en-wovi.en\n","2021-05-29 17:34:20 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized/train.en-wovi.wovi\n","2021-05-29 17:34:20 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized train en-wovi 42026 examples\n","2021-05-29 17:34:20 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda5p/binarized/train.align.en-wovi\n","epoch 020:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:34:20 | INFO | fairseq.trainer | begin training epoch 20\n","epoch 020: 100% 329/330 [00:50<00:00,  6.58it/s]2021-05-29 17:35:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 020 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.55it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  8.29it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  47% 7/15 [00:00<00:00, 10.22it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 12.31it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  87% 13/15 [00:00<00:00, 14.50it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:35:11 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 13.273 | nll_loss 11.191 | alignment_loss 0 | ppl 2337.92 | wps 48579.1 | wpb 2006.9 | bsz 98.8 | num_updates 6581 | best_loss 6.779\n","2021-05-29 17:35:11 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:35:42 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda5p/checkpoints/checkpoint20.pt (epoch 20 @ 6581 updates, score 13.273) (writing took 30.383012723999855 seconds)\n","2021-05-29 17:35:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2021-05-29 17:35:42 | INFO | train | epoch 020 | loss 9.127 | nll_loss 7.755 | alignment_loss 3.434 | ppl 216.03 | wps 13126.7 | ups 4.77 | wpb 2752.8 | bsz 127.5 | num_updates 6581 | lr 0.0155924 | gnorm 1.315 | loss_scale 0.0002 | train_wall 104 | wall 0\n","epoch 021:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:35:42 | INFO | fairseq.trainer | begin training epoch 21\n","epoch 021:  22% 71/330 [00:11<00:40,  6.34it/s]2021-05-29 17:35:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0001220703125\n","epoch 021:  22% 72/330 [00:11<00:39,  6.47it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:785: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n","2021-05-29 17:35:54 | INFO | fairseq.nan_detector | Detected nan/inf grad norm, dumping norms...\n","2021-05-29 17:35:54 | INFO | fairseq.nan_detector | norms: {'encoder.embed_tokens.weight': nan, 'encoder.layers.0.self_attn.k_proj.weight': nan, 'encoder.layers.0.self_attn.k_proj.bias': nan, 'encoder.layers.0.self_attn.v_proj.weight': nan, 'encoder.layers.0.self_attn.v_proj.bias': nan, 'encoder.layers.0.self_attn.q_proj.weight': nan, 'encoder.layers.0.self_attn.q_proj.bias': nan, 'encoder.layers.0.self_attn.out_proj.weight': nan, 'encoder.layers.0.self_attn.out_proj.bias': nan, 'encoder.layers.0.self_attn_layer_norm.weight': nan, 'encoder.layers.0.self_attn_layer_norm.bias': nan, 'encoder.layers.0.fc1.weight': nan, 'encoder.layers.0.fc1.bias': nan, 'encoder.layers.0.fc2.weight': nan, 'encoder.layers.0.fc2.bias': nan, 'encoder.layers.0.final_layer_norm.weight': nan, 'encoder.layers.0.final_layer_norm.bias': nan, 'encoder.layers.1.self_attn.k_proj.weight': nan, 'encoder.layers.1.self_attn.k_proj.bias': nan, 'encoder.layers.1.self_attn.v_proj.weight': nan, 'encoder.layers.1.self_attn.v_proj.bias': nan, 'encoder.layers.1.self_attn.q_proj.weight': nan, 'encoder.layers.1.self_attn.q_proj.bias': nan, 'encoder.layers.1.self_attn.out_proj.weight': nan, 'encoder.layers.1.self_attn.out_proj.bias': nan, 'encoder.layers.1.self_attn_layer_norm.weight': nan, 'encoder.layers.1.self_attn_layer_norm.bias': nan, 'encoder.layers.1.fc1.weight': nan, 'encoder.layers.1.fc1.bias': nan, 'encoder.layers.1.fc2.weight': nan, 'encoder.layers.1.fc2.bias': nan, 'encoder.layers.1.final_layer_norm.weight': nan, 'encoder.layers.1.final_layer_norm.bias': nan, 'encoder.layers.2.self_attn.k_proj.weight': nan, 'encoder.layers.2.self_attn.k_proj.bias': nan, 'encoder.layers.2.self_attn.v_proj.weight': nan, 'encoder.layers.2.self_attn.v_proj.bias': nan, 'encoder.layers.2.self_attn.q_proj.weight': nan, 'encoder.layers.2.self_attn.q_proj.bias': nan, 'encoder.layers.2.self_attn.out_proj.weight': nan, 'encoder.layers.2.self_attn.out_proj.bias': nan, 'encoder.layers.2.self_attn_layer_norm.weight': nan, 'encoder.layers.2.self_attn_layer_norm.bias': nan, 'encoder.layers.2.fc1.weight': nan, 'encoder.layers.2.fc1.bias': nan, 'encoder.layers.2.fc2.weight': nan, 'encoder.layers.2.fc2.bias': nan, 'encoder.layers.2.final_layer_norm.weight': nan, 'encoder.layers.2.final_layer_norm.bias': nan, 'encoder.layers.3.self_attn.k_proj.weight': nan, 'encoder.layers.3.self_attn.k_proj.bias': nan, 'encoder.layers.3.self_attn.v_proj.weight': nan, 'encoder.layers.3.self_attn.v_proj.bias': nan, 'encoder.layers.3.self_attn.q_proj.weight': nan, 'encoder.layers.3.self_attn.q_proj.bias': nan, 'encoder.layers.3.self_attn.out_proj.weight': nan, 'encoder.layers.3.self_attn.out_proj.bias': nan, 'encoder.layers.3.self_attn_layer_norm.weight': nan, 'encoder.layers.3.self_attn_layer_norm.bias': nan, 'encoder.layers.3.fc1.weight': nan, 'encoder.layers.3.fc1.bias': nan, 'encoder.layers.3.fc2.weight': nan, 'encoder.layers.3.fc2.bias': nan, 'encoder.layers.3.final_layer_norm.weight': nan, 'encoder.layers.3.final_layer_norm.bias': nan, 'encoder.layers.4.self_attn.k_proj.weight': nan, 'encoder.layers.4.self_attn.k_proj.bias': nan, 'encoder.layers.4.self_attn.v_proj.weight': nan, 'encoder.layers.4.self_attn.v_proj.bias': nan, 'encoder.layers.4.self_attn.q_proj.weight': nan, 'encoder.layers.4.self_attn.q_proj.bias': nan, 'encoder.layers.4.self_attn.out_proj.weight': nan, 'encoder.layers.4.self_attn.out_proj.bias': nan, 'encoder.layers.4.self_attn_layer_norm.weight': nan, 'encoder.layers.4.self_attn_layer_norm.bias': nan, 'encoder.layers.4.fc1.weight': nan, 'encoder.layers.4.fc1.bias': nan, 'encoder.layers.4.fc2.weight': nan, 'encoder.layers.4.fc2.bias': nan, 'encoder.layers.4.final_layer_norm.weight': nan, 'encoder.layers.4.final_layer_norm.bias': nan, 'encoder.layers.5.self_attn.k_proj.weight': nan, 'encoder.layers.5.self_attn.k_proj.bias': nan, 'encoder.layers.5.self_attn.v_proj.weight': nan, 'encoder.layers.5.self_attn.v_proj.bias': nan, 'encoder.layers.5.self_attn.q_proj.weight': nan, 'encoder.layers.5.self_attn.q_proj.bias': nan, 'encoder.layers.5.self_attn.out_proj.weight': nan, 'encoder.layers.5.self_attn.out_proj.bias': nan, 'encoder.layers.5.self_attn_layer_norm.weight': nan, 'encoder.layers.5.self_attn_layer_norm.bias': nan, 'encoder.layers.5.fc1.weight': nan, 'encoder.layers.5.fc1.bias': nan, 'encoder.layers.5.fc2.weight': nan, 'encoder.layers.5.fc2.bias': nan, 'encoder.layers.5.final_layer_norm.weight': nan, 'encoder.layers.5.final_layer_norm.bias': nan, 'decoder.layers.0.self_attn.k_proj.weight': nan, 'decoder.layers.0.self_attn.k_proj.bias': nan, 'decoder.layers.0.self_attn.v_proj.weight': nan, 'decoder.layers.0.self_attn.v_proj.bias': nan, 'decoder.layers.0.self_attn.q_proj.weight': nan, 'decoder.layers.0.self_attn.q_proj.bias': nan, 'decoder.layers.0.self_attn.out_proj.weight': nan, 'decoder.layers.0.self_attn.out_proj.bias': nan, 'decoder.layers.0.self_attn_layer_norm.weight': nan, 'decoder.layers.0.self_attn_layer_norm.bias': nan, 'decoder.layers.0.encoder_attn.k_proj.weight': nan, 'decoder.layers.0.encoder_attn.k_proj.bias': nan, 'decoder.layers.0.encoder_attn.v_proj.weight': nan, 'decoder.layers.0.encoder_attn.v_proj.bias': nan, 'decoder.layers.0.encoder_attn.q_proj.weight': nan, 'decoder.layers.0.encoder_attn.q_proj.bias': nan, 'decoder.layers.0.encoder_attn.out_proj.weight': nan, 'decoder.layers.0.encoder_attn.out_proj.bias': nan, 'decoder.layers.0.encoder_attn_layer_norm.weight': nan, 'decoder.layers.0.encoder_attn_layer_norm.bias': nan, 'decoder.layers.0.fc1.weight': nan, 'decoder.layers.0.fc1.bias': nan, 'decoder.layers.0.fc2.weight': nan, 'decoder.layers.0.fc2.bias': nan, 'decoder.layers.0.final_layer_norm.weight': nan, 'decoder.layers.0.final_layer_norm.bias': nan, 'decoder.layers.1.self_attn.k_proj.weight': nan, 'decoder.layers.1.self_attn.k_proj.bias': nan, 'decoder.layers.1.self_attn.v_proj.weight': nan, 'decoder.layers.1.self_attn.v_proj.bias': nan, 'decoder.layers.1.self_attn.q_proj.weight': nan, 'decoder.layers.1.self_attn.q_proj.bias': nan, 'decoder.layers.1.self_attn.out_proj.weight': nan, 'decoder.layers.1.self_attn.out_proj.bias': nan, 'decoder.layers.1.self_attn_layer_norm.weight': nan, 'decoder.layers.1.self_attn_layer_norm.bias': nan, 'decoder.layers.1.encoder_attn.k_proj.weight': nan, 'decoder.layers.1.encoder_attn.k_proj.bias': nan, 'decoder.layers.1.encoder_attn.v_proj.weight': nan, 'decoder.layers.1.encoder_attn.v_proj.bias': nan, 'decoder.layers.1.encoder_attn.q_proj.weight': nan, 'decoder.layers.1.encoder_attn.q_proj.bias': nan, 'decoder.layers.1.encoder_attn.out_proj.weight': nan, 'decoder.layers.1.encoder_attn.out_proj.bias': nan, 'decoder.layers.1.encoder_attn_layer_norm.weight': nan, 'decoder.layers.1.encoder_attn_layer_norm.bias': nan, 'decoder.layers.1.fc1.weight': nan, 'decoder.layers.1.fc1.bias': nan, 'decoder.layers.1.fc2.weight': nan, 'decoder.layers.1.fc2.bias': nan, 'decoder.layers.1.final_layer_norm.weight': nan, 'decoder.layers.1.final_layer_norm.bias': nan, 'decoder.layers.2.self_attn.k_proj.weight': nan, 'decoder.layers.2.self_attn.k_proj.bias': nan, 'decoder.layers.2.self_attn.v_proj.weight': nan, 'decoder.layers.2.self_attn.v_proj.bias': nan, 'decoder.layers.2.self_attn.q_proj.weight': nan, 'decoder.layers.2.self_attn.q_proj.bias': nan, 'decoder.layers.2.self_attn.out_proj.weight': nan, 'decoder.layers.2.self_attn.out_proj.bias': nan, 'decoder.layers.2.self_attn_layer_norm.weight': nan, 'decoder.layers.2.self_attn_layer_norm.bias': nan, 'decoder.layers.2.encoder_attn.k_proj.weight': nan, 'decoder.layers.2.encoder_attn.k_proj.bias': nan, 'decoder.layers.2.encoder_attn.v_proj.weight': nan, 'decoder.layers.2.encoder_attn.v_proj.bias': nan, 'decoder.layers.2.encoder_attn.q_proj.weight': nan, 'decoder.layers.2.encoder_attn.q_proj.bias': nan, 'decoder.layers.2.encoder_attn.out_proj.weight': nan, 'decoder.layers.2.encoder_attn.out_proj.bias': nan, 'decoder.layers.2.encoder_attn_layer_norm.weight': nan, 'decoder.layers.2.encoder_attn_layer_norm.bias': nan, 'decoder.layers.2.fc1.weight': nan, 'decoder.layers.2.fc1.bias': nan, 'decoder.layers.2.fc2.weight': nan, 'decoder.layers.2.fc2.bias': nan, 'decoder.layers.2.final_layer_norm.weight': nan, 'decoder.layers.2.final_layer_norm.bias': nan, 'decoder.layers.3.self_attn.k_proj.weight': nan, 'decoder.layers.3.self_attn.k_proj.bias': nan, 'decoder.layers.3.self_attn.v_proj.weight': nan, 'decoder.layers.3.self_attn.v_proj.bias': nan, 'decoder.layers.3.self_attn.q_proj.weight': nan, 'decoder.layers.3.self_attn.q_proj.bias': nan, 'decoder.layers.3.self_attn.out_proj.weight': nan, 'decoder.layers.3.self_attn.out_proj.bias': nan, 'decoder.layers.3.self_attn_layer_norm.weight': nan, 'decoder.layers.3.self_attn_layer_norm.bias': nan, 'decoder.layers.3.encoder_attn.k_proj.weight': nan, 'decoder.layers.3.encoder_attn.k_proj.bias': nan, 'decoder.layers.3.encoder_attn.v_proj.weight': nan, 'decoder.layers.3.encoder_attn.v_proj.bias': nan, 'decoder.layers.3.encoder_attn.q_proj.weight': nan, 'decoder.layers.3.encoder_attn.q_proj.bias': nan, 'decoder.layers.3.encoder_attn.out_proj.weight': nan, 'decoder.layers.3.encoder_attn.out_proj.bias': nan, 'decoder.layers.3.encoder_attn_layer_norm.weight': nan, 'decoder.layers.3.encoder_attn_layer_norm.bias': nan, 'decoder.layers.3.fc1.weight': nan, 'decoder.layers.3.fc1.bias': nan, 'decoder.layers.3.fc2.weight': nan, 'decoder.layers.3.fc2.bias': nan, 'decoder.layers.3.final_layer_norm.weight': nan, 'decoder.layers.3.final_layer_norm.bias': nan, 'decoder.layers.4.self_attn.k_proj.weight': nan, 'decoder.layers.4.self_attn.k_proj.bias': nan, 'decoder.layers.4.self_attn.v_proj.weight': nan, 'decoder.layers.4.self_attn.v_proj.bias': nan, 'decoder.layers.4.self_attn.q_proj.weight': nan, 'decoder.layers.4.self_attn.q_proj.bias': nan, 'decoder.layers.4.self_attn.out_proj.weight': nan, 'decoder.layers.4.self_attn.out_proj.bias': nan, 'decoder.layers.4.self_attn_layer_norm.weight': nan, 'decoder.layers.4.self_attn_layer_norm.bias': nan, 'decoder.layers.4.encoder_attn.k_proj.weight': nan, 'decoder.layers.4.encoder_attn.k_proj.bias': nan, 'decoder.layers.4.encoder_attn.v_proj.weight': 0.0, 'decoder.layers.4.encoder_attn.v_proj.bias': 0.0, 'decoder.layers.4.encoder_attn.q_proj.weight': nan, 'decoder.layers.4.encoder_attn.q_proj.bias': nan, 'decoder.layers.4.encoder_attn.out_proj.weight': 0.0, 'decoder.layers.4.encoder_attn.out_proj.bias': 0.0, 'decoder.layers.4.encoder_attn_layer_norm.weight': 0.0, 'decoder.layers.4.encoder_attn_layer_norm.bias': 0.0, 'decoder.layers.4.fc1.weight': 0.0, 'decoder.layers.4.fc1.bias': 0.0, 'decoder.layers.4.fc2.weight': 0.0, 'decoder.layers.4.fc2.bias': 0.0, 'decoder.layers.4.final_layer_norm.weight': 0.0, 'decoder.layers.4.final_layer_norm.bias': 0.0, 'decoder.layers.5.self_attn.k_proj.weight': 0.0, 'decoder.layers.5.self_attn.k_proj.bias': 0.0, 'decoder.layers.5.self_attn.v_proj.weight': 0.0, 'decoder.layers.5.self_attn.v_proj.bias': 0.0, 'decoder.layers.5.self_attn.q_proj.weight': 0.0, 'decoder.layers.5.self_attn.q_proj.bias': 0.0, 'decoder.layers.5.self_attn.out_proj.weight': 0.0, 'decoder.layers.5.self_attn.out_proj.bias': 0.0, 'decoder.layers.5.self_attn_layer_norm.weight': 0.0, 'decoder.layers.5.self_attn_layer_norm.bias': 0.0, 'decoder.layers.5.encoder_attn.k_proj.weight': 0.0, 'decoder.layers.5.encoder_attn.k_proj.bias': 0.0, 'decoder.layers.5.encoder_attn.v_proj.weight': 0.0, 'decoder.layers.5.encoder_attn.v_proj.bias': 0.0, 'decoder.layers.5.encoder_attn.q_proj.weight': 0.0, 'decoder.layers.5.encoder_attn.q_proj.bias': 0.0, 'decoder.layers.5.encoder_attn.out_proj.weight': 0.0, 'decoder.layers.5.encoder_attn.out_proj.bias': 0.0, 'decoder.layers.5.encoder_attn_layer_norm.weight': 0.0, 'decoder.layers.5.encoder_attn_layer_norm.bias': 0.0, 'decoder.layers.5.fc1.weight': 0.0, 'decoder.layers.5.fc1.bias': 0.0, 'decoder.layers.5.fc2.weight': 0.0, 'decoder.layers.5.fc2.bias': 0.0, 'decoder.layers.5.final_layer_norm.weight': 0.21350085735321045, 'decoder.layers.5.final_layer_norm.bias': 0.22060264647006989}\n","2021-05-29 17:35:54 | INFO | fairseq.nan_detector | gradients: {'encoder.embed_tokens.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), 'encoder.layers.0.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc1.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc1.bias': tensor([0., nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.0.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc1.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc1.bias': tensor([0., nan, nan,  ..., 0., 0., nan], device='cuda:0', dtype=torch.float16), 'encoder.layers.1.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.1.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc1.bias': tensor([nan, 0., 0.,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.2.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc1.bias': tensor([nan, nan, 0.,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.3.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.4.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc1.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), 'encoder.layers.5.fc1.bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float16), 'encoder.layers.5.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'encoder.layers.5.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc1.bias': tensor([nan, nan, 0.,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.0.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc1.bias': tensor([nan, 0., nan,  ..., nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.1.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), 'decoder.layers.2.fc1.bias': tensor([nan, nan, nan,  ..., nan, nan, 0.], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.2.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.encoder_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc1.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc1.bias': tensor([nan, 0., nan,  ..., nan, 0., nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc2.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.fc2.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.final_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.3.final_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.v_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.v_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.out_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn.out_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn_layer_norm.weight': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.self_attn_layer_norm.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.encoder_attn.k_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.encoder_attn.k_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.encoder_attn.q_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n","       dtype=torch.float16), 'decoder.layers.4.encoder_attn.q_proj.bias': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n","       dtype=torch.float16)}\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n","    sys.exit(cli_main())\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n","    distributed_utils.call_main(args, main)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n","    main(args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 125, in main\n","    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\n","  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n","    return func(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 208, in train\n","    log_output = trainer.train_step(samples)\n","  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n","    return func(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 580, in train_step\n","    grad_norm = self.clip_grad_norm(self.args.clip_norm)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 855, in clip_grad_norm\n","    return self.optimizer.clip_grad_norm(clip_norm, aggregate_norm_fn=None)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/fp16_optimizer.py\", line 181, in clip_grad_norm\n","    self.scaler.check_overflow(grad_norm)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/dynamic_loss_scaler.py\", line 66, in check_overflow\n","    ).format(self.min_loss_scale)\n","FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\n","\n","real\t1m44.402s\n","user\t1m16.724s\n","sys\t0m9.950s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tYrFjb3-jUpB"},"source":["##Train baseline with slignment-lambda=0.5 and 8 heads"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZglRPFvjOnu","executionInfo":{"status":"ok","timestamp":1622308492577,"user_tz":-420,"elapsed":3430739,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"0064f031-eeae-4473-bce0-872fae7be8ca"},"source":["!time fairseq-train --alignment-heads 8\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.5\\\n","    --fp16"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-29 16:17:42 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, alignment_heads=8, alignment_lambda=0.5, alignment_layer=4, all_gather_list_size=16384, arch='transformer_align', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_with_alignment', cross_self_attention=False, curriculum=0, data='en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, full_context_alignment=False, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=True, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n","2021-05-29 16:17:43 | INFO | fairseq.tasks.translation | [en] dictionary: 56792 types\n","2021-05-29 16:17:43 | INFO | fairseq.tasks.translation | [wovi] dictionary: 56792 types\n","2021-05-29 16:17:43 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized/valid.en-wovi.en\n","2021-05-29 16:17:43 | INFO | fairseq.data.data_utils | loaded 1482 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized/valid.en-wovi.wovi\n","2021-05-29 16:17:43 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized valid en-wovi 1482 examples\n","2021-05-29 16:17:44 | INFO | fairseq_cli.train | TransformerAlignModel(\n","  (encoder): TransformerEncoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoder(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(56792, 512, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=512, out_features=56792, bias=False)\n","  )\n",")\n","2021-05-29 16:17:44 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n","2021-05-29 16:17:44 | INFO | fairseq_cli.train | model: transformer_align (TransformerAlignModel)\n","2021-05-29 16:17:44 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_with_alignment (LabelSmoothedCrossEntropyCriterionWithAlignment)\n","2021-05-29 16:17:44 | INFO | fairseq_cli.train | num. model params: 73216000 (num. trained: 73216000)\n","2021-05-29 16:17:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n","2021-05-29 16:17:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n","2021-05-29 16:17:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 16:17:48 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2021-05-29 16:17:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2021-05-29 16:17:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2021-05-29 16:17:48 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n","2021-05-29 16:17:48 | INFO | fairseq.trainer | no existing checkpoint found en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint_last.pt\n","2021-05-29 16:17:48 | INFO | fairseq.trainer | loading train data for epoch 1\n","2021-05-29 16:17:48 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized/train.en-wovi.en\n","2021-05-29 16:17:48 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized/train.en-wovi.wovi\n","2021-05-29 16:17:48 | INFO | fairseq.tasks.translation | en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized train en-wovi 42026 examples\n","2021-05-29 16:17:48 | INFO | fairseq.data.data_utils | loaded 42026 examples from: en2wovi-align0lw-h8average-l4-alignment-lambda50p/binarized/train.align.en-wovi\n","epoch 001:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:17:48 | INFO | fairseq.trainer | begin training epoch 1\n","epoch 001:   1% 2/330 [00:00<01:30,  3.63it/s]2021-05-29 16:17:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n","epoch 001:  58% 193/330 [00:30<00:21,  6.34it/s]2021-05-29 16:18:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n","epoch 001: 100% 329/330 [00:51<00:00,  6.45it/s]2021-05-29 16:18:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.68it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.29it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.29it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.41it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.74it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:18:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.466 | nll_loss 11.981 | alignment_loss 0 | ppl 4042.18 | wps 47229.8 | wpb 2006.9 | bsz 98.8 | num_updates 328\n","2021-05-29 16:18:41 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:20:00 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint1.pt (epoch 1 @ 328 updates, score 12.466) (writing took 78.93429042299977 seconds)\n","2021-05-29 16:20:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2021-05-29 16:20:00 | INFO | train | epoch 001 | loss 14.232 | nll_loss 13.975 | alignment_loss 3.84 | ppl 16097.4 | wps 6853.4 | ups 2.49 | wpb 2754.1 | bsz 124.9 | num_updates 328 | lr 1.64e-05 | gnorm 2.637 | loss_scale 32 | train_wall 51 | wall 132\n","epoch 002:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:20:00 | INFO | fairseq.trainer | begin training epoch 2\n","epoch 002:  45% 147/330 [00:24<00:28,  6.32it/s]2021-05-29 16:20:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n","epoch 002:  45% 149/330 [00:25<00:28,  6.45it/s]2021-05-29 16:20:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n","epoch 002:  64% 210/330 [00:34<00:19,  6.23it/s]2021-05-29 16:20:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n","epoch 002:  72% 238/330 [00:39<00:14,  6.39it/s]2021-05-29 16:20:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0\n","epoch 002:  79% 260/330 [00:42<00:11,  6.12it/s]2021-05-29 16:20:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0\n","epoch 002:  80% 263/330 [00:43<00:10,  6.51it/s]2021-05-29 16:20:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5\n","epoch 002:  81% 266/330 [00:43<00:09,  6.82it/s]2021-05-29 16:20:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25\n","epoch 002:  87% 286/330 [00:46<00:07,  6.02it/s]2021-05-29 16:20:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125\n","epoch 002:  89% 293/330 [00:48<00:05,  6.18it/s]2021-05-29 16:20:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\n","epoch 002: 100% 329/330 [00:53<00:00,  6.18it/s]2021-05-29 16:20:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.93it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.89it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:20:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.637 | nll_loss 9.773 | alignment_loss 0 | ppl 874.86 | wps 44898.5 | wpb 2006.9 | bsz 98.8 | num_updates 649 | best_loss 10.637\n","2021-05-29 16:20:55 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:21:53 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint2.pt (epoch 2 @ 649 updates, score 10.637) (writing took 58.61452492800163 seconds)\n","2021-05-29 16:21:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2021-05-29 16:21:53 | INFO | train | epoch 002 | loss 11.304 | nll_loss 10.646 | alignment_loss 3.369 | ppl 1602.5 | wps 7797.8 | ups 2.83 | wpb 2756 | bsz 129.1 | num_updates 649 | lr 3.245e-05 | gnorm 2.525 | loss_scale 0.0625 | train_wall 53 | wall 245\n","epoch 003:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:21:53 | INFO | fairseq.trainer | begin training epoch 3\n","epoch 003:  13% 42/330 [00:07<00:47,  6.05it/s]2021-05-29 16:22:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\n","epoch 003:  15% 48/330 [00:08<00:46,  6.13it/s]2021-05-29 16:22:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625\n","epoch 003:  28% 93/330 [00:16<00:38,  6.12it/s]2021-05-29 16:22:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125\n","epoch 003: 100% 329/330 [00:55<00:00,  5.97it/s]2021-05-29 16:22:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.67it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.75it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:22:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.567 | nll_loss 9.643 | alignment_loss 0 | ppl 799.65 | wps 43983.6 | wpb 2006.9 | bsz 98.8 | num_updates 976 | best_loss 10.567\n","2021-05-29 16:22:50 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:23:51 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint3.pt (epoch 3 @ 976 updates, score 10.567) (writing took 61.403529134000564 seconds)\n","2021-05-29 16:23:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2021-05-29 16:23:51 | INFO | train | epoch 003 | loss 10.546 | nll_loss 9.688 | alignment_loss 2.983 | ppl 824.86 | wps 7634 | ups 2.78 | wpb 2749.8 | bsz 127.8 | num_updates 976 | lr 4.88e-05 | gnorm 2.626 | loss_scale 0.0078 | train_wall 55 | wall 363\n","epoch 004:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:23:52 | INFO | fairseq.trainer | begin training epoch 4\n","epoch 004: 100% 329/330 [00:57<00:00,  6.25it/s, loss=11.996, nll_loss=11.399, alignment_loss=3.386, ppl=2700.66, wps=7477.8, ups=2.72, wpb=2753.3, bsz=127.1, num_updates=1000, lr=5e-05, gnorm=2.597, loss_scale=0.0078, train_wall=163, wall=368]2021-05-29 16:24:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.56it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:24:50 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.239 | nll_loss 9.293 | alignment_loss 0 | ppl 627.24 | wps 44456.2 | wpb 2006.9 | bsz 98.8 | num_updates 1306 | best_loss 10.239\n","2021-05-29 16:24:50 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:25:50 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint4.pt (epoch 4 @ 1306 updates, score 10.239) (writing took 60.47541770599855 seconds)\n","2021-05-29 16:25:50 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2021-05-29 16:25:50 | INFO | train | epoch 004 | loss 10.327 | nll_loss 9.419 | alignment_loss 2.818 | ppl 684.36 | wps 7618.9 | ups 2.77 | wpb 2752.1 | bsz 127.4 | num_updates 1306 | lr 6.53e-05 | gnorm 2.376 | loss_scale 0.0078 | train_wall 56 | wall 482\n","epoch 005:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:25:50 | INFO | fairseq.trainer | begin training epoch 5\n","epoch 005: 100% 329/330 [00:55<00:00,  6.40it/s]2021-05-29 16:26:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.65it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.29it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.22it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.13it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.07it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.19it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:26:47 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.9 | nll_loss 8.883 | alignment_loss 0 | ppl 472.17 | wps 45127.8 | wpb 2006.9 | bsz 98.8 | num_updates 1636 | best_loss 9.9\n","2021-05-29 16:26:47 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:27:47 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint5.pt (epoch 5 @ 1636 updates, score 9.9) (writing took 60.00230684399867 seconds)\n","2021-05-29 16:27:47 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2021-05-29 16:27:47 | INFO | train | epoch 005 | loss 10.026 | nll_loss 9.065 | alignment_loss 2.711 | ppl 535.68 | wps 7773.3 | ups 2.82 | wpb 2752.1 | bsz 127.4 | num_updates 1636 | lr 8.18e-05 | gnorm 2.359 | loss_scale 0.0078 | train_wall 55 | wall 599\n","epoch 006:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:27:47 | INFO | fairseq.trainer | begin training epoch 6\n","epoch 006: 100% 329/330 [00:54<00:00,  6.20it/s]2021-05-29 16:28:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 006 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.22it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.83it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.74it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.67it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.65it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.76it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:28:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.459 | nll_loss 8.358 | alignment_loss 0 | ppl 328.21 | wps 44934.8 | wpb 2006.9 | bsz 98.8 | num_updates 1966 | best_loss 9.459\n","2021-05-29 16:28:43 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:29:43 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint6.pt (epoch 6 @ 1966 updates, score 9.459) (writing took 59.968924393000634 seconds)\n","2021-05-29 16:29:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2021-05-29 16:29:43 | INFO | train | epoch 006 | loss 9.627 | nll_loss 8.612 | alignment_loss 2.598 | ppl 391.2 | wps 7848.6 | ups 2.85 | wpb 2752.1 | bsz 127.4 | num_updates 1966 | lr 9.83e-05 | gnorm 2.35 | loss_scale 0.0078 | train_wall 54 | wall 715\n","epoch 007:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:29:43 | INFO | fairseq.trainer | begin training epoch 7\n","epoch 007: 100% 329/330 [00:54<00:00,  6.32it/s, loss=9.962, nll_loss=8.995, alignment_loss=2.697, ppl=510.26, wps=7789.5, ups=2.83, wpb=2751, bsz=127.3, num_updates=2000, lr=0.0001, gnorm=2.351, loss_scale=0.0078, train_wall=166, wall=722]2021-05-29 16:30:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 007 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.55it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  9.34it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.07it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.04it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.02it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.13it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:30:39 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9 | nll_loss 7.868 | alignment_loss 0 | ppl 233.64 | wps 45406.6 | wpb 2006.9 | bsz 98.8 | num_updates 2296 | best_loss 9\n","2021-05-29 16:30:39 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:31:37 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint7.pt (epoch 7 @ 2296 updates, score 9.0) (writing took 58.68529239500094 seconds)\n","2021-05-29 16:31:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2021-05-29 16:31:37 | INFO | train | epoch 007 | loss 9.205 | nll_loss 8.13 | alignment_loss 2.473 | ppl 280.1 | wps 7921.6 | ups 2.88 | wpb 2752.1 | bsz 127.4 | num_updates 2296 | lr 0.0001148 | gnorm 2.224 | loss_scale 0.0078 | train_wall 53 | wall 830\n","epoch 008:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:31:37 | INFO | fairseq.trainer | begin training epoch 8\n","epoch 008: 100% 329/330 [00:54<00:00,  6.69it/s]2021-05-29 16:32:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 008 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.48it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.86it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.68it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.63it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 12.65it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 14.95it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:32:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.756 | nll_loss 7.575 | alignment_loss 0 | ppl 190.67 | wps 44931.4 | wpb 2006.9 | bsz 98.8 | num_updates 2626 | best_loss 8.756\n","2021-05-29 16:32:32 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:33:32 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint8.pt (epoch 8 @ 2626 updates, score 8.756) (writing took 59.20606870300071 seconds)\n","2021-05-29 16:33:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2021-05-29 16:33:32 | INFO | train | epoch 008 | loss 8.834 | nll_loss 7.702 | alignment_loss 2.367 | ppl 208.23 | wps 7945.5 | ups 2.89 | wpb 2752.1 | bsz 127.4 | num_updates 2626 | lr 0.0001313 | gnorm 2.224 | loss_scale 0.0078 | train_wall 53 | wall 944\n","epoch 009:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:33:32 | INFO | fairseq.trainer | begin training epoch 9\n","epoch 009: 100% 329/330 [00:54<00:00,  6.22it/s]2021-05-29 16:34:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 009 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.63it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.23it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.15it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.12it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.11it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.33it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:34:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.377 | nll_loss 7.139 | alignment_loss 0 | ppl 140.91 | wps 45612.4 | wpb 2006.9 | bsz 98.8 | num_updates 2956 | best_loss 8.377\n","2021-05-29 16:34:27 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:35:22 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint9.pt (epoch 9 @ 2956 updates, score 8.377) (writing took 55.1189616190004 seconds)\n","2021-05-29 16:35:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2021-05-29 16:35:22 | INFO | train | epoch 009 | loss 8.488 | nll_loss 7.301 | alignment_loss 2.268 | ppl 157.72 | wps 8231.4 | ups 2.99 | wpb 2752.1 | bsz 127.4 | num_updates 2956 | lr 0.0001478 | gnorm 2.249 | loss_scale 0.0078 | train_wall 53 | wall 1054\n","epoch 010:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:35:22 | INFO | fairseq.trainer | begin training epoch 10\n","epoch 010: 100% 329/330 [00:54<00:00,  6.05it/s, loss=8.801, nll_loss=7.664, alignment_loss=2.361, ppl=202.77, wps=8084.8, ups=2.94, wpb=2752.1, bsz=127, num_updates=3000, lr=0.00015, gnorm=2.232, loss_scale=0.0078, train_wall=161, wall=1062]2021-05-29 16:36:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.37it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.97it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.93it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.06it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:36:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.055 | nll_loss 6.737 | alignment_loss 0 | ppl 106.68 | wps 45857.5 | wpb 2006.9 | bsz 98.8 | num_updates 3286 | best_loss 8.055\n","2021-05-29 16:36:18 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:37:18 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint10.pt (epoch 10 @ 3286 updates, score 8.055) (writing took 60.09050243999991 seconds)\n","2021-05-29 16:37:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2021-05-29 16:37:18 | INFO | train | epoch 010 | loss 8.151 | nll_loss 6.909 | alignment_loss 2.163 | ppl 120.2 | wps 7840 | ups 2.85 | wpb 2752.1 | bsz 127.4 | num_updates 3286 | lr 0.0001643 | gnorm 2.148 | loss_scale 0.0078 | train_wall 54 | wall 1170\n","epoch 011:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:37:18 | INFO | fairseq.trainer | begin training epoch 11\n","epoch 011: 100% 329/330 [00:54<00:00,  6.11it/s]2021-05-29 16:38:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 011 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.20it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.77it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  40% 6/15 [00:00<00:00,  9.64it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 11.52it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 13.45it/s]\u001b[A\n","epoch 011 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 15.51it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:38:14 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.823 | nll_loss 6.462 | alignment_loss 0 | ppl 88.18 | wps 44131.1 | wpb 2006.9 | bsz 98.8 | num_updates 3616 | best_loss 7.823\n","2021-05-29 16:38:14 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:39:11 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint11.pt (epoch 11 @ 3616 updates, score 7.823) (writing took 57.534623215000465 seconds)\n","2021-05-29 16:39:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n","2021-05-29 16:39:11 | INFO | train | epoch 011 | loss 7.801 | nll_loss 6.503 | alignment_loss 2.047 | ppl 90.68 | wps 7998 | ups 2.91 | wpb 2752.1 | bsz 127.4 | num_updates 3616 | lr 0.0001808 | gnorm 2.088 | loss_scale 0.0078 | train_wall 54 | wall 1284\n","epoch 012:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:39:12 | INFO | fairseq.trainer | begin training epoch 12\n","epoch 012: 100% 329/330 [00:56<00:00,  6.40it/s]2021-05-29 16:40:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 012 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.70it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.30it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.26it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.17it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.06it/s]\u001b[A\n","epoch 012 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.11it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:40:10 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.581 | nll_loss 6.132 | alignment_loss 0 | ppl 70.12 | wps 44719.5 | wpb 2006.9 | bsz 98.8 | num_updates 3946 | best_loss 7.581\n","2021-05-29 16:40:10 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:41:11 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint12.pt (epoch 12 @ 3946 updates, score 7.581) (writing took 61.39916630500011 seconds)\n","2021-05-29 16:41:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n","2021-05-29 16:41:11 | INFO | train | epoch 012 | loss 7.459 | nll_loss 6.104 | alignment_loss 1.934 | ppl 68.78 | wps 7592.5 | ups 2.76 | wpb 2752.1 | bsz 127.4 | num_updates 3946 | lr 0.0001973 | gnorm 2.023 | loss_scale 0.0078 | train_wall 55 | wall 1403\n","epoch 013:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:41:11 | INFO | fairseq.trainer | begin training epoch 13\n","epoch 013: 100% 329/330 [00:55<00:00,  6.08it/s, loss=7.743, nll_loss=6.435, alignment_loss=2.025, ppl=86.54, wps=7856, ups=2.85, wpb=2755.5, bsz=128.4, num_updates=4000, lr=0.0002, gnorm=2.07, loss_scale=0.0078, train_wall=165, wall=1413]2021-05-29 16:42:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 013 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.94it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.45it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.39it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.28it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.41it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:42:08 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.453 | nll_loss 5.982 | alignment_loss 0 | ppl 63.21 | wps 44648.7 | wpb 2006.9 | bsz 98.8 | num_updates 4276 | best_loss 7.453\n","2021-05-29 16:42:08 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:43:08 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint13.pt (epoch 13 @ 4276 updates, score 7.453) (writing took 59.539782855999874 seconds)\n","2021-05-29 16:43:08 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2021-05-29 16:43:08 | INFO | train | epoch 013 | loss 7.108 | nll_loss 5.693 | alignment_loss 1.821 | ppl 51.74 | wps 7792.9 | ups 2.83 | wpb 2752.1 | bsz 127.4 | num_updates 4276 | lr 0.000193438 | gnorm 1.961 | loss_scale 0.0078 | train_wall 55 | wall 1520\n","epoch 014:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:43:08 | INFO | fairseq.trainer | begin training epoch 14\n","epoch 014: 100% 329/330 [00:56<00:00,  5.95it/s]2021-05-29 16:44:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 014 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.19it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.88it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.82it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.72it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.61it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.69it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:44:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.198 | nll_loss 5.698 | alignment_loss 0 | ppl 51.92 | wps 45048 | wpb 2006.9 | bsz 98.8 | num_updates 4606 | best_loss 7.198\n","2021-05-29 16:44:05 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:45:05 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint14.pt (epoch 14 @ 4606 updates, score 7.198) (writing took 59.45999018300063 seconds)\n","2021-05-29 16:45:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2021-05-29 16:45:05 | INFO | train | epoch 014 | loss 6.793 | nll_loss 5.324 | alignment_loss 1.718 | ppl 40.07 | wps 7747.2 | ups 2.81 | wpb 2752.1 | bsz 127.4 | num_updates 4606 | lr 0.000186379 | gnorm 1.931 | loss_scale 0.0078 | train_wall 55 | wall 1637\n","epoch 015:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:45:05 | INFO | fairseq.trainer | begin training epoch 15\n","epoch 015: 100% 329/330 [00:56<00:00,  6.06it/s]2021-05-29 16:46:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 015 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.86it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.40it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.22it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.35it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:46:02 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.978 | nll_loss 5.403 | alignment_loss 0 | ppl 42.31 | wps 44734.1 | wpb 2006.9 | bsz 98.8 | num_updates 4936 | best_loss 6.978\n","2021-05-29 16:46:02 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:47:03 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint15.pt (epoch 15 @ 4936 updates, score 6.978) (writing took 60.6749179239996 seconds)\n","2021-05-29 16:47:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2021-05-29 16:47:03 | INFO | train | epoch 015 | loss 6.501 | nll_loss 4.985 | alignment_loss 1.621 | ppl 31.68 | wps 7706.6 | ups 2.8 | wpb 2752.1 | bsz 127.4 | num_updates 4936 | lr 0.000180041 | gnorm 1.857 | loss_scale 0.0078 | train_wall 55 | wall 1755\n","epoch 016:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:47:03 | INFO | fairseq.trainer | begin training epoch 16\n","epoch 016: 100% 329/330 [00:56<00:00,  6.12it/s, loss=6.752, nll_loss=5.278, alignment_loss=1.707, ppl=38.79, wps=7761.8, ups=2.82, wpb=2748.7, bsz=125.7, num_updates=5000, lr=0.000178885, gnorm=1.912, loss_scale=0.0078, train_wall=167, wall=1767]2021-05-29 16:48:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 016 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.21it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.80it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.73it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.65it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.57it/s]\u001b[A\n","epoch 016 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.67it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:48:01 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.843 | nll_loss 5.226 | alignment_loss 0 | ppl 37.42 | wps 44641.6 | wpb 2006.9 | bsz 98.8 | num_updates 5266 | best_loss 6.843\n","2021-05-29 16:48:01 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:49:01 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint16.pt (epoch 16 @ 5266 updates, score 6.843) (writing took 59.73049732099935 seconds)\n","2021-05-29 16:49:01 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2021-05-29 16:49:01 | INFO | train | epoch 016 | loss 6.254 | nll_loss 4.698 | alignment_loss 1.549 | ppl 25.96 | wps 7697.5 | ups 2.8 | wpb 2752.1 | bsz 127.4 | num_updates 5266 | lr 0.000174309 | gnorm 1.857 | loss_scale 0.0078 | train_wall 55 | wall 1873\n","epoch 017:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:49:01 | INFO | fairseq.trainer | begin training epoch 17\n","epoch 017: 100% 329/330 [00:56<00:00,  6.06it/s]2021-05-29 16:49:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 017 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.96it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.56it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.56it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.43it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.23it/s]\u001b[A\n","epoch 017 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.27it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:49:58 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.748 | nll_loss 5.122 | alignment_loss 0 | ppl 34.81 | wps 44622.6 | wpb 2006.9 | bsz 98.8 | num_updates 5596 | best_loss 6.748\n","2021-05-29 16:49:58 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:50:57 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint17.pt (epoch 17 @ 5596 updates, score 6.748) (writing took 59.06468703399878 seconds)\n","2021-05-29 16:50:57 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2021-05-29 16:50:57 | INFO | train | epoch 017 | loss 6.034 | nll_loss 4.446 | alignment_loss 1.49 | ppl 21.79 | wps 7814.6 | ups 2.84 | wpb 2752.1 | bsz 127.4 | num_updates 5596 | lr 0.000169091 | gnorm 1.79 | loss_scale 0.0078 | train_wall 55 | wall 1989\n","epoch 018:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:50:57 | INFO | fairseq.trainer | begin training epoch 18\n","epoch 018: 100% 329/330 [00:55<00:00,  5.86it/s]2021-05-29 16:51:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 018 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.68it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.35it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.34it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.21it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.06it/s]\u001b[A\n","epoch 018 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.12it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:51:54 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.659 | nll_loss 4.995 | alignment_loss 0 | ppl 31.9 | wps 45002 | wpb 2006.9 | bsz 98.8 | num_updates 5926 | best_loss 6.659\n","2021-05-29 16:51:54 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:52:53 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint18.pt (epoch 18 @ 5926 updates, score 6.659) (writing took 58.751462214999265 seconds)\n","2021-05-29 16:52:53 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2021-05-29 16:52:53 | INFO | train | epoch 018 | loss 5.837 | nll_loss 4.219 | alignment_loss 1.434 | ppl 18.63 | wps 7813 | ups 2.84 | wpb 2752.1 | bsz 127.4 | num_updates 5926 | lr 0.000164316 | gnorm 1.756 | loss_scale 0.0078 | train_wall 55 | wall 2105\n","epoch 019:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:52:53 | INFO | fairseq.trainer | begin training epoch 19\n","epoch 019: 100% 329/330 [00:56<00:00,  6.19it/s, loss=5.998, nll_loss=4.405, alignment_loss=1.478, ppl=21.18, wps=7837.9, ups=2.85, wpb=2753.5, bsz=128.2, num_updates=6000, lr=0.000163299, gnorm=1.798, loss_scale=0.0078, train_wall=167, wall=2118]2021-05-29 16:53:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.47it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.17it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.98it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.87it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.96it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:53:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.66 | nll_loss 5.016 | alignment_loss 0 | ppl 32.36 | wps 45001.4 | wpb 2006.9 | bsz 98.8 | num_updates 6256 | best_loss 6.659\n","2021-05-29 16:53:50 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:54:25 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint19.pt (epoch 19 @ 6256 updates, score 6.66) (writing took 34.70358696000039 seconds)\n","2021-05-29 16:54:25 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2021-05-29 16:54:25 | INFO | train | epoch 019 | loss 5.669 | nll_loss 4.029 | alignment_loss 1.388 | ppl 16.32 | wps 9900.4 | ups 3.6 | wpb 2752.1 | bsz 127.4 | num_updates 6256 | lr 0.000159923 | gnorm 1.777 | loss_scale 0.0078 | train_wall 55 | wall 2197\n","epoch 020:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:54:25 | INFO | fairseq.trainer | begin training epoch 20\n","epoch 020: 100% 329/330 [00:55<00:00,  6.18it/s]2021-05-29 16:55:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 020 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.83it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.48it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.41it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.26it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.13it/s]\u001b[A\n","epoch 020 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.16it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:55:22 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.547 | nll_loss 4.876 | alignment_loss 0 | ppl 29.37 | wps 44487.7 | wpb 2006.9 | bsz 98.8 | num_updates 6586 | best_loss 6.547\n","2021-05-29 16:55:22 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:56:16 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint20.pt (epoch 20 @ 6586 updates, score 6.547) (writing took 54.356255407999925 seconds)\n","2021-05-29 16:56:16 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2021-05-29 16:56:16 | INFO | train | epoch 020 | loss 5.515 | nll_loss 3.854 | alignment_loss 1.348 | ppl 14.46 | wps 8166.1 | ups 2.97 | wpb 2752.1 | bsz 127.4 | num_updates 6586 | lr 0.000155865 | gnorm 1.747 | loss_scale 0.0078 | train_wall 55 | wall 2308\n","epoch 021:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:56:16 | INFO | fairseq.trainer | begin training epoch 21\n","epoch 021: 100% 329/330 [00:55<00:00,  6.21it/s]2021-05-29 16:57:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 021 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  8.06it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  9.64it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 11.59it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 13.42it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 15.27it/s]\u001b[A\n","epoch 021 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 17.30it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:57:13 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.52 | nll_loss 4.845 | alignment_loss 0 | ppl 28.73 | wps 44153.1 | wpb 2006.9 | bsz 98.8 | num_updates 6916 | best_loss 6.52\n","2021-05-29 16:57:13 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 16:58:06 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint21.pt (epoch 21 @ 6916 updates, score 6.52) (writing took 53.2421255270001 seconds)\n","2021-05-29 16:58:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2021-05-29 16:58:06 | INFO | train | epoch 021 | loss 5.383 | nll_loss 3.705 | alignment_loss 1.316 | ppl 13.04 | wps 8246.2 | ups 3 | wpb 2752.1 | bsz 127.4 | num_updates 6916 | lr 0.000152101 | gnorm 1.792 | loss_scale 0.0078 | train_wall 55 | wall 2418\n","epoch 022:   0% 0/330 [00:00<?, ?it/s]2021-05-29 16:58:07 | INFO | fairseq.trainer | begin training epoch 22\n","epoch 022: 100% 329/330 [00:56<00:00,  6.12it/s, loss=5.485, nll_loss=3.821, alignment_loss=1.342, ppl=14.13, wps=8713.6, ups=3.17, wpb=2748.1, bsz=127.8, num_updates=7000, lr=0.000151186, gnorm=1.768, loss_scale=0.0078, train_wall=167, wall=2434]2021-05-29 16:59:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 022 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.97it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.62it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.51it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.41it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.35it/s]\u001b[A\n","epoch 022 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.45it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 16:59:04 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.459 | nll_loss 4.776 | alignment_loss 0 | ppl 27.39 | wps 44726.4 | wpb 2006.9 | bsz 98.8 | num_updates 7246 | best_loss 6.459\n","2021-05-29 16:59:04 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:00:01 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint22.pt (epoch 22 @ 7246 updates, score 6.459) (writing took 57.373447400999794 seconds)\n","2021-05-29 17:00:01 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2021-05-29 17:00:01 | INFO | train | epoch 022 | loss 5.252 | nll_loss 3.558 | alignment_loss 1.28 | ppl 11.78 | wps 7876.2 | ups 2.86 | wpb 2752.1 | bsz 127.4 | num_updates 7246 | lr 0.000148597 | gnorm 1.756 | loss_scale 0.0078 | train_wall 55 | wall 2534\n","epoch 023:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:00:02 | INFO | fairseq.trainer | begin training epoch 23\n","epoch 023: 100% 329/330 [00:56<00:00,  5.92it/s]2021-05-29 17:00:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 023 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.47it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.04it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  40% 6/15 [00:00<00:00,  9.93it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 11.86it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 13.79it/s]\u001b[A\n","epoch 023 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 15.94it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:00:59 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.419 | nll_loss 4.733 | alignment_loss 0 | ppl 26.59 | wps 44622 | wpb 2006.9 | bsz 98.8 | num_updates 7576 | best_loss 6.419\n","2021-05-29 17:00:59 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:01:59 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint23.pt (epoch 23 @ 7576 updates, score 6.419) (writing took 60.68935971099927 seconds)\n","2021-05-29 17:01:59 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2021-05-29 17:01:59 | INFO | train | epoch 023 | loss 5.142 | nll_loss 3.435 | alignment_loss 1.253 | ppl 10.81 | wps 7705.9 | ups 2.8 | wpb 2752.1 | bsz 127.4 | num_updates 7576 | lr 0.000145325 | gnorm 1.778 | loss_scale 0.0078 | train_wall 55 | wall 2652\n","epoch 024:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:02:00 | INFO | fairseq.trainer | begin training epoch 24\n","epoch 024: 100% 329/330 [00:56<00:00,  6.16it/s]2021-05-29 17:02:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 024 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.91it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.29it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.17it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.07it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.00it/s]\u001b[A\n","epoch 024 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.09it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:02:57 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.446 | nll_loss 4.759 | alignment_loss 0 | ppl 27.07 | wps 42801.9 | wpb 2006.9 | bsz 98.8 | num_updates 7906 | best_loss 6.419\n","2021-05-29 17:02:57 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:03:34 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint24.pt (epoch 24 @ 7906 updates, score 6.446) (writing took 36.08347035600127 seconds)\n","2021-05-29 17:03:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n","2021-05-29 17:03:34 | INFO | train | epoch 024 | loss 5.035 | nll_loss 3.314 | alignment_loss 1.227 | ppl 9.95 | wps 9642.2 | ups 3.5 | wpb 2752.1 | bsz 127.4 | num_updates 7906 | lr 0.00014226 | gnorm 1.788 | loss_scale 0.0078 | train_wall 55 | wall 2746\n","epoch 025:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:03:34 | INFO | fairseq.trainer | begin training epoch 25\n","epoch 025: 100% 329/330 [00:56<00:00,  5.99it/s, loss=5.113, nll_loss=3.402, alignment_loss=1.243, ppl=10.57, wps=8408.6, ups=3.04, wpb=2763.9, bsz=127.6, num_updates=8000, lr=0.000141421, gnorm=1.776, loss_scale=0.0078, train_wall=167, wall=2762]2021-05-29 17:04:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 025 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.83it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.78it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.69it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.58it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.68it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:04:31 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.397 | nll_loss 4.701 | alignment_loss 0 | ppl 26.01 | wps 44419.1 | wpb 2006.9 | bsz 98.8 | num_updates 8236 | best_loss 6.397\n","2021-05-29 17:04:31 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:05:30 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint25.pt (epoch 25 @ 8236 updates, score 6.397) (writing took 58.71139043000039 seconds)\n","2021-05-29 17:05:30 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n","2021-05-29 17:05:30 | INFO | train | epoch 025 | loss 4.936 | nll_loss 3.202 | alignment_loss 1.201 | ppl 9.21 | wps 7811.2 | ups 2.84 | wpb 2752.1 | bsz 127.4 | num_updates 8236 | lr 0.00013938 | gnorm 1.788 | loss_scale 0.0078 | train_wall 55 | wall 2862\n","epoch 026:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:05:30 | INFO | fairseq.trainer | begin training epoch 26\n","epoch 026: 100% 329/330 [00:56<00:00,  6.09it/s]2021-05-29 17:06:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 026 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.78it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.42it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.37it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.31it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.21it/s]\u001b[A\n","epoch 026 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.35it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:06:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.386 | nll_loss 4.7 | alignment_loss 0 | ppl 25.99 | wps 45212.7 | wpb 2006.9 | bsz 98.8 | num_updates 8566 | best_loss 6.386\n","2021-05-29 17:06:28 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:07:22 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint26.pt (epoch 26 @ 8566 updates, score 6.386) (writing took 54.23843172300076 seconds)\n","2021-05-29 17:07:22 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n","2021-05-29 17:07:22 | INFO | train | epoch 026 | loss 4.85 | nll_loss 3.105 | alignment_loss 1.182 | ppl 8.61 | wps 8099.2 | ups 2.94 | wpb 2752.1 | bsz 127.4 | num_updates 8566 | lr 0.000136669 | gnorm 1.811 | loss_scale 0.0078 | train_wall 55 | wall 2974\n","epoch 027:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:07:22 | INFO | fairseq.trainer | begin training epoch 27\n","epoch 027: 100% 329/330 [00:55<00:00,  6.08it/s]2021-05-29 17:08:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 027 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.95it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.60it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.51it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.39it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.31it/s]\u001b[A\n","epoch 027 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.41it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:08:19 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.404 | nll_loss 4.708 | alignment_loss 0 | ppl 26.13 | wps 44647.6 | wpb 2006.9 | bsz 98.8 | num_updates 8896 | best_loss 6.386\n","2021-05-29 17:08:19 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:08:57 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint27.pt (epoch 27 @ 8896 updates, score 6.404) (writing took 37.907664447999196 seconds)\n","2021-05-29 17:08:57 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n","2021-05-29 17:08:57 | INFO | train | epoch 027 | loss 4.78 | nll_loss 3.023 | alignment_loss 1.166 | ppl 8.13 | wps 9586.3 | ups 3.48 | wpb 2752.1 | bsz 127.4 | num_updates 8896 | lr 0.00013411 | gnorm 1.87 | loss_scale 0.0078 | train_wall 55 | wall 3069\n","epoch 028:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:08:57 | INFO | fairseq.trainer | begin training epoch 28\n","epoch 028: 100% 329/330 [00:55<00:00,  6.00it/s, loss=4.831, nll_loss=3.083, alignment_loss=1.179, ppl=8.48, wps=8467.3, ups=3.08, wpb=2747.6, bsz=127.7, num_updates=9000, lr=0.000133333, gnorm=1.833, loss_scale=0.0078, train_wall=167, wall=3087]2021-05-29 17:09:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 028 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.98it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.66it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.54it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.43it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.42it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:09:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.403 | nll_loss 4.692 | alignment_loss 0 | ppl 25.84 | wps 44518.5 | wpb 2006.9 | bsz 98.8 | num_updates 9226 | best_loss 6.386\n","2021-05-29 17:09:53 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:10:26 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint28.pt (epoch 28 @ 9226 updates, score 6.403) (writing took 32.72310464600014 seconds)\n","2021-05-29 17:10:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n","2021-05-29 17:10:26 | INFO | train | epoch 028 | loss 4.694 | nll_loss 2.927 | alignment_loss 1.144 | ppl 7.61 | wps 10161.1 | ups 3.69 | wpb 2752.1 | bsz 127.4 | num_updates 9226 | lr 0.00013169 | gnorm 1.845 | loss_scale 0.0078 | train_wall 55 | wall 3158\n","epoch 029:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:10:26 | INFO | fairseq.trainer | begin training epoch 29\n","epoch 029: 100% 329/330 [00:56<00:00,  6.30it/s]2021-05-29 17:11:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 029 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.41it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.95it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  40% 6/15 [00:00<00:00,  9.80it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 11.70it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 13.67it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 15.82it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:11:23 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.372 | nll_loss 4.659 | alignment_loss 0 | ppl 25.27 | wps 44131.8 | wpb 2006.9 | bsz 98.8 | num_updates 9556 | best_loss 6.372\n","2021-05-29 17:11:23 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:12:23 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint29.pt (epoch 29 @ 9556 updates, score 6.372) (writing took 59.591659790999984 seconds)\n","2021-05-29 17:12:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n","2021-05-29 17:12:23 | INFO | train | epoch 029 | loss 4.619 | nll_loss 2.841 | alignment_loss 1.124 | ppl 7.17 | wps 7764.4 | ups 2.82 | wpb 2752.1 | bsz 127.4 | num_updates 9556 | lr 0.000129396 | gnorm 1.834 | loss_scale 0.0078 | train_wall 55 | wall 3275\n","epoch 030:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:12:23 | INFO | fairseq.trainer | begin training epoch 30\n","epoch 030: 100% 329/330 [00:56<00:00,  6.00it/s]2021-05-29 17:13:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 030 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.88it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.41it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  40% 6/15 [00:00<00:00, 10.25it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 12.18it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset:  80% 12/15 [00:00<00:00, 14.10it/s]\u001b[A\n","epoch 030 | valid on 'valid' subset: 100% 15/15 [00:00<00:00, 16.15it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:13:21 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.355 | nll_loss 4.653 | alignment_loss 0 | ppl 25.16 | wps 43520.5 | wpb 2006.9 | bsz 98.8 | num_updates 9886 | best_loss 6.355\n","2021-05-29 17:13:21 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:14:20 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint30.pt (epoch 30 @ 9886 updates, score 6.355) (writing took 59.58437250699899 seconds)\n","2021-05-29 17:14:20 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n","2021-05-29 17:14:20 | INFO | train | epoch 030 | loss 4.556 | nll_loss 2.77 | alignment_loss 1.108 | ppl 6.82 | wps 7741.6 | ups 2.81 | wpb 2752.1 | bsz 127.4 | num_updates 9886 | lr 0.000127218 | gnorm 1.875 | loss_scale 0.0078 | train_wall 56 | wall 3393\n","epoch 031:   0% 0/330 [00:00<?, ?it/s]2021-05-29 17:14:20 | INFO | fairseq.trainer | begin training epoch 31\n","epoch 031:  34% 113/330 [00:19<00:36,  5.88it/s]2021-05-29 17:14:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 031 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.69it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.15it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.87it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.70it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 12.43it/s]\u001b[A\n","epoch 031 | valid on 'valid' subset:  93% 14/15 [00:00<00:00, 14.16it/s]\u001b[A\n","                                                                        \u001b[A2021-05-29 17:14:41 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.361 | nll_loss 4.655 | alignment_loss 0 | ppl 25.19 | wps 42203.6 | wpb 2006.9 | bsz 98.8 | num_updates 10000 | best_loss 6.355\n","2021-05-29 17:14:41 | INFO | fairseq_cli.train | begin save checkpoint\n","2021-05-29 17:14:49 | INFO | fairseq.checkpoint_utils | saved checkpoint en2wovi-align0lw-h8average-l4-alignment-lambda50p/checkpoints/checkpoint_last.pt (epoch 31 @ 10000 updates, score 6.361) (writing took 8.063275118000092 seconds)\n","2021-05-29 17:14:49 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n","2021-05-29 17:14:49 | INFO | train | epoch 031 | loss 4.453 | nll_loss 2.654 | alignment_loss 1.087 | ppl 6.3 | wps 11029.1 | ups 3.96 | wpb 2788.6 | bsz 130.7 | num_updates 10000 | lr 0.000126491 | gnorm 1.875 | loss_scale 0.0078 | train_wall 19 | wall 3421\n","2021-05-29 17:14:49 | INFO | fairseq_cli.train | done training in 3421.2 seconds\n","\n","real\t57m10.252s\n","user\t30m35.592s\n","sys\t3m35.535s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0p8Qq-CNjXX6"},"source":["##Train user-defined model"]},{"cell_type":"code","metadata":{"id":"6hGtq6LCE4td"},"source":["!time fairseq-train --user-dir \"$MODEL\"\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align_average_l4 --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.5\\\n","    --fp16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2duavufyKY_c","executionInfo":{"status":"ok","timestamp":1622057524890,"user_tz":-420,"elapsed":434,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"6376d009-40d8-4ad6-fd55-356bba812850"},"source":["MODEL"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'vi2en-align0vl-hmedian-l4-alignment-lambda50p'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"nT1eAlvKKWKw"},"source":["!time fairseq-train --user-dir \"$MODEL\"\\\n","    \"$MODEL\"/binarized \\\n","    --arch transformer_align_median_l4 --share-all-embeddings \\\n","    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n","    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n","    --dropout 0.3 \\\n","    --max-tokens 3200 --label-smoothing 0.1 \\\n","    --save-dir \"$MODEL\"/checkpoints --log-interval 1000 --max-update 10000 \\\n","    --keep-interval-updates -1 --save-interval-updates 0 \\\n","    --load-alignments --criterion label_smoothed_cross_entropy_with_alignment --alignment-lambda 0.5\\\n","    --fp16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ZOsudvaULu62","executionInfo":{"status":"ok","timestamp":1623432875415,"user_tz":-420,"elapsed":369,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"ee974108-0ec2-4c66-e248-07b1adbe056a"},"source":["MODEL"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'en2wovi-align0lw-h1-l4-alignment-lambda5p_2'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xa6BzqBk9HVj","executionInfo":{"status":"ok","timestamp":1623432940638,"user_tz":-420,"elapsed":58302,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"ff1290a0-ab14-4d45-815f-a353eae48650"},"source":["!time fairseq-generate \\\n","    \"$MODEL\"/binarized --gen-subset test \\\n","    --source-lang \"$SOURCE\" --target-lang \"$TARGET\" \\\n","    --path \"$MODEL\"/checkpoints/checkpoint_best.pt --beam 5 --nbest 1 \\\n","    --fp16 > \"$MODEL\"/result.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","real\t0m58.056s\n","user\t0m17.729s\n","sys\t0m3.407s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQ7SFjjV9XW4","executionInfo":{"status":"ok","timestamp":1622054215514,"user_tz":-420,"elapsed":28728,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"cc31cc02-a31a-40e5-8c18-325d48bf004e"},"source":["!time fairseq-generate --user-dir \"$MODEL\"\\\n","    \"$MODEL\"/binarized --gen-subset test \\\n","    --source-lang \"$SOURCE\" --target-lang \"$TARGET\" \\\n","    --path \"$MODEL\"/checkpoints/checkpoint_best.pt --beam 5 --nbest 1 \\\n","    --fp16 > \"$MODEL\"/result.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","real\t0m28.306s\n","user\t0m17.381s\n","sys\t0m3.186s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pvc1atfU9wgJ"},"source":["def extract(result, predict):\n","    result = open(result, 'r').read().splitlines()\n","    predict = open(predict, 'w')\n","    select = result[9:-5:5]\n","    select = [s.split('\\t') for s in select]\n","    select = [(int(s[0].split('-')[-1]), s[2]) for s in select]\n","    select = sorted(select)\n","    for s in select:\n","        predict.write(s[-1] + '\\n')\n","    predict.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rCKJQk7d4bq"},"source":["extract(os.path.join(MODEL, 'result.txt'), os.path.join(MODEL, 'predict.wovi'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvj-RZWLd2wD"},"source":["!cat $MODEL/predict.wovi | sed 's/_/ /g' > $MODEL/predict.vi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vj-JKLHJIJiZ","executionInfo":{"status":"ok","timestamp":1622308597897,"user_tz":-420,"elapsed":898,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"270de62d-4a19-4111-9b44-b652eb92ac69"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.vi < \"$MODEL\"/predict.vi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["en2wovi-align0lw-h8average-l4-alignment-lambda50p\n","BLEU = 25.71, 55.8/32.3/19.6/12.4 (BP=1.000, ratio=1.010, hyp_len=43687, ref_len=43245)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKVpcwC853kI","executionInfo":{"status":"ok","timestamp":1622304854937,"user_tz":-420,"elapsed":895,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"641999d9-ff83-4260-dc50-42545504250f"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.vi < \"$MODEL\"/predict.vi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["en2wovi-align0lw-h8average-l4-alignment-lambda5p\n","BLEU = 18.94, 50.8/26.5/14.6/8.3 (BP=0.943, ratio=0.945, hyp_len=40845, ref_len=43245)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j2AalEa7v7Vf","executionInfo":{"status":"ok","timestamp":1622302249651,"user_tz":-420,"elapsed":1075,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"443917d5-ce10-44ec-967f-5c35a3cf6711"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.vi < \"$MODEL\"/predict.vi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["en2wovi-align0lw-h1-l4-alignment-lambda50p\n","BLEU = 23.86, 55.3/31.1/18.4/11.4 (BP=0.974, ratio=0.974, hyp_len=42119, ref_len=43245)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaA3AgUgeOMu","executionInfo":{"status":"ok","timestamp":1622297622604,"user_tz":-420,"elapsed":1357,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"0e5a0061-0043-4082-cb4c-b5ad8557d607"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.vi < \"$MODEL\"/predict.vi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["en2wovi-align0lw-h1-l4-alignment-lambda5p\n","BLEU = 21.34, 55.2/30.3/17.5/10.6 (BP=0.904, ratio=0.909, hyp_len=39297, ref_len=43245)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O7eI-aeVJoDu"},"source":["##Repeat experiment for the baseline model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v667_CPeJiKo","executionInfo":{"status":"ok","timestamp":1623433035791,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"bd3ff808-9e02-43d0-fe62-0b143918045b"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.vi < \"$MODEL\"/predict.vi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["en2wovi-align0lw-h1-l4-alignment-lambda5p_2\n","BLEU = 21.15, 55.2/30.2/17.3/10.3 (BP=0.905, ratio=0.909, hyp_len=39320, ref_len=43245)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vRGe-wuw93zU"},"source":["extract(os.path.join(MODEL, 'result.txt'), os.path.join(MODEL, 'predict.en'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVs0pldwGrS1","executionInfo":{"status":"ok","timestamp":1622224325425,"user_tz":-420,"elapsed":789,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"3f3123eb-8dfa-47fa-b450-c463602bccaf"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en < \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["wovi2en-align0wl-h8average-l4-alignment-lambda50p\n","BLEU = 16.75, 50.3/22.3/11.4/6.1 (BP=1.000, ratio=1.021, hyp_len=32188, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flpqN42jonw8","executionInfo":{"status":"ok","timestamp":1622199668966,"user_tz":-420,"elapsed":1022,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"318214cc-3314-4e82-8400-46b0729a178c"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en < \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vi2en-align0vl-h8average-l4-alignment-lambda50p\n","BLEU = 17.49, 52.4/23.7/12.3/6.7 (BP=0.978, ratio=0.978, hyp_len=30827, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-4k3UK7adVi","executionInfo":{"status":"ok","timestamp":1622195956411,"user_tz":-420,"elapsed":1401,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"fe391223-ad01-40d6-d867-982e75ec8809"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en < \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vi2en-align0vl-hmedian-l4-alignment-lambda50p\n","BLEU = 16.37, 51.4/22.6/11.6/6.2 (BP=0.966, ratio=0.966, hyp_len=30446, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRB1Ca6C9-S1","executionInfo":{"status":"ok","timestamp":1622054270810,"user_tz":-420,"elapsed":1235,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"ecff4dd4-894d-469b-a8a2-f4e60a71c43d"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en < \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vi2en-align0vl-haverage-l4-alignment-lambda50p\n","BLEU = 17.78, 52.2/23.5/12.4/6.9 (BP=0.989, ratio=0.989, hyp_len=31156, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6jmLnWXIUub","executionInfo":{"status":"ok","timestamp":1622124093969,"user_tz":-420,"elapsed":1675,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"caae766a-a8f8-4677-d07d-317b04bb7526"},"source":["print(MODEL)\n","!/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en < \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vi2en-align0vl-hmedian-l4-alignment-lambda50p\n","Use of uninitialized value in division (/) at /content/tools/moses/scripts/generic/multi-bleu.perl line 139, <STDIN> line 1527.\n","Use of uninitialized value in division (/) at /content/tools/moses/scripts/generic/multi-bleu.perl line 139, <STDIN> line 1527.\n","Use of uninitialized value in division (/) at /content/tools/moses/scripts/generic/multi-bleu.perl line 139, <STDIN> line 1527.\n","Use of uninitialized value in division (/) at /content/tools/moses/scripts/generic/multi-bleu.perl line 139, <STDIN> line 1527.\n","BLEU = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=9.691, hyp_len=305400, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6BQVHlPZIaOv"},"source":["!head \"$MODEL\"/predict.en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w222C8WSLhUW","executionInfo":{"status":"ok","timestamp":1622225869147,"user_tz":-420,"elapsed":6686,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"caedb8af-bf78-4c0c-efb4-954db59d1b34"},"source":["import os\n","folders = [item for item in os.listdir(HOME) if os.path.isdir(item)]\n","for folder in folders:\n","    print(folder)\n","    li = os.path.join(HOME, folder)\n","    !/content/tools/moses/scripts/generic/multi-bleu.perl -lc test.en \\\n","    < \"$li\"/predict.en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["baseline-h1-l4\n","BLEU = 16.52, 52.0/23.1/11.7/6.3 (BP=0.959, ratio=0.959, hyp_len=30233, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","word2word-align0wl-h1-l4\n","BLEU = 16.02, 48.6/21.4/10.9/5.8 (BP=1.000, ratio=1.076, hyp_len=33900, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h1-l4-full_context_alignment\n","BLEU = 16.52, 52.1/23.0/11.8/6.3 (BP=0.956, ratio=0.957, hyp_len=30158, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h1-l4-alignment-lambda50p\n","BLEU = 17.47, 52.1/23.5/12.2/6.7 (BP=0.980, ratio=0.981, hyp_len=30901, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h1-l4-alignment-lambda100p\n","BLEU = 16.17, 49.0/21.7/11.0/5.8 (BP=1.000, ratio=1.058, hyp_len=33350, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h1-l5-alignment-lambda50p\n","BLEU = 16.09, 49.1/21.6/10.9/5.8 (BP=1.000, ratio=1.054, hyp_len=33226, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-haverage-l4-alignment-lambda50p\n","BLEU = 17.78, 52.2/23.5/12.4/6.9 (BP=0.989, ratio=0.989, hyp_len=31156, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h7-l4-alignment-lambda50p\n","/bin/bash: /content/drive/MyDrive/nmt_models/Alignment2/vi2en-align0vl-h7-l4-alignment-lambda50p/predict.en: No such file or directory\n","vi2en-align0vl-hmedian-l4-alignment-lambda50p\n","BLEU = 16.37, 51.4/22.6/11.6/6.2 (BP=0.966, ratio=0.966, hyp_len=30446, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","vi2en-align0vl-h8average-l4-alignment-lambda50p\n","BLEU = 17.49, 52.4/23.7/12.3/6.7 (BP=0.978, ratio=0.978, hyp_len=30827, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","wovi2en-align0wl-h8average-l4-alignment-lambda50p\n","BLEU = 16.75, 50.3/22.3/11.4/6.1 (BP=1.000, ratio=1.021, hyp_len=32188, ref_len=31513)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GXT0lDgLwzL","executionInfo":{"status":"ok","timestamp":1622225796871,"user_tz":-420,"elapsed":421,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"d2fba2bd-958d-4d9b-e6ae-cfac848f2062"},"source":["import os\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['baseline-h1-l4',\n"," 'word2word-align0wl-h1-l4',\n"," 'vi2en-align0vl-h1-l4-full_context_alignment',\n"," 'vi2en-align0vl-h1-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-h1-l4-alignment-lambda100p',\n"," 'vi2en-align0vl-h1-l5-alignment-lambda50p',\n"," 'vi2en-align0vl-haverage-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-h7-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-hmedian-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-h8average-l4-alignment-lambda50p',\n"," 'wovi2en-align0wl-h8average-l4-alignment-lambda50p']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3a63bG6LxPy","executionInfo":{"status":"ok","timestamp":1622225662664,"user_tz":-420,"elapsed":323,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"c4b0814d-9c2f-492c-edc0-497b513313a3"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['align_translate21.ipynb',\n"," 'train.wovi',\n"," 'train.leen',\n"," 'train.en',\n"," 'test.wovi',\n"," 'test.en',\n"," 'dev.wovi',\n"," 'dev.en',\n"," 'train.vi',\n"," 'dev.vi',\n"," 'test.vi',\n"," 'train.vi-en',\n"," 'train.align0',\n"," 'train.ralign0',\n"," 'train.gdalign0',\n"," 'baseline-h1-l4',\n"," 'train.wovi-leen',\n"," 'train.align0wl',\n"," 'train.ralign0wl',\n"," 'train.gdalign0wl',\n"," 'word2word-align0wl-h1-l4',\n"," 'vi2en-align0vl-h1-l4-full_context_alignment',\n"," 'vi2en-align0vl-h1-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-h1-l4-alignment-lambda100p',\n"," 'vi2en-align0vl-h1-l5-alignment-lambda50p',\n"," 'vi2en-align0vl-haverage-l4-alignment-lambda50p',\n"," 'align_translate2.ipynb',\n"," 'vi2en-align0vl-h7-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-hmedian-l4-alignment-lambda50p',\n"," 'vi2en-align0vl-h8average-l4-alignment-lambda50p',\n"," 'wovi2en-align0wl-h8average-l4-alignment-lambda50p']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"rizFd6yOLyrI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Ndjade9EUH4"},"source":["# Run locally"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OczjJkihEXnR","executionInfo":{"status":"ok","timestamp":1626471708679,"user_tz":-420,"elapsed":42,"user":{"displayName":"Nguyễn Chí Thiện","photoUrl":"","userId":"17428228934276242215"}},"outputId":"5b6fac8f-4a1f-488a-fe89-86c3a10014e0"},"source":["folder = '/Users/nguyenchithien/King_Saud_CIS_6_2021_textfiles'\n","import os\n","files = [\"test (1).en\",\n","         \"test (1).vi\",\n","         \"predict.vi\",\n","         \"predict (1).vi\",\n","         \"predict (2).vi\",         \n","]\n","ps = [os.path.join(folder, file) for file in files]\n","def catl(stop=1, start=0, length=5, ps=ps):  \n","    line_sets = [open(p, 'r').read().splitlines() for p in ps]\n","\n","    line_sets = [line_set for line_set in zip(*line_sets) if len(line_set[0].split(' '))==length]\n","    for i in range(start, stop):\n","        print(f'----------{i}----------')\n","        print('\\n'.join(line_sets[i]))\n","        print('         ')\n","\n","catl(stop=10, start=0, length=10)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["----------0----------\n","it usually affects people over 65 years of age .\n","đối tượng thường mắc bệnh là người già trên 65 tuổi .\n","người ta thường ảnh hưởng đến 65 tuổi .\n","nó thường ảnh hưởng đến mọi người trên 65 tuổi .\n","nó thường xảy ra ở những người trên 65 tuổi .\n","         \n","----------1----------\n","this starts to affect a person 's daily life .\n","điều này bắt đầu gây ảnh hưởng đến cuộc sống thường nhật của người bệnh .\n","điều này bắt đầu ảnh hưởng đến cuộc sống hàng ngày của người .\n","điều này bắt đầu ảnh hưởng đến cuộc sống hàng ngày của người .\n","điều này bắt đầu ảnh hưởng đến cuộc sống hằng ngày của người .\n","         \n","----------2----------\n","try talking with a parent or another trusted adult .\n","bạn nên cố nói chuyện với bố mẹ hoặc một người lớn tuổi nào đó mà bạn tin cậy .\n","hãy cố nói chuyện với bố mẹ hoặc một người lớn khác mà bạn tin tưởng .\n","hãy cố nói chuyện với bố hoặc một người lớn nào đó tin tưởng .\n","hãy thử nói chuyện với bố mẹ hoặc một người lớn tuổi khác tin tưởng .\n","         \n","----------3----------\n","breast cancer patients \" stop drugs \" due to side-effects\n","bệnh nhân ung thư vú \" ngưng thuốc \" do tác dụng phụ\n","bệnh nhân bị ung thư vú \" ngưng thuốc \" vì các tác dụng phụ phụ\n","bệnh nhân bị ung thư vú \" ngăn thuốc \" do tác dụng phụ\n","bệnh nhân ung thư vú \" ngưng \" vì các loại thuốc gây tác dụng phụ\n","         \n","----------4----------\n","a further 26 % had stopped by four years .\n","thêm 26% nữa ngừng thuốc trước bốn năm .\n","26 tuổi đã dừng lại bằng bốn năm .\n","một con số thêm 26 năm đã dừng lại do bốn năm .\n","hơn 26 phần trăm đã dừng lại bằng cách 4 năm .\n","         \n","----------5----------\n","residents of 12 villages fell ill after consuming toxic alcohol\n","cư dân của 12 ngôi làng đã ngã bệnh sau khi uống phải rượu độc\n","cư dân 12 ngôi làng bị bệnh nặng sau khi tiêu thụ rượu .\n","cư dân của 12 làng bị ngộ độc đã giảm sau khi tiêu thụ rượu\n","cư dân của 12 ngôi làng bị bệnh lao xuống sau khi uống rượu độc hại\n","         \n","----------6----------\n","toxic alcohol deaths are a regular occurrence in India .\n","việc tử vong do rượu độc là chuyện thường xảy ra ở Ấn Độ .\n","rượu gây tử vong thường xảy ra ở Ấn Độ .\n","rượu chứa rượu là một sự xuất hiện thường xuyên ở Ấn Độ .\n","nhiều người chết rượu cũng thường xảy ra ở Ấn Độ .\n","         \n","----------7----------\n","India has witnessed many incidents of toxic alcohol deaths .\n","Ấn Độ đã chứng kiến nhiều ca tử vong do rượu độc .\n","Ấn Độ đã chứng kiến nhiều sự cố về rượu gây tử vong .\n","Ấn Độ đã chứng kiến nhiều sự cố gây tử vong rượu .\n","Ấn Độ đã chứng kiến nhiều sự cố về rượu độc hại .\n","         \n","----------8----------\n","• Some 107 people killed in Gujarat in July 2009\n","• Khoảng 107 người thiệt mạng ở Gujarat vào tháng bảy năm 2009 .\n","• Một số người thiệt mạng trong tháng bảy 2009\n","• Một số người thiệt mạng vào tháng bảy năm 2009\n","• Một số người chết trong tháng bảy năm 2009\n","         \n","----------9----------\n","5 . it 's a slightly smaller screen siz #e\n","5 . nó có kích thước màn hình hơi nhỏ hơn\n","5 . màn hình nhỏ hơn một chút\n","5 . màn hình nhỏ hơn một chút\n","5 . nó hơi nhỏ hơn kích thước màn hình nhẹ\n","         \n"],"name":"stdout"}]}]}